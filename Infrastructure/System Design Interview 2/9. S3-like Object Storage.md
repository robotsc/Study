# S3-like Object Storage

- 이번 장에서는 Amazon S3와 비슷한 object storage 서비스를 설계해보자.  
  S3는 AWS에 의해 제공되는 object storage이며 RESTful API를 기반으로 하는 인터페이스를 제공한다.  
  아래는 S3에 대한 사실 중 일부이다.

  - 2006년 6월: GA
  - 2010년: Versioning, Bucket Policy, Multipart upload 지원
  - 2011년: SSE, Multi-object Delete, Object Expiration 지원
  - 2013년: S3에 저장된 object 개수 2조개 달성
  - 2014, 2015년: Life Cycle Policy, Event Notification, Cross-Region Replication 지원
  - 2021년: S3에 저장된 object 개수 200조개 달성

- Object storage를 설계하기 전, 우선 몇 가지의 storage system들을 살펴보고, 용어들을 정의하자.

## Storage System 101

- 개략적으로 봤을 때 storage system들은 아래의 3개 카테고리로 나뉜다.

  - Block storage
  - File storage
  - Object storage

### Block storage

- Block storage는 1960년대에 탄생했다. HDD, SSD와 같이 서버에 물리적으로 연결된 storage device를 block storage라 한다.

- Block storage는 저장된 raw block들을 서버에게 volume으로 제공한다. 이렇게 함으로써 storage를 유연하고, 다양한 용도로 사용할 수  
  있게 했다. 서버는 raw block들을 format해 file system으로 사용할 수도 있고, block들에 대한 제어권을 특정 애플리케이션에게  
  넘길 수도 있다. 데이터베이스나 VM engine같은 몇 가지 애플리케이션들은 성능을 쥐어짜내기 위해 block들을 직접 관리한다.

- Block storage는 단지 물리적으로 연결된 storage는 아니다. 빠른 속도를 가진 네트워크로 서버에게 연결될 수도 있고, FC(Fibre Channel),  
  iSCSI 등의 표준 프로토콜로 서버에게 연결될 수도 있다. 개념적으로 네트워크로 연결된 block storage는 여전히 raw block을 다루며  
  서버 입장에서는 물리적으로 연결된 block storage와 동일하게 사용할 수 있다.

### File storage

- File storage는 block storage에 기반해 만들어진 storage system이며 파일과 디렉토리를 더 쉽게 다루기 위해 추상화되어 있다.  
  데이터는 file의 형식으로 계층적 디렉토리 구조 내에 저장된다. 일반적인 용도로 가장 많이 사용되는 storage solution은 file storage이다.  
  File storage는 SMB/CIFS, NFS 등의 일반적인 file-level 네트워크 프로토콜을 사용해 동시에 수많은 서버들로 연결되어질 수 있다.  
  그리고 file storage에 접근하는 서버들은 block 관리, volume formatting 등 복잡한 과정을 관리할 필요가 없다.  
  이러한 file storage의 단순함과 간편함은 특정 organization 내에서 여러 개의 파일, 디렉토리를 공유하기에 너무나 좋은 선택지가 된다.

### Object storage

- Object storage는 block storage, file storage에 비하면 최신 기술에 속한다. 그리고 항상 성능과 안전성, 확장성, 비용에 대해  
  tradeoff 관계가 형성된다. 주로 상대적으로 archiving이나 backup을 위해 사용되는 상대적으로 _"cold"_ 한 데이터를 저장하기 위해  
  많이 사용된다. Objec tsotrage는 모든 데이터를 flat한 구조에 object로 저장한다. 따라서 계층적인 디렉토리 구조가 없다.  
  데이터에 대한 접근은 일반적으로 RESTful API로 제공된다. 그리고 다른 storage들에 비해 상대적으로 처리 속도가 느린 편이다.  
  대부분의 public cloud provider는 Amazon S3, Google Object Storage, Azure Blob Storage와 같이 object storage를  
  제공한다.

#### 3개 storage system들의 비교

![picture 45](/images/SDI2_SOS_1.png)

- 아래 표는 block storage, file storage, 그리고 object storage를 비교한 내용을 가진다.

| 특성            | Block storage                                              | File storage                        | Object storage                        |
| --------------- | ---------------------------------------------------------- | ----------------------------------- | ------------------------------------- |
| Mutable content | Y                                                          | Y                                   | N(object versioning을 사용해야 한다.) |
| Cost            | High                                                       | Medium to high                      | Low                                   |
| Performance     | Medium to high, very high                                  | Medium to high                      | Low to medium                         |
| Consistency     | Strong consistency                                         | Strong consistency                  | Strong consistency                    |
| Data access     | SAS/iSCSI/FC                                               | Standard file access, CIFS/SMB, NFS | RESTful API                           |
| Scalability     | Medium                                                     | High                                | Vast                                  |
| Good for        | VMs(Virtual Machines), Database와 같은 고성능 애플리케이션 | 일반적인 용도의 file system access  | Binary data, unstructured data        |

### 용어 정의

- S3-like object storage를 설계하려면 우선 object storage의 핵심 개념을 정립해야 한다.  
  여기서는 관련 용어들과 그 내용을 정의해보자.

- **Bucket** : Object들의 논리적인 container. Bucket의 이름은 전역적으로 unique해야 하며 데이터를 S3에 upload하기 전, 먼저  
  bucket을 생성해야 한다.

- **Object** : Object는 bucket에 저장할 독립적인 데이터 조각을 말한다. Payload라고도 불리는 object data와 metadata를 가진다.  
  Object data는 저장하고 싶은 순차적인 byte가 될 수도 있고, meatadata는 object에 대한 정보를 담은 name-value pair들로 구성된다.

- **Versioning** : 같은 bucket 내의 한 object의 여러 변형을 저장하도록 하는 기능이다. Bucket-level에서 활성화시킬 수 있다.  
  이 기능은 의도치 않게 삭제되거나 갱신된(overwritten) object를 사용자들이 복구할 수 있게 한다.

- **URI(Uniform Resource Identifier)** : Object storage는 bucket, object에 접근하기 위해 RESTful API들을 제공한다.  
  이를 위해 각 리소스들은 URI로 구별된다.

- **SLA(Service-Level Agreement)** : SLA는 서비스 프로바이더와 클라이언트 사이의 계약이다.  
  예를 들어, Amazon S3 Standard-IA storage class는 아래의 SLA를 제공한다.

  - 여러 개의 AZ에 걸쳐 99.999999999%의 내구성을 위해 설계되었다.
  - 데이터는 하나의 AZ가 장애 나더라도 회복할 수 있다.
  - 99.9%의 가용성을 위해 설계되었다.

---

## 문제 이해 및 설계 범위 확정

- 이번에 설계할 시스템의 요구사항들은 아래와 같다.

### 기능적 요구사항

- Bucket 생성
- Object upload, download
- Object versioning
- Bucket 내의 object들의 목록 조회. `aws s3 ls` 명령과 비슷하다.
- 대용량의(GB 단위 또는 그 이상) 데이터와 작은 데이터(수십 KB)를 모두 효율적으로 저장할 수 있어야 한다.
- 1년에 저장되는 총 데이터량은 100PB이다.

### 비기능적 요구사항

- 데이터 내구성은 99.9999%, 가용성은 99.99%를 만족해야 한다.
- Storage는 효율적이어야 한다. 높은 안정성과 성능을 제공함과 동시에 storage 비용은 최소화해야 한다.

### 추정치 계산

- Object storage는 일반적으로 디스크의 용량이나 IOPS(Input Output Per Second)가 병목 지점이 된다. 간단히 살펴보자.

  - Disk 용량: Object들이 아래와 같이 분포되어 있다고 가정해보자.

    - 1MB 미만의 object들: 20%
    - 1MB~64MB의 object들: 60%
    - 64MB 이상의 object들: 20%

  - IOPS: 하나의 hard disk(SATA interface, 7200rpm)가 random seek를 1초에 100~150번 수행할 수 있다 하자.(100 ~ 150 IOPS)

- 위의 수치들에 기반해 시스템이 보관할 수 있는 object들의 개수를 추정해볼 수 있다. 계산 과정을 단순화하기 위해 small object는 0.5MB,  
  medium object는 32MB, 그리고 large object는 200MB의 크기를 가진다고 가정하자.

- Storage를 40% 정도 사용한다 가정하면, 아래와 같은 수치가 나온다.

  - 100PB = `100 * 1000 * 1000 * 1000MB = 10^11MB`
  - `10^11MB * 0.4 / (0.2 * 0.5MB + 0.6 * 32MB + 0.2 * 200MB) = 6억 8000`개의 object들
  - Object의 metadata가 1KB라 가정하면 metadata 정보들을 위한 공간은 0.68TB가 된다.

- 위에서 나온 수치들을 직접 사용하지 않을 수도 있지만, 설계하려는 시스템의 규모와 제약 조건들을 파악하기에는 좋은 방법이다.

---

## 개략적 설계안 제시 및 동의 구하기

- 설계를 다루기 전, 먼저 object storage의 재미있는 특징들을 먼저 살펴보자.

- **Object immutability** : Object storage가 file storage, block storage와 달리 가지는 주요한 차이점 중 하나는 object  
  storage 내에 저장된 object는 불변하다는 것이다. 기존 object를 삭제하거나 완전히 대체할 수는 있어도, 갱신할 수는 없다.

- **Key-value store** : Object data를 가져오기 위해 object URI를 사용할 수 있다. Object URI는 key가 되며, object data가  
  value가 된다.

```
Request:
GET /bucket1/object1/txt HTTP/1.1

Response:
HTTP/1.1 200 OK
Content-Length: 4567

[4567 bytes of object data]
```

- **Write once, read many times** : Object data에 대한 접근 패턴은 한 번 쓰고 여러번 읽는 특징을 가진다.  
  LinkedIn에 따르면 요청의 95%가 read 연산을 수행한다고 한다.

- **Support both small and large objects** : Object의 크기는 각각 다르며 크거나 작은 데이터를 모두 지원해야 한다.

- Object storage의 설계 철학은 UNIX file system과 매우 유사하다. UNIX에서 local file system에 파일을 저장할 때  
  filename과 file data를 함께 저장하지 않는다. 대신 filename은 inode라는 자료구조에 저장되고 file data는 disk의 다른 위치에  
  저장된다. inode는 file data를 가리키는 file block pointer들을 가진다. 특정 local file에 접근할 때 먼저 inode로부터  
  metadata를 가져오게 된다. 그 후 file block pointer를 따라 디스크에 file data가 저장된 위치를 파악해 데이터를 읽어오게 된다.

- Object storage도 위와 비슷하게 동작한다. inode는 meatdata store가 되며, metadata store는 모든 object metadata를  
  보관한다. Hard disk는 object data가 저장되는 data store가 된다. UNIX file system에서 inode가 file block pointer를  
  통해 hard disk에 저장된 데이터의 위치를 찾아가는 것처럼 object storage에서는 네트워크 요청을 사용해 metadata store가  
  object의 ID를 통해 data store에 저장된 object data를 찾아간다. 아래 그림은 이 둘의 개랴적인 과정을 나타낸다.

  ![picture 47](/images/SDI2_SOS_2.png)

- 이렇게 metadata와 object data를 분리해 저장하는 것은 설계를 단순하게 해준다. Data store는 불변(immutable) 데이터를 저장하는  
  반면, metadata store는 가변(mutable) 데이터를 저장한다. 이러한 분리는 서로 다른 컴포넌트를 독립적으로 구현하고 최적화할 수 있도록 한다.

- 아래 그림은 bucket과 object가 어떤 모습인지 간단히 나타낸다.

  ![picture 48](/images/SDI2_SOS_3.png)

### 개략적 설계안

- 아래 그림은 개략적 설계안이다.

![picture 49](/images/SDI2_SOS_4.png)

- 각 컴포넌트들을 하나씩 살펴보자.

  - Load balancer: 여러 개의 API server들로 RESTful API 요청들을 분산시킨다.
  - API service: RPC들을 IAM service, Metadata service, Date store들로 적절히 orchestrate한다.  
    이 컴포넌트는 수평적 확장을 쉽게 하기 위해 stateless하다.
  - IAM(Identity & Access Management): 이 서비스는 인증, 인가, 그리고 접근 제어를 수행하는 중앙 서비스이다.  
    인증은 요청자의 신원을 파악하고, 인가는 신원에 따라 특정 작업을 수행할 권한이 있는지를 검증한다.
  - Data store: 실제 데이터를 저장하고 반환한다. 데이터와 관련된 모든 연산은 Object ID(UUID)를 기반으로 수행된다.
  - Metadata store: Object들의 metadata들을 저장한다.

- 이제 개략적 설계안에 대해 기본적인 이해가 되었으니, 아래의 object storage의 가장 주된 workflow들을 다뤄보자.

  - Object upload
  - Object download

#### Object upload

![picture 50](/images/SDI2_SOS_5.png)

- Object는 특정 bucket내에 저장된다. `bucket-to-share`라는 이름의 bucket을 만들고, 이 bucket에 `script.txt`라는 파일을  
  저장하는 과정을 살펴보자.

  - (1) 클라이언트는 `bucket-to-share`의 이름을 가지는 bucket을 생성하는 HTTP PUT request를 보낸다.  
    이 요청은 load balancer에서 API service로 전달된다.
  - (2) API service는 IAM을 호출해 해당 사용자가 WRITE 작업에 대한 권한을 가지는지 확인한다.
  - (3) API service는 bucket에 대한 새로운 entry를 metadata store에 생성한다. 이 entry가 생성되면, 성공 응답이 클라이언트에게 보내진다.
  - (4) Bucket이 생성된 후, 클라이언트는 `script.txt`라는 파일을 업로드하는 HTTP PUT request를 보낸다.
  - (5) 다시 IAM으로 권한이 있는지 확인하고, 성공하면 API service는 HTTP PUT request의 payload를 data store에 저장시킨다.  
    Data store는 payload를 object로 저장하며, 저장된 object의 UUID를 반환한다.
  - (6) API service가 metadata database에 새로운 entry를 저장시킨다. `object_id(UUID)`와 `bucket_id`,  
    `object_name` 등 중요한 metadata를 포함한다. 아래 표는 저장된 entry의 예시이다.

  | object_name | object_id                        | bucket_id                         |
  | ----------- | -------------------------------- | --------------------------------- |
  | script.txt  | 239D8443-0122-44F5-043E-C13E345S | 82AA1B2E-F599-4593-B353-1F5FA4FS1 |

- Object를 upload하는 API는 아래와 같다.

  ```
  PUT /bucket-to-share/script.txt HTTP/1.1
  Host: foo.s3example.org
  Date: Mon, 1 Aug 2022 17:03:00 KST
  Authorization: authorization string
  Content-Type: text/plain
  Content-Length: 4567
  x-amz-meta-author: Sangwoo

  [4567 bytes of object data]
  ```

#### Object download

- Bucket은 계층적인 디렉토리 구조를 갖지 않는다. 하지만 bucket 이름과 object 이름을 연결시켜 논리적으로 폴더 구조를 따라할 수 있다.  
  예를 들어 object 이름을 `script.txt`에 대신 `bucket-to-share/script.txt`로 짓는 것이다.

- 특정 object를 가져오기 위해서는 GET 요청에 object이름을 지정해야 한다. 이 API는 아래와 같이 생겼다.

  ```
  GET /bucket-to-share/script.txt HTTP/1.1
  Host: foo.s3example.org
  Date: Mon, 1 Aug 2022 17:03:00 KST
  Authorization: authorization string
  ```

- 이전에 언급했듯이 data store는 object의 이름을 저장하지 않고, 오직 `object_id(UUID)`를 기반으로 한 object 관련 작업만을  
  수행한다. 따라서 object를 download하기 위해서는 먼저 object의 이름을 UUID로 매핑시켜야 한다.

- Object를 다운로드하는 과정은 아래 그림과 같다.

![picture 51](/images/SDI2_SOS_6.png)

- 각 과정을 살펴보자.

  - (1) 클라이언트는 load balancer로 HTTP GET 요청을 보낸다. (`GET /bucket-to-share/script.txt`)
  - (2) API service는 IAM을 통해 요청을 보낸 자가 bucket에 대한 READ 권한이 있는지 검증한다.
  - (3) 검증이 성공하면 API service는 metadata store를 통해 해당 object의 UUID를 가져온다.
  - (4) API service는 data store로부터 UUID를 통해 해당 object data를 가져온다.
  - (5) API service는 object data를 HTTP GET의 응답으로 보낸다.

---

## 상세 설계

- 이번에는 아래의 영역들을 깊게 다뤄보자.

  - Data store
  - Metadata data model
  - Bucket 내의 object들 조회
  - Object versioning
  - 대용량 파일 업로드 최적화
  - Garbage collection

### Data store

- Data store의 설계를 더 깊게 보자. 이전에 봤듯이 API service는 사용자들로부터 발생한 외부 request들을 처리하며, 적절한 내부 서비스가  
  해당 요청을 처리하도록 한다. Object를 저장하고 가져오기 위해 API service는 data store를 사용한다.  
  아래 그림은 object를 upload, download할 때 API service와 data store의 상호작용을 나타낸다.

  ![picture 52](/images/SDI2_SOS_7.png)

#### Data store의 개략적 설계

- Data store는 아래 그림과 같이 3개의 주요 컴포넌트를 가진다.

![picture 53](/images/SDI2_SOS_8.png)

- 각 컴포넌트에 대해 알아보자.

#### Data routing service

- Data routing service는 data node cluster에 접근하기 위한 RESTful 또는 gRPC API를 제공한다.  
  Stateless 서비스이며 서버를 추가함으로써 확장할 수 있다. 이 서비스는 아래의 책임을 가진다.

  - 데이터를 저장하기 위한 최적의 data node를 가져오기 위해 Placement service에 요청
  - Data node들로부터 데이터를 읽고, API service로 반환
  - Data node에 데이터 저장

#### Placement service

- Placement service는 어떤 data node(primary, replicas)에 object를 저장할 것인지 결정한다.  
  이 과정은 cluster의 물리적 정보를 제공하는 virtual cluster map을 사용하여 구현된다. Virtual cluster map은 placement
  service가 사용하는 data node들 각각의 위치 정보를 가져 replica들이 물리적으로 분리되어 있도록 한다. 이러한 분리(separation)는  
  높은 내구성을 제공하기 위한 핵심 속성이다.

- 아래는 virtual cluster map의 예시이다.

  ![picture 54](/images/SDI2_SOS_9.png)

- Placement service는 heartbeat를 사용해 모든 data node들을 지속적으로 모니터링한다. 만약 특정 data node가 설정된 주기 내에  
  heartbeat를 보내지 않으면, placement service는 해당 node를 virtual cluster map에 "down"으로 표기한다.

- 이 서비스는 굉장히 중요하기에 Paxos 또는 Raft 합의 프로토콜을 사용해 5개에서 7개의 placement service node들로 구성된 cluster를  
  운영하는 것이 좋다. 합의(consensus) 프로토콜은 절반 이상의 node들이 healthy하면 전체 서비스가 작동함을 보장해준다.  
  예를 들어 만약 placement service cluster가 7개의 node들로 구성되어 있다면 3개의 node가 장애나도 이를 감래할 수 있다.

#### Data node

- Data node는 object의 실제 데이터를 저장하며 여러 개의 data node들(replication group)으로 데이터를 복제함으로써 안정성과  
  내구성을 보장한다.

- 각 data node에는 data service daemon이 실행되는데, 이 data service daemon은 계속해서 placement service로 heartbeat를  
  전송한다. Heartbeat message는 아래의 중요한 정보들을 포함한다.

  - Data node가 관리하고 있는 disk drive(HDD 또는 SSD)의 개수
  - 각 disk drive에 저장된 데이터량

- Placement service는 data node로부터 heartbeat를 처음 수신하면, 해당 heartbeat를 보낸 data node에 ID를 부여하고,  
  virtual cluster map에 추가하고, 아래의 정보들을 반환한다.

  - Data node의 고유한 ID
  - Virtual cluster map
  - 해당 data node의 데이터가 복제될 data node

#### Data persistence flow

![picture 55](/images/SDI2_SOS_10.png)

- 데이터가 data node로 저장되는 과정을 살펴보자.

  - (1) API service는 object data를 data store로 전달한다.
  - (2) Data routing service는 해당 object를 위한 UUID를 생성하고, 해당 object를 어떤 data node에 저장할지 알아내기 위해  
    placement service에 질의한다. Placement service는 virtual cluster map을 검사하고, primary data node를 반환한다.
  - (3) Data routing service가 primary data node에 UUID와 함께 object data를 저장시킨다.
  - (4) Primary data node는 전달받은 데이터를 저장하고, 2개의 secondary data node들에게 복제시킨다.  
    Primary node는 모든 secondary node들에 데이터의 복제가 완료되었을 때 data routing service에 데이터 저장 성공 응답을 보낸다.
  - (5) API service로 object의 UUID가 반환된다.

- (2)번 단계에서 placement service는 object의 UUID를 받아 해당 object가 저장될 replication group을 찾아 반환한다.  
  이를 어떻게 하는 걸까? 이러한 lookup은 결정적(deterministic)해야 하며, replication group가 추가 또는 삭제 되는 등의 경우도  
  잘 처리해야 한다. 주로 consistent hashing이 이러한 lookup 기능의 구현을 위해 사용된다.

- (4)번 단계에서 parimary data node는 응답을 보내기 전, 모든 secondary node들로 데이터를 복제시킨다. 이렇게 하면 모든 data node들에  
  걸쳐 데이터의 높은 일관성을 보장할 수 있다. 하지만 이러한 일관성은 가장 느린 replica도 복제가 될 때까지 기다려야 하므로 latency와의  
  tradeoff 관계를 형성한다. 아래 표는 일관성과 latency의 tradeoff 관계를 보여준다.

  ![picture 56](/images/SDI2_SOS_11.png)

  - Option 1: 데이터는 모든 3개의 node에 데이터가 저장되면 성공적으로 저장되었다고 간주된다. 이 방식은 가장 일관성이 높지만, latency가 길다.
  - Option 2: 데이터는 primary node에 저장되고, secondary node들 중 하나만 저장되어도 성공적으로 저장되었다고 간주된다.  
    이 방식은 일관성도 중간, latency도 중간 정도를 가진다.
  - Option 3: 데이터는 primary에만 저장되어도 성공적으로 저장되었다고 간주된다. 가장 낮은 일관성을 갖지만, latency가 가장 짧다.
  - Option 2, Option 3는 eventual consistency를 제공한다.

#### 데이터 구성 방식

- 이번에는 각 data node가 어떻게 데이터를 관리하는지 살펴보자. 가장 간단한 방법으로 각 object를 하나의 파일로 저장할 수 있을 것이다.  
  이는 동작하지만, 작은 파일이 많으면 성능이 떨어질 것이다. File system에 작은 파일이 너무 많아지면, 아래의 2개 문제가 발생한다.

  - 많은 양의 data block을 낭비하게 된다. File system은 파일을 별도의 disk block에 저장한다. Disk block들은 모두 동일한  
    크기를 가지며, 이 크기는 volume이 초기화될 때 고정된다. 일반적인 block size는 4KB 정도이다. 4KB보다 작은 파일은 여전히  
    disk block이 4KB이기에 4KB의 공간을 차지할 것이다. 따라서 만약 시스템이 작은 파일을 대량으로 가진다면 많은 양의 data block을  
    낭비하게 될 것이다.

  - 시스템의 inode 용량을 초과할 수 있다. File system은 파일의 위치와 관련된 다른 정보들을 inode라고 불리는 특수한 block에 저장한다.  
    대부분의 file system에서 inode의 개수는 디스크가 초기화될 때 고정된다. 작은 파일이 수백만개가 되면 모든 inode를 소비해버릴 수 있다.  
    또한 OS도 file system metadata를 아무리 caching하더라도 많은 개수의 inode를 잘 처리하지 못한다.

  - 이러한 이유로 작은 object들을 하나의 파일로 저장하는 것은 좋지 못한 선택이다.

- 위의 문제들을 해결하기 위해 작은 object들은 이들끼리 합쳐 하나의 큰 파일로 관리하도록 할 수 있다. 개념적으로는 WAL(Write-Ahead Log)와  
  유사하게 동작한다. Object를 저장하면, 해당 object는 이미 존재하는 read-write 파일에 append된다. 만약 이 read-write 파일이  
  용량 제한(일반적으로 몇 GB)을 초과하면, 해당 read-write 파일은 read-only로 변경되고 새로운 read-write 파일이 생성되어 새롭게  
  저장될 object들을 저장한다. 파일이 read-only로 변경되는 순간부터는 오직 read 요청만을 처리할 수 있다.  
  아래 그림은 이 방식이 동작하는 과정을 보여준다.

  ![picture 57](/images/SDI2_SOS_12.png)

- Read-write 파일에 대한 write 접근은 직렬화되어야 함에 유의하자. 위 그림에 나타난 것처럼 object들은 read-write 파일에 순서대로 하나씩  
  저장된다. 이러한 on-disk layout을 유지하려면 병렬적으로 write 요청을 처리하는 여러 개의 core들은 read-write 파일에 write하기 위해  
  순서를 지켜야 한다. 현대 서버는 core수가 많으며 이렇게 요청을 동시적으로 수행할 때 순서를 지키게끔 하는 제약은 처리량을 현저히 떨어뜨리게 된다.  
  이를 고치고 싶다면 요청을 처리하는 core 각각 만을 위한 read-write 파일을 제공해 각 core가 자신만의 read-write 파일을 사용하게끔 할 수 있다.

#### Object lookup

- 이제 각 파일은 여러 개의 작은 object들을 가지게 된다. 이 경우, data node가 UUID를 기반으로 특정 object의 위치를 어떻게 알아낼까?  
  Data node는 이를 위해 아래의 정보들을 필요로 한다.

  - Object를 포함하고 있는 data file
  - Data file 내의 해당 object의 start offset
  - Object의 크기

- 아래 표는 이러한 lookup을 지원하기 위한 데이터베이스 스키마이다.(테이블명: `object_mapping`)

  | 필드          | 설명                          |
  | :------------ | ----------------------------- |
  | object_id(PK) | object의 UUID                 |
  | file_name     | object를 포함하는 파일의 이름 |
  | start_offset  | 파일 내의 object의 시작 주소  |
  | object_size   | Object의 byte 크기            |

- 위의 mapping 정보를 보관할 장소로 RocksDB와 같은 파일 기반의 key-value store와 RDBMS를 비교해보자.

  - RocksDB: SSDTable을 기반으로 하기에 write는 빠르지만, read가 상대적으로 느리다.
  - RDBMS: 일반적으로 B+ tree 기반의 storage engine을 사용하기에 read는 빠르지만, write는 느리다.

- 위 비교를 토대로 RDBMS가 더 나은 read 성능을 제공하기에, RDBMS를 사용하는 것이 좋아보인다.

- 그렇다면 이 RDBMS를 어떻게 배포해야 할까? 우리가 설계하는 시스템의 규모에서 mapping 테이블의 크기는 매우 크다.  
  수많은 data node들을 가지는 하나의 큰 cluster로 배포할 수 있지만, 이는 관리하기 어렵다. 그리고 mapping data는 각 data node마다  
  서로 격리되어 있다는 점에 유의해야 한다. 즉, 여러 data node들이 데이터를 공유할 필요가 없다. 이러한 속성을 최대한 살리기 위해  
  각 data node마다 RDBMS를 배포하도록 할 수 있다. 파일 기반으로 되어있는 SQLite가 좋은 선택지일 것이다.

#### 수정된 data persistence flow

- Data node에 관련해 몇 가지 수정 사항을 이야기했으니, 이 변경 사항들을 반영한 새로운 object를 저장하는 workflow를 살펴보자.

![picture 58](/images/SDI2_SOS_13.png)

- 각 과정은 아래와 같다.

  - (1) API service가 object 4라는 이름을 가진 object를 저장하는 요청을 보낸다.
  - (2) Data node service가 /data/c의 이름을 가진 read-write 파일에 object 4를 append한다.
  - (3) object 4의 mapping 내용을 담은 row가 `object_mapping` 테이블에 저장된다.
  - (4) Data node가 저장된 object의 UUID를 API service에게 반환한다.

#### Durability(내구성)

- Data storage system에 있어 데이터의 안전성은 매우 중요하다. 99.9999%의 내구성을 제공하는 storage system을 어떻게 만들 수 있을까?  
  장애가 날 수 있는 상황 각각에 대한 고려가 필요하며 데이터가 올바르게 복제되어야 한다.

##### 하드웨어 장애 및 그 외의 장애 요소

- 하드웨어 장애는 어떠한 매개체를 사용하더라도 사실상 불가피하다. 일부 storage media는 다른 것들에 비해 더 나은 내구성을 제공하기도 하지만,  
  99.9999%의 높은 내구성을 위해서는 하나의 hard drive에 의존할 수 없다. 내구성을 높힐 수 있는 증명된 방식 중 하나는 하나의 disk가  
  장애가 나더라도 데이터의 가용성에 영향을 주지 않기 위해 데이터를 여러 개의 hard drive들로 복제하는 것이다.  
  이전에 본 설계안에서, 이 시스템은 데이터를 총 3번 복제한다.

- 1년에 장애가 날 확률이 0.81%인 hard drive를 사용한다 해보자. 이 장애 확률 수치는 모델과 어떤 hard drive를 사용하는지에 따라 달라진다.  
  이 상황에서 데이터를 3번 복제하는 것은 `1 - 0.0081^3 = 0.999999`의 안전성을 제공한다. 이 수치는 매우 개략적인 추정치이다.

- 내구성을 완벽히 측정하기 위해서는 다른 영역에서 발생할 수 있는 장애에 또한 감안해야 한다. 이를 failure domain이라 하는데, 정의는 아래와 같다.

  > Failure domain: 주요 서비스가 문제가 생겼을 때 부정적인 영향을 받는 물리적, 또는 논리적 환경

- 현대 data center에서 서버는 주로 rack에 있으며, rack들은 rows/floors/rooms로 grouping된다.  
  각 rack이 네트워크 스위치와 전원을 공유하기 때문에 하나의 rack 안에 있는 모든 서버들은 rack-level의 failure domain이 된다.  
  현대 서버는 motherboard, processor, 전원 공급 장치, HDD drive 등의 컴포넌트들을 서로 공유한다. 이러한 컴포넌트들은  
  node-level의 failure domain에 속한다.

- 대규모 환경에서 failure domain을 격리하는 좋은 예시를 살펴보자. 일반적으로 data center는 서로 다른 AZ끼리 그 어떠한 것도  
  공유하지 않도록 격리되어 있다. 따라서 장애의 영향 범위를 줄이기 위해 아래와 같이 데이터를 다른 AZ로 복제하도록 할 수 있다.  
  이때 failure domain을 격리하는 것은 직접적으로 데이터의 내구성을 향상시키지는 않지만 자연 재해, 정전 등의 상황 발생 시  
  더 높은 안정성을 제공할 수 있다는 점에 유의하자.

  ![picture 59](/images/SDI2_SOS_14.png)

#### Erasure coding

- 위에서 봤듯이 데이터를 3번 복제하면 대충 99.9999%의 내구성을 제공할 수 있다. 이에 더해 내구성을 더욱 높이는 방법은 없을까?  
  Erasure coding이 한 가지 방법이 될 수 있다. Erasure coding은 데이터 내구성에 대해 조금 다른 방법으로 접근한다.  
  데이터를 서로 다른 서버에 작은 단위(chunk)로 분산시키고, 중복 제거를 위해 parity를 생성한다.

> Parity: 데이터가 한 storage에서 다른 storage로 옮겨지거나 다른 컴퓨터로 옮겨지는 등의 상황이 발생했을 때,  
> 데이터의 누락이 생겼거나 변경 사항이 생겼는지를 검사하는 기술

- 장애가 발생하면 chunk data와 parity를 사용해 데이터를 재건(reconstruct)할 수 있다.  
  아래 그림은 4+2 erasure coding을 보여준다.

  ![picture 60](/images/SDI2_SOS_15.png)

- 각 과정을 하나씩 살펴보자.

  - (1) 데이터가 같은 크기를 가진 data chunk(d1, d2, d3, d4)들로 분리된다.
  - (2) 특정 수식을 사용해 parity p1, p2가 계산된다. 아래는 매우 단순화시킨 예시이다.
    - `p1 = d1 + 2 * d2 - d3 + 4 * d4`
    - `p2 = -d1 + 5 * d2 + d3 - 3 *d4`
  - (3) d3, d4가 node의 장애로 인해 삭제된다.
  - (4) Parity를 만들 때 사용된 수식을 적용해 누락된 데이터인 d3, d4를 d1, d2, p1, p2를 이용해 다시 만들어낸다.

- Erasure coding이 어떻게 동작하는지 확실히 이해하기 위해 또다른 예시를 보자.  
  아래는 8+4 erasure coding의 예시이다.

  ![picture 61](/images/SDI2_SOS_16.png)

- 8+4 erasure coding은 데이터를 동일한 크기를 가지는 8개의 chunk들로 분할하고, 4개의 parity들을 계산해 만들어낸다.  
  이렇게 생긴 12개의 데이터는 모두 동일한 크기를 가진다. 그리고 12개의 데이터는 모두 서로 다른 12개의 failure domain들로 분산된다.  
  Parity를 만들어내는(erasure coding에 사용되는) 수식은 최대 4개의 node들이 장애가 나 데이터가 누락되더라도 남아있는 데이터들로부터  
  누락된 데이터를 다시 만들어낼 수 있도록 보장한다.

- Data router가 object를 조회하기 위해 단 하나의 healty한 node에만 질의하면 되었던 것에 비해 erasure coding에서는 data  
  router가 최소 8개의 healthy한 node들로부터 데이터를 읽어와야 한다. 이는 설계 상에 생기는 tradeoff이다.  
  이렇게 더 높은 내구성과 적은 storage 비용 vs 데이터 접근 속도의 tradeoff가 발생하는 것이다.  
  우리가 설계하는 object storage에서 대부분의 비용은 storage에서 발생하기에 이렇게 erasure coding을 사용하는 것도 좋은 선택이다.

- 그렇다면 erasure coding을 사용할 때 얼마 만큼의 추가 저장 공간이 필요할까? 2개의 data chunk에 대해 하나의 parity block이  
  생성되므로 storage overhead는 50%이다. 반면, 3번 데이터를 복제하는 설계에서 storage overhead는 200%이다.

  ![picture 62](/images/SDI2_SOS_17.png)

- 그렇다면 erasure coding이 데이터 내구성을 어떻게 향상시키는 것일까? 전과 같이 1년 동안 장애 확률이 0.81%인 node가 있다 해보자.  
  Backblaze에 의해 진행된 연구 결과에 따르면, erasure coding을 적용하면 99.999999999%의 내구성에 도달할 수 있다고 한다.

- 아래 표는 데이터 복제와 erasure coding의 장단점을 나타낸다.

  |       특성       |                                          데이터 복제                                           |                                                                        erasure coding                                                                        |
  | :--------------: | :--------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------: |
  |      내구성      |                                  99.9999%(3번의 데이터 복제)                                   |                                                             8+4 erasure coding 시 99.999999999%                                                              |
  | 저장 공간 효율성 |                                     200% storage overhead                                      |                                                                     50% storage overhead                                                                     |
  |  컴퓨팅 리소스   |                                              없음                                              |                                                                  parity 계산을 위해 사용됨                                                                   |
  |    write 성능    |                              복제만 하면 되고, 계산 과정이 없다.                               |                                               disk에 데이터를 쓰기 전 parity를 계산해야 하므로 latency가 증가                                                |
  |    read 성능     | 일반적으로 read는 replica에서 처리된다. 장애가 나더라도 read는 다른 replica에서 수행 가능하다. | 일반적으로 모든 read 요청은 처리되기 위해 cluster 내의 여러 node로부터 데이터를 읽어와야 한다. Node에 장애가 나면 먼저 데이터를 재건해야 하기 때문에 느리다. |

- 요약하자면 latency가 중요한 요인인 애플리케이션에서는 데이터 복제가 자주 사용되며 storage cost를 줄이는게 중요한 상황에서는  
  erasure coding이 자주 사용된다. Erasure coding은 비용 효율성과 내구성 덕분에 굉장히 매력적이지만, data node의 설계를  
  매우 복잡하게 만든다. 따라서 이번 설계에서는 데이터 복제를 사용하도록 한다.

#### 정확성 검증

- Erasure coding은 데이터 내구성을 향상시키면서 storage cost를 줄인다.  
  이제 다른 문제를 살펴보자. 바로 데이터 오염이다.

- 만약 디스크 전체가 장애가 발생하고 이 장애가 파악 가능하다면, data node에 장애가 발생했다고 판단해도 된다.  
  이 경우 erasure coding을 사용해 누락된 데이터를 재건할 수 있다. 하지만 대규모 시스템에서는 disk가 아니라 in-memory 데이터의 오염이  
  빈번하게 발생한다.

- 이 문제는 process 영역 사이에 checksum을 사용해 검증함으로써 해결할 수 있다.  
  Checksum은 데이터 에러를 파악하기 위해 사용되는 작은 data block이다. 아래 그림은 checksum이 만들어지는 과정을 나타낸다.

  ![picture 63](/images/SDI2_SOS_18.png)

- 만약 원본 데이터의 checksum을 알고 있다면, 데이터가 이동된 후 checksum을 계산해내 기존 checksum과 비교할 수 있다.

  - 이 둘이 다르다면 데이터는 오염된 것이다.
  - 이 둘이 동일하다면 데이터가 오염되지 않았을 확률이 매우 높다. 100%로 확신할 수는 없지만, 일반적으로는 오염되지 않았다고 판단할 수 있다.

    ![picture 64](/images/SDI2_SOS_19.png)

- Checksum 알고리즘은 MD5, SHA1, HMAC 등 많이 있다. 좋은 checksum 알고리즘은 데이터가 작게 변경되어도 checksum 값이 매우 다르게  
  계산되도록 한다. 이번 장에서는 MD5와 같이 단순한 checksum 알고리즘을 선택하자.

- 이 설계에서는 각 object의 끝에 checksum을 append하도록 한다. 파일이 read-only로 변경되기 전, 파일 전체에 대한 checksum을 끝에  
  append한다. 아래 그림은 이를 보여준다.

  ![picture 65](/images/SDI2_SOS_20.png)

- 8+4 erasure coding과 더불어 checksum 검증을 수행한다면 데이터를 읽을 때 아래의 과정이 수행된다.

  - (1) Object data와 checksum을 가져온다.
  - (2) 받아온 데이터의 checksum을 계산한다.
    - 가져온 checksum과 계산한 checksum이 일치하면 데이터는 에러가 없는 것으로 간주한다.
    - 두 checksum 값이 다르다면 데이터가 오염된 것으로 간주하고, 다른 failure domain들로부터 관련 데이터를 읽어와 데이터를 복구한다.
  - (3) 8개의 data chunk가 생길 때까지 (1), (2)를 반복한다. 이후 8개 data chunk로 데이터를 만들어내 클라이언트에게 반환한다.

### Metadata data model

### Bucket 내의 object들 조회

### Object versioning

### 대용량 파일 업로드 최적화

### Garbage collection

---
