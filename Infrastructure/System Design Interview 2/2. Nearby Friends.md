# Nearby Friends

- 이번 장에서는 가까운 친구들의 목록을 알려주는 _"Nearby friends"_ 기능을 포함한 확장 가능한 백엔드 시스템을 구현해보자.  
  위치 정보에 대한 접근 권한을 허가한 사용자들에 대해서 모바일 클라이언트는 지리적으로 가까운 친구들의 목록을 보여주게 된다.  
  예를 들어 Facebook에서도 이러한 기능을 제공한다.

- 이전에 본 Proximity Service와 상당히 유사하다는 것을 느낄 수 있겠지만, 자세히 생각해보면 이 둘은 꽤나 큰 차이점이 많다.  
  Proximity Service에서 각 장소의 위치 정보는 변하지 않기에 정적인 반면, Nearby Friends에서는 사용자의 위치가 자주 바뀌기 때문에  
  위치 정보가 더욱 동적이다.

## 문제 이해 및 설계 범위 확정

- 이번에 설계할 시스템의 요구사항은 아래와 같다.

  - 5miles 내에 있어야 _"nearby"_ 하다고 취급한다.
  - 사용자들 간의 거리는 일직선 상의 거리로 계산한다.
  - 사용자 수는 1억 명이며, 이 중 10%만이 Nearby Friends 기능을 사용한다.
  - 위치 정보는 머신 러닝 등을 위해 저장되어야 한다.
  - 10분 이상 활동을 하지 않는 사용자는 인접 친구 목록에 뜨지 않는다.
  - 개인 정보 보호 법 등의 법규에 대한 신경은 쓰지 않아도 된다.

- 추가적으로 아래의 요구 사항들이 있다.

  - 사용자는 모바일 애플리케이션 상에서 가까운 친구들의 목록을 볼 수 있어야 한다. 각 친구들에 대해서는 일직선 상의 거리와 마지막으로 거리가  
    갱신된 timestamp를 나타내준다.
  - 가까운 친구들의 목록은 몇 초 마다 갱신되어야 한다.
  - Low latency: 친구들의 위치 정보를 빠르게 갱신하기 위해 지연 시간이 낮아야 한다.
  - Reliability: 시스템은 전체적으로 안정적이어야 하고, 가끔 어쩔 수 없는 데이터의 손실은 허용된다.
  - Eventual Consistency: 위치 정보의 저장소는 강한 일관성(strong consistency)를 필요로 하지 않는다.  
    다른 replica들로부터 위치 정보를 조회해 발생하는 몇 초의 delay는 허용된다.

- 이제 위의 요구사항들에 기반해 필요한 정보들을 수치화해보자.

  - 가까운 친구들은 5 mile 반경 내에 있는 친구들로 정의된다.
  - 사용자의 위치 정보는 매 30초마다 갱신된다. 30초라는 숫자가 나온 이유는 사람의 걷는 속도가 느리기 때문이다.  
    30초 동안 걷는다고 했을 때, 걷기 전의 위치와 30초 후의 위치는 그렇게 큰 차이가 없다.
  - 평균적으로 매일 1000만 명의 사용자가 "Nearby Friends" 기능을 사용한다.
  - 동시 접속자의 수는 DAU의 10%라고 해보자. 따라서 동시 접속자 수는 100만 명이다.
  - 평균적으로 한 사용자는 400명의 친구가 있다. 그리고 이 400명 모두가 "Nearby Friends" 기능을 사용한다 가정한다.
  - 앱은 한 페이지당 20명의 가까운 친구들을 표시하며, 추가적인 요청에 의해 더 많은 가까운 친구들을 불러올 수 있다.
  - QPS를 계산하는 과정은 아래와 같다.
    - DAU: 1000만 명
    - 동시 접속자 수: `1000만 * 10% = 100만`
    - 사용자는 자신의 위치 정보를 30초마다 갱신시킨다.
    - 위치 정보 갱신의 QPS는 `100만 / 30 = 334,000` 이다.

---

## 개략적 설계안 제시 및 동의 구하기

- 이번에는 아래의 내용들을 다뤄보자.

  - 개략적 설계
  - API 설계
  - 데이터 모델

- 보통 개략적 설계안을 생각하기 전에 API와 데이터 모델을 먼저 하지만, 이 시스템의 경우 사용자의 위치 정보를 모든 친구들에게 보내야(push)하기 때문에  
  client, server 사이의 평범한 HTTP Protocol을 사용하지 않을 수도 있다. 개략적 설계안을 이해하기 전에 API를 먼저 보면 더 이해가 어렵기에  
  개략적 설계안 부터 보자.

### 개략적 설계

- 개략적인 관점에서 봤을 때 이 시스템은 message를 쉽게 전달할 수 있어야 한다. 개념적으로 한 사용자는 모든 가까운 친구들의 위치 정보가 갱신되는 것을  
  알아야 한다. 이론적으로는 peer-to-peer로 구현 가능하다. Peer-to-peer에서는 아래 그림과 같이 사용자가 모든 가까운 친구들과  
  persistent한 connection을 맺는다.

![picture 1](/images/SDI2_NF_1.png)

- 하지만 배터리 소모량이 많고 가끔씩 connection이 끊기는 일이 발생하기에 Peer-to-peer 를 사용하는 방식은 여기에서는 적합하지 않다.

- 조금 더 괜찮은 방식은 아래처럼 Backend를 활용하는 방식일 것이다.

![picture 2](/images/SDI2_NF_2.png)

- 위 그림에서 Backend의 책임은 아래와 같다.

  - 모든 활성 사용자들의 위치 정보 갱신을 처리한다.
  - 위치 정보 갱신을 처리할 때마다 이 정보를 받아야할 해당 사용자의 가까운 활성 친구들에게 전달한다.
    - 활성 친구들 중 가까운 친구들(이 시스템에서는 5 miles 이내)에게만 전달한다.

- 위 아키텍쳐를 사용하는 것은 꽤나 간단해 보인다. 무엇이 문제일까? 바로 쉽게 확장할 수 없다는 것이다.  
  이 시스템은 1000만 명의 활성 사용자가 있다. 각 사용자가 30초마다 자신의 위치 정보를 갱신하면 1초 당 약 334,000개의 갱신이 발생한다.  
  평균적으로 한 사용자는 400명의 친구가 있고, 이들 중 10%가 활성이며 가깝게 있다면 총 일어나는 위치 갱신은 `334,000 * 400 * 10% = 1400만`이다.

### 계략적 설계안

- 우선 이 시스템의 개략적 설계안을 보자. 이후에 확장을 쉽게 할 수 있도록 최적화를 진행해보자.

- 아래 그림은 요구사항을 어느 정도 만족하는 기본적인 설계안이다.

![picture 3](/images/SDI2_NF_3.png)

- 각 컴포넌트들을 하나씩 살펴보자.

#### Load Balancer

- Load balancer는 RESTful API 서버들과 stateful하며 양방향 통신을 제공하는 WebSocket 서버들 앞에 위치한다.  
  트래픽을 이들 서버로 분산시켜주는 역할을 담당한다.

#### RESTful API Servers

- RESTful API 서버는 일반적인 request/response 형태의 트래픽을 처리하는 stateless한 HTTP 서버들의 클러스터이다.  
  API 요청의 흐름은 아래 그림에서 확인할 수 있다. API layer에서 친구 추가, 삭제, 사용자 프로필 갱신 등을 처리한다.  
  이런 요청은 매우 흔하므로 더 깊게 살펴보지 않겠다.

![picture 4](/images/SDI2_NF_4.png)

#### WebSocket Servers

- WebSocket 서버는 친구들의 위치 정보 갱신을 거의 실시간으로 처리해내는 stateful 서버들의 클러스터이다. 각 클라이언트는 이 서버들 중 하나의  
  서버와 persistent한 WebSocket connection을 맺고 있다. 검색 반경 내에 있는 친구로부터 위치 정보 갱신이 발생하면, 이 갱신된 정보가  
  클라이언트에게 해당 connection으로 전달된다.

- WebSocket server의 또다른 큰 책임은 "Nearby Friends" 기능을 사용하기 위해 클라이언트의 초기화를 처리하는 것이다.  
  즉 모든 가까운 활성 친구들의 목록을 클라이언트에게 초기에 전달해줘야 한다.

#### Redis Location Cache

- 여기서 Redis는 각 활성 사용자의 가장 최근 위치 정보를 저장하기 위해 사용된다. Cache의 entry 각각에는 TTL이 설정되고, 이 TTL이  
  만료되면 해당 사용자는 더 이상 활성으로 간주되지 않고 위치 정보가 cache에서 제거된다. 위치 정보의 갱신은 TTL도 함께 갱신한다.  
  TTL이 핵심이기에 TTL 기능을 제공하는 다른 Key-Value store도 Redis 대신 사용 가능하다.

#### User Database

- User database는 사용자 정보와 사용자의 친구 관계를 저장한다. RDB 또는 NoSQL Database 모두가 사용될 수 있다.

#### Location History Database

- Location history database는 사용자의 위치 정보들을 기록한다. 이 컴포넌트는 "Nearby Friends" 기능과 직접적으로 연관되지는 않는다.

#### Redis Pub/Sub Server

- Redis Pub/Sub는 굉장히 가벼운 message bus이다. Redis Pub/Sub은 Channel을 통해 message들을 주고 받는데, 이 Channel을 만드는  
  비용도 매우 저렴하다. GB 단위의 메모리를 사용하는 최신 Redis server는 수천만개의 channel을 관리할 수 있다.  
  아래 그림은 Redis Pub/Sub이 어떻게 동작하는지를 개략적으로 나타낸다.

![picture 5](/images/SDI2_NF_5.png)

- 이 설계에서 WebSocket server가 수신한 위치 정보 갱신은 Redis Pub/Sub server를 사용해 해당 사용자의 Channel로 전달된다.  
  이 channel에 구독하는 것은 해당 사용자의 친구가 WebSocket server와 맺은 connection을 통해 WebSocket server가 담당한다.  
  위치 정보 갱신이 발생하면 WebSocket의 handler가 호출되고, 각 활성 친구들에 대해 함수가 일직선 상의 거리를 다시 계산한다.  
  새롭게 계산된 거리가 검색 반경(5 miles) 내에 있으면 새로운 위치 정보와 timestamp가 WebSocket을 통해 각 친구의 단말로 전송된다.  
  꼭 Redis Pub/Sub가 아닌 다른 경량화된 channel과 message bus를 제공하는 솔루션도 사용 가능하다.

#### 주기적인 위치 정보 갱신 처리 흐름

- 주기적으로 위치 정보가 갱신될 때의 흐름을 조금 더 자세히 살펴보자. 기본적으로 모바일 클라이언트는 WebSocket server와의 persistent한  
  connection 상으로 주기적으로 갱신되는 위치 정보를 보낸다. 아래 그림은 전체적인 흐름을 나타낸다.

![picture 6](/images/SDI2_NF_6.png)

- (1) 모바일 클라이언트가 위치 정보 갱신 요청을 Load Balancer에게 보낸다.
- (2) Load Balancer는 위치 정보 갱신 요청을 해당 클라이언트가 WebSocket과 수립한 persistent connection을 사용해 WebSocket  
  Server로 전달한다.
- (3) WebSocket server는 위치 정보를 Location History Database에 저장한다.
- (4) WebSocket Server는 Location Cache에 해당 사용자의 위치 정보를 갱신된 정보로 업데이트한다.  
  이 갱신 작업은 해당 entry의 TTL도 함께 갱신시킨다. 이후 WebSocket Server는 새로운 위치 정보를 거리 계산 과정의 최적화를 위해  
  해당 사용자가 사용하는 WebSocket connection에 특정 변수로 저장한다.
- (5) WebSocket server는 새로운 위치 정보를 해당 사용자의 Redis Pub/Sub Server channel로 publish한다.  
  (3) ~ (5)는 병렬적으로 수행될 수 있다.
- (6) Redis Pub/Sub Server의 channel이 새로운 위치 정보를 수신하면 이 갱신 작업을 해당 channel의 모든 구독자(subscriber)들에게  
  전달한다. 이 경우, 구독자들은 사용자의 활성인 친구들이다. 각 구독자의 WebSocket handler가 위치 정보 갱신을 처리한다.
- (7) 메시지를 수신하면 connection handler가 위치한 WebSocket server가 구독자(활성 친구)와 위치 정보를 갱신한 사용자와의 거리를  
  계산한다. 구독자의 위치 정보는 WebSocket connection handler의 변수에 저장되어 있고, 사용자의 새로운 위치 정보는 message에 담겨 있다.
- (8) 이 단계는 그림에는 없다. 사용자와 구독자의 거리가 검색 반경(5 miles)을 초과하지 않는다면 새로운 위치 정보와 갱신된 timestamp가  
  구독자의 클라이언트(단말)로 전달되며, 초과한다면 이 단계는 수행되지 않는다.

- 이 모든 과정을 이해하는 것이 매우 중요하기에 더 구체적인 예시를 통해 살펴보자.

- 시작하기 전, 몇 가지 가정을 하고 넘어가자.
  - `User 1`의 친구들은 `User 2`, `User 3`, 그리고 `User 4`이다.
  - `User 5`의 친구들은 `User 4`와 `User 6`이다.

![picture 7](/images/SDI2_NF_7.png)

- (1) `User 1`의 위치가 변경되면 `User 1`과의 connection을 맺고 있는 WebSocket server로 위치 정보 갱신 요청이 전달된다.
- (2) 위치 정보는 Redis Pub/Sub server에서 `User 1`이 사용하는 channel로 전달된다.
- (3) Redis Pub/Sub server는 해당 channel을 구독 중인 모든 subscriber들에게 갱신된 위치 정보를 전달한다.  
  여기서 subscriber들은 `User 1`의 친구들이 WebSocket server와 맺은 connection이다.
- (4) 위치 정보를 보내는 사용자(`User 1`)와 구독자 각각에 대해 거리를 계산하고, 이 거리가 특정 범위(5 miles)를 넘지 않는다면  
  새로운 위치가 구독자의 클라이언트에게 전달된다. 이 경우 `User 2`만 거리가 범위 내에 있다고 하자.

- 위 과정은 channel의 모든 구독자들에 대해 수행된다. 초기에 요구사항에서 각 사용자는 평균적으로 400명의 친구가 있고, 이들 중 10%가  
  활성이며 "Nearby Friends" 기능을 사용한다고 가정했기에 한 사용자가 위치 정보를 갱신하면 위 과정은 40번 수행될 것이다.

### API 설계

- 개략적 설계안을 살펴보았으니, 필요한 API들을 보자.

#### WebSocket

- 사용자는 WebSocket protocol을 사용해 위치 정보 갱신을 보내고, 수신한다.  
  최소한으로 아래의 API들이 필요할 것이다.

  - 주기적 위치 정보 갱신
    - Request: 클라이언트가 위도, 경도와 timestamp를 보낸다.
    - Response: Nothing
  - 클라이언트가 위치 정보 갱신을 수신
    - 전달되는 데이터: 친구의 위치 정보와 timestamp
  - WebSocket 초기화 과정
    - Request: 클라이언트가 위도, 경도와 timestamp를 보낸다.
    - Response: 클라이언트가 친구들의 위치 정보를 받는다.
  - 새로운 친구 구독
    - Request: WebSocket server가 친구의 ID를 보낸다.
    - Response: 해당 친구의 가장 최근 위도, 경도 그리고 timestamp
  - 친구 구독 취소
    - Request: WebSocket server가 친구의 ID를 보낸다.
    - Response: Nothing

#### HTTP Requests

- API Server는 친구 추가, 삭제, 사용자 프로필 갱신 등의 요청을 처리한다.

### 데이터 모델

- 데이터 모델 또한 꽤나 고려해야 하는 요소이다. User Database에 대해서는 개략적 설계 부분에서 살펴보았으니 여기서는 Location Cache와  
  Location History Database를 중점적으로 다뤄보자.

#### Location Cache

- Location Cache는 활성 사용자 중 Nearby Friends 기능이 켜져 있는 모든 사용자들의 가장 최근 위치 정보를 저장한다.  
  여기서는 Redis를 사용하기로 했다. 아래는 Redis에 들어가는 key-value pair의 구조이다.

|   key   |              value               |
| :-----: | :------------------------------: |
| user_id | {latitude, longitude, timestamp} |

##### 왜 사용자 정보를 저장하는 데 DB를 사용하지 않을까?

- Nearby Friends 기능은 오로지 특정 사용자의 **현재 위치**만을 필요로 한다. 따라서 한 사용자 당 하나의 위치 정보만 저장하면 된다.  
  이때 Redis는 굉장히 빠른 read, write 연산 속도를 제공하기 때문에 아주 좋은 선택지가 될 것이다. 또한 TTL도 제공하기에  
  cache에 위치 정보가 없는 사용자를 비활성 사용자로 처리하게끔 할 수도 있다. 가장 최근 위치 정보는 꼭 내구성 있게 저장될 필요가 없다.  
  만약 Redis 인스턴스에 장애가 난다면 단순히 아무런 정보가 없는 새로운 인스턴스로 교체하고, 새로운 위치 정보가 오면 해당 정보들로  
  cache를 채우도록 하면 된다. 물론 Redis에 장애가 나면 활성 사용자들은 1 또는 2번의 주기 동안 위치 정보를 못 불러올 수 있다.  
  하지만 이는 납득 가능한 tradeoff이다.

#### Location History Database

- Location History Database는 사용자의 모든 위치 정보들을 저장한다. 스키마는 아래와 같다.

| user_id | latitude | longitude | timestamp |
| :-----: | :------: | :-------: | :-------: |

- 여기에 사용할 데이터베이스는 많은 write 연산을 처리할 수 있고 수평적 확장이 가능해야 한다. Cassandra가 좋은 선택지가 될 것이다.  
  RDB를 사용할 수도 있지만, 모든 위치 정보를 한 대의 RDB가 처리하기는 어려울 수 있으므로 데이터를 sharding해야 한다.  
  가장 기본적인 접근 법은 사용자 ID로 sharding을 하는 것이다. 이렇게 sharding하면 부하가 shard들로 골고루 분산됨이 보장되고,  
  유지보수하기도 쉽다.

---

## 상세 설계

- 개략적 설계안에서 설계한 시스템은 대부분의 경우 잘 동작하겠지만, 이 시스템의 요구사항을 만족시키기엔 조금 부족할 수 있다.  
  이번에는 scale을 높이면서 마주칠 수 있는 병목 현상들을 생각해보고, 그 병목 지점들을 어떻게 처리해갈지 자세히 살펴보자.

### 각 컴포넌트가 얼마나 쉽게 확장할 수 있는가?

#### API Servers

- RESTful API tier를 확장하기 위한 방법들은 매우 많다. 이들은 모두 stateless한 서버들이고 CPU 사용량, 부하, 또는 IO 등을 기준으로  
  클러스터를 auto-scale하는 방법 또한 매우 많다.

#### WebSocket Servers

- WebSocket cluster는 사용량에 따라 auto scaling하기 어렵지 않다. 하지만 WebSocket server들은 stateful하므로 존재하는 node를  
  삭제할 때 주의 깊게 처리해야 한다.

- 특정 node가 제거되기 전에, 해당 node에 붙어 있는 connection들이 끊겨질 수 있는 상황이 되어야 한다.(connection drain)  
  이를 구현하기 위해 해당 node를 _"draining"_ 상태로 변경해 load balancer가 새로운 WebSocket connection을 해당 node와  
  수립하지 않도록 할 수 있다. 해당 node에 수립된 모든 connection들이 닫히면, 그제서야 해당 node는 삭제될 수 있다.

- Node 삭제 뿐만 아니라 WebSocket server를 새롭게 배포할 때도 동일한 로직이 적용되어야 한다.

- 즉 stateful server들의 원활한 auto scaling을 수행하려면 **Load Balancer가 이를 잘 처리해야 한다.**  
  대부분의 cloud load balancer들은 이를 매우 잘 처리한다.

##### Client Initialization

- 모바일 클라이언트는 실행 시 WebSocket server들 중 하나와 persistent한 WebSocket connection을 수립한다. 각 connection은 오랫동안  
  유지되고 사용된다. 대부분의 현대 프로그래밍 언어들은 오랫동안 사용되는 connection을 많이, 그리고 적은 메모리로 잘 관리할 수 있도록 한다.

- WebSocket connection이 초기화되면 클라이언트는 해당 사용자의 초기 위치 정보를 보내고, 이 정보를 받은 WebSocket connection handler는  
  아래의 작업들을 수행한다.

  - (1) Location Cache에서 사용자의 위치 정보 갱신
  - (2) 거리 계산을 용이하게 하기 위해 connection handler의 변수에 사용자의 위치를 저장한다.
  - (3) User Database에서 사용자의 모든 친구들을 불러온다.
  - (4) (3)에서 찾은 모든 친구들의 위치 정보를 조회하는 batch request를 Location Cache에 보낸다.  
    Location Cache의 각 entry에는 TTL이 있고, 만약 친구의 위치 정보가 Location Cache에 없다면 해당 친구는 비활성 상태인 것이다.
  - (5) Location Cache로부터 반환된 각 위치 정보에 대해 해당 사용자와 친구의 거리를 계산한다. 만약 거리가 주어진 요구사항 내에 있다면  
    친구 정보, 위치 정보, 그리고 마지막 갱신된 timestamp가 WebSocket connection을 통해 client에게 전달된다.
  - (6) 각 친구에 대해 서버는 Redis Pub/Sub server에 있는 친구의 channel에 subscribe(구독)한다.  
    새로운 channel을 만드는 비용이 매우 적으니, 사용자는 활성이든 비활성이든 관계없이 모든 친구들의 channel에 구독한다.  
    비활성 상태인 친구들은 Redis Pub/Sub Server에서 적은 양의 메모리를 차지하겠지만, 활성이 되기 전까지 CPU나 I/O는 소모하지 않는다.
  - (7) 사용자의 현재 위치를 Redis Pub/Sub Server에 있는 해당 사용자의 channel에 publish한다.

#### User Database

- User Database는 크게 두 가지 데이터를 저장한다.

  - 사용자 프로필(user ID, username, profile URL 등)
  - 친구 관계

- 이 데이터는 한 대의 RDB instance로 쉽게 확장하기 어렵다. 대신 User ID를 기준으로 sharding을 수행하면 된다.  
  RDB Sharding은 매우 흔히 사용되는 기술이다.

- 추가적으로 사용자와 친구 관계 데이터는 특정 팀에 의해 관리되고, 내부 API(Internal API)로 조회할 수 있을 수도 있다.  
  이전에 WebSocket Server가 사용자 정보와 친구들을 불러오기 위해 데이터베이스에 직접 query한다고 했는데, 이 대신 internal API를  
  호출할 수도 있을 것이다. Internal API를 사용하든 데이터베이스에 직접 query하든 성능이나 요구사항 만족 면에서는 아무런 차이가 없다.

#### Location Cache

- 이전에 모든 활성 사용자의 위치 정보를 저장하기 위해 Redis를 사용한다고 했다. 각 entry(key)에는 TTL이 지정된다.  
  그리고 위치 정보가 갱신되면 TTL도 함께 갱신된다. 이는 Redis가 사용할 수 있는 최대 메모리에 제한을 거는 것과 마찬가지 효과를 발생시킨다.  
  최대 1000만 사용자가 있고, 각 사용자의 위치 정보가 100byte를 차지한다 했을 때 GB 단위의 메모리를 사용하는 최신 Redis Server는  
  이를 쉽게 처리할 수 있을 것이다.

- 하지만 peak에는 1000만 사용자가 자신의 위치 정보를 30초마다 갱신시키게 된다. 즉 Redis server가 1초당 약 334,000건의 요청을  
  처리해야할 수도 있다는 것이다. 이는 최신 Redis를 사용하더라도 부하를 발생시킬 수 있다. 이럴 때는 cache data도 sharding하면 된다.  
  사용자의 위치 정보들은 서로 독립적이기 때문에 User ID를 기반으로 쉽게 여러 대의 Redis Server들로 정보를 분산시킬 수 있다.

- 가용성을 향상시키기 위해 각 shard에 저장된 위치 정보들을 standby node에 복제시킬 수도 있다.  
  이렇게 하면 Primary node가 죽으면 standby node가 재빠르게 primary로 승급되어 downtime을 최소화할 수 있을 것이다.

#### Redis Pub/Sub Server

- Redis Pub/Sub Server는 위치 정보 갱신 시 발생하는 message를 한 사용자로부터 해당 사용자의 활성 친구들에게 보내기 위한  
  routing layer로써 사용된다. 이전에 봤듯이 여기서는 새로운 channel을 만드는 비용이 굉장히 저렵하기에 Redis Pub/Sub Server를  
  사용하기로 결정했다. 새로운 channel은 어떤 사람이 구독을 하면 생기게 된다. 만약 메시지가 channel에 publish는 되었지만  
  subscriber가 1명도 없다면 해당 message는 단순히 무시되고, 서버에 아주 작은 부분만을 사용하게 된다. 새로운 channel이 생성되면  
  Redis는 hash table과 linked list를 사용해 subscriber들을 관리하기 위해 적은 양의 메모리를 사용하게 된다.  
  만약 사용자가 비활성 상태여서 channel에 아무런 작업이 없다면 channel이 생성된 이후 사용되는 CPU cycle 또한 없다.  
  이는 이 시스템을 설계할 때 아래의 장점들을 가져다준다.

  - (1) 설계안에서는 "Nearby Friends" 기능을 사용하는 모든 사용자들에게 각각 고유의 channel을 부여한다.  
    해당 사용자는 애플리케이션이 초기화되면 친구들 각각의 channel에 친구가 활성이든 비활성이든 subscribe하게 된다.  
    이는 친구의 활성 상태에 따라 해당 친구의 channel에 subscribe 할지 말지, 그리고 친구가 비활성 상태가 되면 unsubscribe할지  
    등을 서버가 구현해야 하는 일을 없앤다.

  - (2) 대신 이 방법대로 사용하면 memory를 더 많이 사용하게 된다. 이후에 보겠지만, 주로 메모리는 병목 지점이 되지 않는다.  
    이 경우에는 더 간단한 아키텍쳐를 위해 메모리를 조금 더 사용하는 것이 아주 괜찮은 선택지이다.

##### Redis Pub/Sub Server가 얼마나 필요할까?

- 메모리 사용량

  - 이전에 Nearby Friends를 사용하는 사람은 1000만 명으로 가정했으니, 이들 각각에 대해 고유의 channel을 부여하면 총 1000만 개의  
    channel이 필요할 것이다. 한 사용자 당 이 기능을 사용하는 친구가 100명이 있다고 가정하고, 각 subscriber를 관리하기 위한  
    내부적인 hash table과 linked list는 대략 20byte의 포인터를 사용하므로 대략적으로 `1000만 * 20bytes * 100명 친구 / 10^9`  
    `= 200GB` 정도의 메모리를 사용할 것이다. 100GB의 메모리를 사용하는 최신 서버를 사용한다면, 대략 2개의 Redis Pub/Sub Server를  
    사용하면 모든 channel을 처리할 수 있을 것이다.

- CPU 사용량

  - 이전에 계산했듯이 Redis Pub/Sub Server는 초당 1400만 건의 갱신을 subscriber들에게 보낸다. 물론 구체적인 benchmarking을  
    하지 않고 최신 Redis 서버가 1초에 얼마나 많은 메시지를 보낼 수 있는지를 알아내는 것은 어렵겠지만, 이 정도를 한 대의 Redis 서버가  
    처리하기 어렵겠다고 간주하면 된다. 보수적으로 생각해 Gigabit network를 사용하는 최신 Redis 서버 1대가 초당 100,000건을  
    subscriber에게 보낼 수 있다고 해보자. 여기서 사용할 위치 정보 갱신 메시지의 크기를 생각하면 100,000이라는 숫자는 매우 보수적인게 맞다.  
    이 보수적인 추정치를 갖고 생각해보면 1400만 건의 요청을 처리하려면 `1400만 / 10만 = 140`대의 Redis 서버가 필요해 보인다.  
    다시 말하지만 이 수치는 매우 보수적이므로 실제로 필요한 Redis 서버 수는 140대 보단 적을 것이다.

- 지금까지의 간단한 계산 결과로 봤을 때 아래의 결과를 얻을 수 있다.

  - Redis Pub/Sub Server의 주된 병목 지점은 메모리 사용량이 아니라 CPU 사용량이다.
  - 이 시스템을 안정하게 운영하기 위해서는 분산된 Redis Pub/Sub Cluster가 필요하다.

- Distributed Redis Pub/Sub Server Cluster

  - 수많은 channel들을 어떻게 100개가 넘는 Redis 서버들에 분산시킬 수 있을까? 좋은 점은 각 channel은 서로로부터 독립적이라는 것이다.  
    이는 publisher의 user ID를 기준으로 channel들을 여러 대의 Redis Pub/Sub Server들로 sharding하기 편하게 해준다.  
    하지만 실세계의 대용량 시스템의 장애는 불가피하고, 이를 생각했을 때는 다른 방안을 더 생각해봐야 한다.

  - 이런 경우에 service discovery component를 도입해볼 수 있다. Service discovery를 위한 컴포넌트로는 대표적으로 etcd, ZooKeeper  
    등 많은 서비스가 있다. 지금 service discovery component가 필요한 이유는 아주 단순한데, 아래의 두 가지 기능을 사용하기 위함이다.

    - (1) 서버들의 목록을 service discovery component에 저장하고, 간단한 UI 또는 API로 해당 목록을 수정할 수 있어야 한다.  
      기본적으로 service discovery는 설정 데이터들을 담는 작은 key-value store에 불과하다. 아래 그림을 예시로 하면 hash ring의  
      key, value는 아래와 같을 것이다.

      ```
      Key: /config/pub_sub_ring
      Value: ["p_1", "p_2", "p_3", "p_4"]
      ```

    - (2) _"Value"_ (Redis Pub/Sub Server들)에 변경이 생겼을 때 클라이언트(WebSocket Server)가 변경된 것에 subscribe할  
      수 있어야 한다.

  - 위의 (1)에서 봤듯이 모든 사용 가능한 Redis Pub/Sub server들을 담은 hash ring을 service discovery component에 저장한다.  
    이후 hash ring은 Redis Pub/Sub server의 publisher와 subscriber가 특정 channel을 사용하기 위해 어떤 Pub/Sub server를 사용할지  
    결정하는 데에 사용된다. 예를 들어 아래 그림에서 `channel 2`는 Redis Pub/Sub Server 1에 존재한다.

    ![picture 8](/images/SDI2_NF_8.png)

  - 아래 그림은 WebSocket server가 특정 사용자의 channel에 위치 정보 갱신을 publish할 때 발생하는 일을 나타낸다.

    ![picture 9](/images/SDI2_NF_9.png)

    - (1) WebSocket server는 hash ring과 소통해 어떤 Redis Pub/Sub Server에 publish해야 하는지를 결정한다.  
       Service discovery에 저장된 정보를 활용할 수도 있겠지만 효율성을 위해 각 WebSocket server에 hash ring의 복사본을  
       caching할 수 있을 것이다. 그럼 WebSocket Server는 hash ring에 변경 사항이 발생하면 이를 반영하기 위해 hash ring에  
       구독하게 된다.

    - (2) WebSocket server는 위치 정보 갱신을 해당 사용자의 channel이 존재하는 Redis Pub/Sub Server에 publish한다.

- Redis Pub/Sub Server 확장 시 고려해야하는 사항

  - Redis Pub/Sub Server Cluster를 어떻게 확장해야 할까? 트래픽 패턴에 따라 scale up/down을 가능하도록 해야할까?  
    이렇게 트래픽에 따라 scale up/down을 하는 패턴은 위험 부담이 적고 비용을 절감시켜주기에 stateless server들에 대해 매우 흔히  
    사용되는 패턴이다. 이 질문에 답하기 위해 Redis Pub/Sub Server Cluster의 속성 몇 개를 살펴보자.

    - (1) Pub/Sub channel에 전송된 message는 메모리나 디스크에 보관되지 않고, 바로 해당 channel의 모든 subscriber들에게 전달된 후  
      삭제된다. 만약 subscriber가 하나도 없다면 message는 단순히 무시된다. 따라서 Pub/Sub channel을 통해 오가는 데이터는 statelss하다고  
      할 수 있다.

    - (2) 하지만 channel을 위해 Pub/Sub server에 저장되는 상태값들이 있긴 하다. 예를 들어 특정 channel의 subscriber들의 목록은  
      당연히 Pub/Sub server에 의해 관리되어야 하는 상태값일 것이다. 특정 channel이 있던 Pub/Sub server가 교체되거나 이전 서버가  
      제거되고 새로운 서버가 추가되는 등의 상황에 의해 channel이 이동하면 해당 channel의 모든 subscriber들은 이런 사실을 알아서  
      기존(이전) 서버에 있던 channel로부터 unsubscribe하고 새로운 서버의 channel에 subscribe할 수 있어야 한다.  
      이렇게 봤을 때 Pub/Sub server는 stateful하며 서비스를 원활히 사용하도록 하기 위해 서버의 subscriber들은 적절히 관리(orchestrate)  
      되어야 한다.

  - 이러한 이유들로 인해 Redis Pub/Sub Cluster는 Storage Cluster를 다룰 때와 마찬가지로 stateful하게 취급되어야 한다.  
    Stateful cluster에 대해서 scale up/down 하는 것은 운영 오버헤드와 위험성이 존재하기 때문에 조심히 이뤄저야 한다.  
    Cluster 자체는 보통 daily peak 트래픽을 원활히 처리할 수 있도록 필요량보다 조금 더 좋은 스펙으로 provisioning된다.

  - 만약 꼭 scale을 해야 한다면, 아래의 잠재적인 문제점들을 꼭 상기해야 한다.

    - Cluster의 크기를 재조정하면 많은 channel들이 hash ring 상의 다른 서버들로 이동될 것이다. Service discovery component가  
      hash ring이 갱신됨을 모든 WebSocket server들에 통지하면 수많은 resubscription 요청이 발생할 것이다.

    - 이런 무수히 많은 resubscription 요청이 발생하면 클라이언트 중 일부의 위치 정보 갱신 요청이 제대로 처리되지 못할 수 있다.  
      이 시스템의 경우 요청이 간혹 처리되지 못하는 것이 용납되지만, 여전히 이런 현상을 줄일 수 있도록 생각해야 한다.

    - 잠재적인 방해 요소들로 인해 cluster 크기의 재조정은 시스템 사용량이 가장 적은 시간대에 수행되어야 한다.

  - 그렇다면 cluster 크기의 재조정이 어떻게 진행될까? 생각보다 간단한데, 아래의 단계들로 구성된다.

    - (1) 새로운 ring 크기를 결정하고 만약 scale up을 수행한다면 충분히 많은 서버를 새로 provision한다.
    - (2) 새로운 서버들로 hash ring의 내용을 갱신한다.
    - (3) 모니터링을 수행한다. WebSocket cluster의 CPU usage에 spike가 나타날 것이다.

  - 위 그림의 hash ring을 다시 참고해 만약 새로운 node를 2개 추가한다면, hash ring은 아래처럼 갱신될 것이다.
    ```
    Old: ["p_1", "p_2", "p_3", "p_4"]
    New: ["p_1", "p_2", "p_3", "p_4", "p_5", "p_6"]
    ```

- Redis Pub/Sub Server 운영 시 고려해야하는 사항

  - 존재하는 Redis Pub/Sub server를 교체할 때 발생하는 운영적인 위험성은 확장 시 고려해야 할 위험성보다 매우 적다.  
    우선 많은 개수의 channel들이 이동해야 할 필요가 없다. 해당 서버에서 사용하던 channel들만 새로운 서버로 이동되면 되기 때문이다.  
    이것이 좋은 이유는 서버를 교체해야 하는 상황은 어쩔 수 없이 발생하기 때문이다.

  - Pub/Sub server가 죽으면 모니터링 서비스는 담당자에게 이를 알려야할 것이다. 정확히 어떻게 모니터링 도구가 Pub/Sub server의  
    health를 check하는지는 이 장의 범위를 넘어서기 때문에 다루지 않는다. 담당자는 service discovery component의 hash ring을  
    죽은 node를 새로운 standby node로 교체함으로써 갱신시킨다. 이후 WebSocket server들이 hash ring의 변경에 대해 통지받고,  
    이후 각 connection handler가 자신의 channel을 새로운 Pub/Sub server로 resubscribe하도록 한다. 각 WebSocket handler는  
    자신이 subscribe한 모든 channel의 목록을 갖고 있으며, 각 channel을 hash ring에 대해 검사함으로써 channel이 새로운 서버로  
    resubscribe되어야 하는지 유무를 확인한다.

  - 위 그림의 hash ring을 다시 생각했을 때 만약 `p_1`이 죽고 이를 `p_1_new`로 교체한다면 hash ring은 아래와 같이 변경될 것이다.

    ```
    Old: ["p_1", "p_2", "p_3", "p_4"]
    New: ["p_1_new", "p_2", "p_3", "p_4"]
    ```

    ![picture 10](/images/SDI2_NF_10.png)

- 친구 추가 및 삭제

  - 사용자가 친구를 추가하거나 삭제하면 클라이언트는 무엇을 해야할까? 새로운 친구가 추가되면 해당 클라이언트의 WebSocket handler는 이에 대해  
    통지받아야 하고, 통지받으면 새로운 친구의 Pub/Sub channel에 subscribe하게 된다.

  - "Nearby Friends" 기능이 꽤나 사용자가 큰 애플리케이션들에서 많이 제공되기 때문에 "Nearby Friends" 기능이 새로운 친구가 추가될 때마다  
    모바일 클라이언트에 callback을 등록할 수 있다고 할 수 있을 것이다. Callback은 호출되면 WebSocket server에게 새로운 친구의  
    Pub/Sub channel에 구독하라는 메시지를 보낸다. 이후 만약 새로운 친구가 활성 상태라면 WebSocket Server는 새로운 친구의 최근 위치 정보와  
    timestamp를 반환할 수 있을 것이다.

  - 비슷하게 클라이언트는 친구가 삭제되었을 때도 애플리케이션에 callback을 등록할 수 있을 것이다. 이 callback은 호출되면 삭제된 친구의  
    Pub/Sub channel로부터 unsubscribe하라는 메시지를 WebSocket server에 보낼 것이다.

  - 이 subscribe/unsubscribe callback은 친구가 위치 정보를 동의하지 않아 "Nearby Friends" 기능을 사용하지 않게 되거나, 동의해  
    "Nearby Friends" 기능을 사용하게 되었을 때도 동일하게 적용될 수 있다.

- 친구가 매우 많은 사용자

  - 친구가 매우 많은 사용자가 존재해 이 사용자가 시스템의 성능 hotspot을 발생시킬 수 있을지 생각해봐야 한다. 보통 추가 가능한 친구의 최대 수에는  
    제한이 있다.(Facebook의 경우 5000명 제한이 있다) 또한 친구 관계는 양방향이다. 유명인이 수백만의 팔로워를 보유하는 follower 모델을  
    다루는 것이 아니다.

  - 수백명의 친구가 있을 때 Pub/Sub subscriber들은 cluster 내의 여러 WebSocket 서버들로 분산되어 있을 것이다. 갱신 시의 부하는  
    여러 서버로 분산되므로 hotspot 문제를 일으킬 여지가 거의 없다.

  - 물론 친구가 많은 사용자는 해당 사용자의 channel이 있는 Pub/Sub server에 조금 더 많은 부하를 발생시킬 수 있다.  
    하지만 100개가 넘는 Pub/Sub server가 있기에 이런 _"whale"_ 사용자들은 여러 Pub/Sub server들로 분산될 것이고  
    조금 더 많은 부하는 전혀 서버에 무리를 주지 않을 것이다.

- 주변에 있는 random 사용자 보여주기

  - 이 부분은 시스템의 요구사항에는 없기에 추가적인 부분이다. 만약 위치 정보를 공유하는 친구가 아닌 근처에 있는 무작위의 사람들을 보여주고 싶다면  
    어떻게 해야할까?

  - 기존 설계를 활용하면서 이를 구현하는 방식으로는 Geohash를 사용하는 Pub/Sub channel의 pool을 추가하는 것이다.  
    아래 그림에서 볼 수 있듯이 특정 지역은 4개의 geohash grid로 분할되고, channel은 각 grid에 대해 생성된다.

    ![picture 11](/images/SDI2_NF_11.png)

  - 특정 grid에 있는 모든 사용자는 동일한 channel을 subscribe하게 된다. 9q8znd grid를 예시로 살펴보자.

    ![picture 12](/images/SDI2_NF_12.png)

    - (1) `User 2`가 위치를 갱신시키면 WebSocket connection handler는 새로운 위치의 geohash ID를 계산하고 해당 geohash에  
      대응하는 channel에 위치 정보를 보낸다.

    - (2) `User 2`를 제외한 같은 geohash channel에 subscribe하는 모든 사용자들은 위치 정보 갱신 메시지를 수신한다.

  - Geohash grid의 경계선에 가까이 위치한 사용자들을 처리하고 싶다면 모든 클라이언트가 8개의 둘러싸는 geohash grid까지  
    subscribe하도록 할 수 있다. 예를 들면 아래 그림처럼 9q8zn9에 있는 사용자는 이를 둘러싸는 8개 영역의 channel까지 subscribe한다.

    ![picture 13](/images/SDI2_NF_13.png)

- Redis Pub/Sub 의 대안

  - Routing layer에서 Redis Pub/Sub 대신 사용할 수 있는 대안들도 충분히 많다. 이 문제의 경우 Erlang이 매우 좋은 선택지가 된다.  
    물론 Erlang 자체가 Redis Pub/Sub 보다 좋다고 말할 수 있겠지만, Erlang은 약간 _틈새_ 시장 같은 느낌이 있고, 훌륭한 Erlang  
    개발자를 고용하는 것도 꽤나 어렵다. 하지만 만약 팀원들이 Erlang 경험이 있다면 좋은 선택지가 될 것이다.

  - 그래서 왜 Erlang이 좋은 선택지인 것일까? Erlang은 고도로 분산되고(highly distributed) 동시성을 활용하는 애플리케이션을  
    개발하는 데에 사용되는 주된 언어이다. 여기서 말하는 Erlang은 Erlang의 생태계를 의미한다. 이는 언어 컴포넌트(Erlang, Elixir),  
    런타임 환경(BEAM), Erlang 런타임 라이브러리(OTP)를 포함한다.

  - Erlang의 강력함은 경량화된 프로세스에서 비롯된다. Erlang process는 BEAM VM 상에서 동작하는 하나의 entity이다.  
    Erlang process는 Linux process보다 더 비용이 적게 만들 수 있다. Erlang process는 최소 300byte의 크기를 가지며  
    최신 서버에서는 수천만개의 프로세스를 사용해도 거뜬하다. 만약 Erlang process에 아무런 할 일이 없다면, 해당 프로세스는 CPU cycle을  
    전혀 사용하지 않으며 가만히 대기한다. 다르게 표현하면 이 애플리케이션의 천만 사용자 각각을 erlang process로 모델링해도 거뜬하다는 것이다.

  - Erlang은 또한 많은 Erlang 서버들로 분산하기 매우 쉽다. 운영 오버헤드는 매우 낮고, live production issue에 대응하기 위한 디버깅  
    툴들이 많으며 배포 도구 또한 매우 제공하는 기능들이 강력하다.

  - 그렇다면 이 설계에서 Erlang을 어떻게 사용할 수 있을까? WebSocket service를 Erlang으로 구현하고 Redis Pub/Sub cluster 전체를  
    분산된 Erlang 애플리케이션으로 교체할 수 있다. 이 애플리케이션에서 각 사용자는 Erlang process로 모델링된다. 사용자의 프로세스는  
    클라이언트에 의해 사용자의 위치가 갱신됨을 WebSocket server로부터 통지받는다. 또한 사용자 프로세스는 해당 사용자의 친구들의 프로세스를  
    구독함으로써 갱신 사항에 대해 통지받을 수 있다. Subscription 개념은 Erlang/OTP에 기본적으로 탑재되어 있으며 구현하기도 쉽다.

---

## 마무리

- 이번 장에서는 Nearby Friends(가까운 친구들을 보여주는) 기능을 제공하는 시스템을 설계해보았다.  
  기본적으로 한 사용자의 위치 정보 갱신을 해당 사용자의 친구들에게 어떻게 효율적으로 전달할 수 있는지에 중점을 두었다.

- 핵심 컴포넌트의 일부는 아래와 같다.

  - WebSocket: 클라이언트와 서버 사이의 실시간 양방향 통신 제공
  - Redis: 위치 데이터의 빠른 read, write 연산 속도 제공
  - Redis Pub/Sub: 한 사용자의 위치 정보 갱신을 해당 사용자의 모든 활성 친구들에게 알리기 위한 routing layer

- 처음에는 작은 규모를 지원하는 개략적 설계안을 살펴보았고, 규모를 확장하면서 발생할 수 있는 문제점들을 알아보고 해결해 나갔다.  
  이번 장에서는 아래의 것들을 확장하는 방법을 살펴보았다.

  - RESTful API servers
  - WebSocket servers
  - Data layer
  - Redis Pub/Sub servers
  - Redis Pub/Sub의 대안

- 마지막으로 특정 사용자가 많은 친구를 가질 때 발생할 수 있는 병목 현상들을 다뤘고, 주위에 있는 무작위의 사람들을 추천하는 기능도 살펴보았다.

---
