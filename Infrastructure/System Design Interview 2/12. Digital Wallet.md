# Digital Wallet

- 결제 플랫폼들은 일반적으로 클라이언트에게 돈을 저장하고 사용할 수 있도록 해주는 전자 지갑 서비스를 제공한다.  
  예를 들어 신용 카드에 연결된 전자 지갑에 돈을 저장하고, 이후 온라인으로 상품을 구매할 때 해당 전자 지갑에 있는 돈을 사용할 수 있다.  
  아래 그림은 이 프로세스를 나타낸다.

  ![picture 16](/images/SDI2_DW_1.png)

- 전자 지갑이 제공하는 기능은 돈을 사용하는 것이 전부가 아니다. PayPal과 같은 결제 플랫폼에서는 동일한 플랫폼에 있는 다른 사람의  
  전자 지갑으로 돈을 송금할 수도 있다. 은행 계좌로 송금하는 것에 비해 이 송금 방식은 더욱 빠르며 추가적인 비용(수수료)을 발생시키지 않는다.  
  아래 그림은 전자 지갑 사이의 송금 과정을 나타낸다.

  ![picture 17](/images/SDI2_DW_2.png)

- 이번 장에서는 같은 플랫폼 사이의 직접 송금이 가능한 전자 지갑 애플리케이션의 백엔드 부분을 설계해보도록 하자.

## 문제 이해 및 설계 범위 확정

- 요구사항은 아래와 같다.

  - 두 개의 전자 지갑 사이의 송금 기능에만 집중한다.
  - TPS: 1,000,000(백만)
  - 데이터를 기반으로 잔액 기록을 재생성해낼 수 있어야 한다.
  - 가용성: 99.99%
  - 안정성: 99.99%
  - 트랜잭션 지원
  - 외화 거래는 고려하지 않는다.

### 개략적 추정치 계산

- TPS를 사용할 때 일반적으로 트랜잭션을 지원하는 데이터베이스를 사용한다고 가정하게 된다. 현대의 일반적인 data center node에서  
  실행되는 RDBMS는 초당 수천개의 트랜잭션을(1000 TPS) 지원한다. 하나의 데이터베이스 node가 1000 TPS를 지원한다 해보자.  
  위 요구사항처럼 1,000,000 TPS를 지원하기 위해선 최소 이의 2배인 2,000,000 TPS까지는 지원할 수 있어야 하며, 이는 곧  
  2000개의 node들이 필요함을 의미한다.

- 아래 표는 1개의 node가 처리할 수 있는 TPS에 따라 필요한 총 node들의 개수를 나타낸다.  
  하드웨어가 동일하다고 가정했을 때 node가 초당 처리할 수 있는 트랜잭션의 수가 많으면 많을 수록 최종적으로 필요한 node의 개수는  
  줄어들며, 이는 곧 하드웨어 비용을 절감할 수 있음을 나타낸다. 따라서 우리는 이번 설계에서 하나의 node가 가능한 많은 TPS를  
  지원하도록 해야 한다.

  | node당 TPS | node 개수 |
  | ---------- | --------- |
  | 100        | 20000     |
  | 1000       | 2000      |
  | 10000      | 200       |

---

## 개략적 설계안 제시 및 동의 구하기

- 이번에는 아래의 내용들을 살펴볼 것이다.

  - API 설계
  - 3개의 개략적 설계안
    - in-memory를 사용한 방법
    - Database 기반의 분산 트랜잭션을 사용한 방법
    - Event-sourcing을 활용해 재현 가능성을 구현한 방법

### API 설계

- RESTful API 컨벤션을 사용하도록 하자. 여기서는 하나의 API만 지원하면 된다.

  | API                               | 설명                                        |
  | --------------------------------- | ------------------------------------------- |
  | `POST /v1/wallet/balance_transfer | 하나의 지갑으로부터 다른 지갑으로 송금한다. |

- 요청 파라미터는 아래와 같다.

  | 필드             | 설명                         | 타입   |
  | ---------------- | ---------------------------- | ------ |
  | `from_account`   | 송금자 계좌                  | string |
  | `to_account`     | 수취자 계좌                  | string |
  | `amount`         | 금액                         | string |
  | `currency`       | 통화                         | string |
  | `transaction_id` | 중복 제거를 위해 사용되는 ID | uuid   |

- 응답은 아래와 같다.

  ```json
  {
    "status": "success",
    "transaction_id": "0158998-2254-11ec-0493-3938ca403284"
  }
  ```

- 위에서 금액을 나타내는 `amount` 필드의 타입이 double이 아니라 string인데, 이렇게 한 이유는 이전 장에서 찾을 수 있다.

  > - `amount`가 double이면 아래와 같은 이유로 좋지 않다.
  >   - 직렬화, 역직렬화 과정이 프로토콜, 소프트웨어, 하드웨어에 따라 다를 수 있다. 이러한 차이로 인해 숫자 계산이 잘못될 수 있다.
  >   - 숫자가 매우 클 수 있거나 매우 작을 수도 있다.

- 실전에서 대부분의 사람들은 여전히 float나 double이 대부분의 프로그래밍 언어와 데이터베이스에서 지원한다는 이유로 사용한다.  
  하지만 이렇게 하면 precision(정밀도)가 낮아질 수 있다는 점을 감안해야 한다.

### In-memory sharding을 활용한 방법

- 지갑 애플리케이션은 모든 사용자 계좌마다 잔액 정보를 유지한다. 이렇게 `<user, balance>` 관계를 나타내기 좋은 자료구조로  
  map(hash table, hash map) 또는 key-value store를 활용할 수 있다.

- In-memory store 중 가장 유명한 선택지는 Redis이다. 하나의 Redis node로는 1,000,000 TPS를 감당할 수 없다.  
  따라서 Redis node들로 구성된 cluster를 생성하고, 사용자 계좌를 이 cluster 내의 node들로 고르게 분배해야 한다.  
  이 과정을 partitioning 또는 sharding이라 한다.

- Key-value 데이터를 _n_ 개의 partition들로 분산시키기 위해 key의 hash 값을 계산하고, _n_ 으로 나눌 수 있다.  
  그리고 나눈 나머지값이 해당 데이터가 저장될 partition이 된다. 아래는 이러한 sharding 과정을 나타낸 pseudo code이다.

  ```java
  String accountID = "A";
  int partitionNumber = 7;
  int myPartition = accountID.hashCode() % partitionNumber;
  ```

- Redis cluster 내 node들의 개수 및 각각의 주소는 중앙화된 공간에 저장될 수 있다. 예를 들어, 고가용성의 설정 저장소인 Zookeeper를 활용할 수 있다.

- 이렇게 in-memory sharding을 활용하는 방법의 마지막 컴포넌트는 송금 요청을 처리하는 서비스이다.  
  우리는 이 서비스를 Wallet service라 할 것이고, 이 서비스는 아래의 주요 책임을 가진다.

  - (1) 송금 요청 수신
  - (2) 송금 요청 검증
  - (3) 요청이 유효하면 송금인, 수취인의 잔액 정보를 갱신한다. Cluster 내에서 송금인과 수취인의 정보는 서로 다른 node에 저장되어 있을 수 있다.

- Wallet service는 stateless하다. 따라서 horizontal scaling(수평적 확장)이 매우 쉽다.  
  아래 그림은 in-memory sharding을 활용한 방법의 개략적 설계안을 나타낸다.

  ![picture 1](/images/SDI2_DW_3.png)

- 위 설계안에는 3개의 Redis node들이 존재한다. 그리고 A, B, C 고객의 정보가 node들에 고르게 분산되어 저장되어 있다.  
  또한 송금 요청을 처리하기 위해 2개의 Wallet service node들이 있다. 만약 하나의 Wallet service node가 $1을 A 계좌에서  
  B 계좌로 송금하라는 요청을 받으면, 이 요청을 처리하기 위해 2개의 Redis node와 상호작용해야 한다. 고객 A의 계좌 정보를 가지는  
  Redis node에서 Wallet service는 해당 계좌로부터 $1을 차감하고, 고객 B의 계좌에 $1을 입금한다.

- 하지만 이 설계는 매우 심각한 문제를 갖고 있다. 물론 동작은 하지만, Wallet service가 각 송금 과정에서 2개의 Redis node를 모두  
  정상적으로 갱신 처리할 수 있다는 보장이 없다. 만약 예를 들어 Wallet service node가 송금인의 계좌에 대한 갱신 작업을 마친 즉시  
  장애가 난다면, 수취인의 계좌 잔액은 변함이 없을 것이다. 이 두 작업은 모두 single atomic transaction(단일 원자성 트랜잭션) 내에서 수행되어야 한다.

### Database 기반의 분산 트랜잭션을 사용한 방법

#### Database sharding

- 2개의 서로 다른 node들에 대해 수행되는 작업이 어떻게 원자성을 갖도록 할 수 있을까?  
  첫 번째로 각 Redis node를 전통적인 RDBMS node로 바꿀 수 있을 것이다. 아래 그림은 이 설계를 보여준다.  
  이전과 달리 고객 A, B, C의 정보는 3개의 Redis node가 아닌 3개의 RDBMS node들에 분배되어 저장되어 있다.

  ![picture 2](/images/SDI2_DW_4.png)

- 이렇게 RDBMS를 사용하는 것은 문제의 일부분만을 해결한다. 이전에 봤듯이 하나의 송금 요청이 서로 다른 2개의 database node에 작업을  
  수행해야 하는 경우가 분명히 발생한다. 그리고 이러한 경우, 2개의 갱신 작업이 정확히 같은 시간에 정상적으로 처리될 것이라는 보장이 없다.  
  만약 송금인의 잔액 정보를 갱신한 후 Wallet service가 재시작하게 되었다면, 수취인의 잔액 정보를 이후 갱신하도록 어떻게 할 수 있을까?

#### Distributed transaction: 2PC Commit(Two-phase commit)

- 분산 시스템에서 하나의 트랜잭션은 여러 개의 node들에 거쳐 다양한 작업을 수행하게 될 수 있다. 트랜잭션이 원자성을 지니도록 하기 위해서  
  distributed transaction(분산 트랜잭션)을 활용할 수 있다. 분산 트랜잭션을 구현하는 방법으로는 low-level solution, 그리고  
  high-level solution의 2가지 방법이 있다. 이 둘 각각을 살펴보자.

- Low-level solution은 database 자체에 기대게 된다. 가장 일반적으로 활용되는 알고리즘은 2PC(two-phase commit)이다.  
  2PC는 이름에서 알 수 있듯이 2단계의 phase를 가진다.

  ![picture 3](/images/SDI2_DW_5.png)

- 각 과정을 살펴보자.

  - (1) Coordinator(이 경우 Wallet service)가 평소처럼 여러 개의 database들에 read, write 연산을 수행한다.  
    위 그림에서 알 수 있듯이 데이터베이스 A, C에 lock이 걸린다.

  - (2) 애플리케이션이 트랜잭션을 commit할 시점이 되면, coordinator는 연관된 모든 데이터베이스가 트랜잭션을 준비하도록 한다.

  - (3) 2번째 phase에서 coordinator는 연관된 모든 데이터베이스들의 응답을 모아 아래 작업을 수행한다.

    - (a) 모든 데이터베이스가 `yes`로 응답하면, coordinator는 모든 데이터베이스에게 트랜잭션을 commit하도록 요청한다.
    - (b) 하나의 데이터베이스라도 `no`라 응답하면, coordinator는 모든 데이터베이스에게 트랜잭션을 중단하도록 요청한다.

- 이 방법이 low-level solution이라 불리는 이유는 준비 단계(phase 1)를 구현하려면 데이터베이스 트랜잭션에 대해 특수한 설정이  
  들어가야 하기 때문이다. 예를 들어 서로 다른 데이터베이스가 2PC를 지원하도록 하기 위해 X/Open XA 표준이 존재한다.  
  2PC의 가장 큰 문제점은 모든 데이터베이스의 응답을 기다릴 때까지 데이터베이스에 lock이 걸리기 때문에 성능이 떨어진다는 점이다.  
  또다른 문제점으로 아래 그림처럼 coordinator가 SPOF가 될 수 있다.

  ![picture 4](/images/SDI2_DW_6.png)

#### Distributed transaction: TC/C(Try-Confirm/Cancel)

- TC/C는 보상 트랜잭션(compensating transaction)중 하나로 아래의 2개 단계로 구성되어 있다.

  - (1) 첫 번째 phase에서 coordinator는 모든 데이터베이스에게 transaction에 필요한 리소스를 예약할 것을 요청한다.
  - (2) 두 번째 phase에서 coordinator는 모든 데이터베이스의 응답을 모은다.

    - (a) 모든 데이터베이스가 `yes`로 응답하면, coordinator는 모든 데이터베이스에게 연산을 수행하도록 한다.  
      이 과정이 TC(Try-Confirm)에 해당한다.
    - (b) 하나의 데이터베이스라도 `no`라 응답하면, coordinator는 모든 데이터베이스에게 연산을 취소하라고 한다.  
      이 과정이 Try-Cancel에 해당한다.

- 참고로 2PC에서 2개 phase는 모두 하나의 트랜잭션에서 동작하는 반면, TC/C의 각 phase는 서로 다른 트랜잭션에서 수행된다.

##### TC/C 예시

- 계좌 A에서 계좌 C로 $1을 송금한다고 가정해보자. 아래 표는 TC/C가 각 phase에서 수행하는 연산을 보여준다.

  ![picture 5](/images/SDI2_DW_7.png)

- TC/C의 coordinator가 Wallet service라 가정해보자. 분산 트랜잭션이 시작될 때 계좌 A의 잔액은 $1이고, 계좌 C의 잔액은 $0이다.

- **Phase 1: Try**

  - _Try_ phase에서 coordinator로 동작하는 wallet service는 2개의 트랜잭션 명령을 2개의 데이터베이스로 보낸다.

    - (1) 계좌 A의 정보를 저장한 데이터베이스에게 coordinator는 계좌 A의 잔액을 $1 차감하는 local transaction을 시작한다.
    - (2) 계좌 C의 정보를 저장한 데이터베이스에게 coordinator는 NOP(no operation)을 전달한다.  
      NOP 명령을 받은 데이터베이스는 아무런 작업도 수행하지 않으며, 언제나 성공 메시지를 반환한다.

  - 아래 그림은 _Try_ phase를 나타낸다. 굵은 선은 트랜잭션에 의해 lock이 걸린 부분을 나타낸다.

    ![picture 6](/images/SDI2_DW_8.png)

- **Phase 2: Confirm**

  - 두 개의 데이터베이스가 모두 `yes`의 응답을 반환하면, wallet service는 _Confirm_ phase를 시작한다.

  - 계좌 A의 잔액 정보는 이미 phase 1에서 갱신되어 있다. 따라서 A의 정보는 더 이상 갱신할 필요가 없다. 하지만 phase 1에서  
    계좌 C에게 $1이 입금되지는 않았다. 따라서 _Confirm_ phase에서 wallet service는 $1만큼을 계좌 C에 입금해야 한다.

  - 아래 그림은 _Confirm_ phase의 과정을 나타낸다.

    ![picture 7](/images/SDI2_DW_9.png)

- **Phase 2: Cancel**

  - 만약 첫 번째 phase인 _Try_ phase가 실패하면 어떻게 될까? 위에서는 phase 1에서 계좌 C의 정보를 가진 데이터베이스에게  
    보내진 NOP 명령은 항상 성공한다고 가정했지만, 실전에서 이 부분이 실패할 여지는 분명히 존재한다. 예를 들어 계좌 C가 불법 계좌여서  
    관리자가 해당 계좌로의 송금을 모두 금지시켰다고 해보자. 이 경우 분산 트랜잭션은 취소되어야 할 것이다.

  - _Try_ phase에서 계좌 A의 잔액 정보가 이미 갱신되었고, wallet service는 이미 완료된 트랜잭션을 취소할 수 없다.  
    대신 _Try_ phase에서 수행된 트랜잭션을 취소시키는 과정을 담은 또다른 트랜잭션을 시작시킬 수 있다. 이 예시 상황의 경우, 이  
    트랜잭션은 계좌 A에게 $1을 다시 입금시키는 작업을 수행할 것이다.

  - _Try_ phase에서 계좌 C의 정보는 갱신되지 안핬기 때문에 계좌 C의 정보를 가진 데이터베이스에는 NOP 연산을 보내면 된다.  
    아래 그림은 이러한 _Cancel_ 과정을 나타낸다.

    ![picture 8](/images/SDI2_DW_10.png)

#### 2PC vs TC/C

- 아래 표를 보면 2PC와 TC/C는 비슷한 점도 많지만, 차이점도 많다는 것을 알 수 있다.  
  2PC에서 모든 local transaction은 두 번째 phase가 시작될 때까지 수행 중(locked)인 상태인 반면, TC/C에서 모든 local  
  transaction들은 두 번째 phase가 시작될 때 완료(unlock)되어 있다. 다른 말로 표현하자면 2PC의 두 번째 phase는 완료되지 않은  
  트랜잭션을 commit하거나 rollback해 종료시키는 반면, TC/C에서의 두 번째 phase는 이전 트랜잭션에서 에러가 발생했을 때  
  해당 트랜잭션이 수행한 작업을 rollback하는 연산을 수행하는 새로운 트랜잭션을 시작하게 된다.

  | -    | Phase 1                                              | Phase 2: 성공                          | Phase 2: 실패                                                        |
  | ---- | ---------------------------------------------------- | -------------------------------------- | -------------------------------------------------------------------- |
  | 2PC  | local transaction이 끝나지 않은 상태                 | 모든 local transaction commit          | 모든 local transaction 취소                                          |
  | TC/C | 모든 local transaction들은 성공이든 취소든 끝난 상태 | 필요하면 새로운 local transaction 시작 | phase 1에서 실행된 local transaction의 작업을 취소하는 트랜잭션 수행 |

- TC/C는 보상으로 구현한 분산 트랜잭션(distributed transaction by compensation)이라고도 불린다.  
  이 방법은 보상 작업(_"undo"_ 라고도 불린다)을 비즈니스 로직에서 구현하기 때문에 high-level solution이다.  
  이 방법의 장점은 데이터베이스에 대한 깊은 이해 또는 수정이 없이도 구현 가능하다는 것이다. 하지만 모든 세부사항과 분산 시스템의 복잡성을  
  애플리케이션 layer의 비즈니스 로직에서 처리해야 한다는 단점을 가진다.

#### Phase status table

- 2PC, TC/C를 보았으니 다시 "TC/C 수행 중 wallet service가 재시작하면 어떻게 할까?" 에 대한 질문을 살펴보자.  
  Wallet service가 재시작하면 이전에 수행된 모든 연산에 대한 기록도 누락될 것이고, 시스템이 이를 어떻게 복구해야 할지 모르는 상황이  
  발생할 수 있다.

- 해결책은 간단하다. TC/C의 수행 과정을 phase status로 트랜잭션을 지원하는 데이터베이스에 저장하는 것이다.  
  Phase status는 최소한 아래의 정보들을 가져야 한다.

  - 분산 트랜잭션의 ID, 내용
  - _Try_ phase에서의 각 데이터베이스의 상태. 이 상태는 `not sent yet`, `has been sent`, `response received` 중 하나이다.
  - 2번째 phase의 이름. `Confirm` 또는 `Cancel`이고, _Try_ phase의 결과를 통해 알 수 있다.
  - 2번째 phase의 상태
  - Out-of-order flag

- 그렇다면 phase status를 담는 테이블들을 어디에 둬야 할까? 일반적으로 phase status는 local transaction을 수행한  
  데이터베이스에 만들어 사용한다. 이를 반영해 갱신된 설계안은 아래와 같다.

  ![picture 9](/images/SDI2_DW_11.png)

#### Unbalanced state

- 아래 그림에서는 _Try_ phase가 끝난 후, $1이 사라져있다.

  ![picture 10](/images/SDI2_DW_12.png)

- 모든 것이 정상적으로 동작한다 가정하면 _Try_ phase가 끝난 후에 계좌 A에서 $1이 빠져나가야 하고, 계좌 C는 변함없이 있어야 한다.  
  이 상태에서 계좌 A와 계좌 C의 잔액 함은 $0이 되고, 이는 TC/C가 시작되기 전에 비해 $1이 부족하다. 이는 트랜잭션이 끝난 후  
  연관된 계좌의 총 잔액이 이전과 동일해야 한다는 회계 원칙에 위배된다.

- 하지만 TC/C에서는 트랜잭션이 보장된다. TC/C는 여러 개의 독립적인 local transaction을 수행한다.  
  TC/C가 애플리케이션에 의해 수행되기 때문에 애플리케이션 자체에서는 local transaction 사이 사이의 중간 결과를 파악할 수 있다.  
  반면 데이터베이스 트랜잭션 또는 2PC는 데이터베이스에 의해 수행되기 때문에 high-level 애플리케이션에서 중간 상태를 파악할 수 없다.

- 분산 트랜잭션이 실행되는 도중에는 항상 데이터 불일치가 발생한다. 이러한 데이터 불일치는 데이터베이스 등의 low-level 시스템 자체에서는  
  발생할 수 없기 때문에 애플리케이션 레벨에서 대부분 확인 가능하다. 만약 그렇지 않다면 TC/C 등과 같이 직접 보상을 해줘야 한다.

#### Valid operation orders

- _Try_ phase는 아래 표 처럼 3개의 선택지를 가진다.

  | Try phase choices | 계좌 A | 계좌 C |
  | ----------------- | ------ | ------ |
  | Choice 1          | -$1    | NOP    |
  | Choice 2          | NOP    | +$1    |
  | Choice 3          | -$1    | +$1    |

- 위 3개 선택지 모두 유효해 보이지만, 일부는 사실 유효하지 않다.

- 2번 선택지의 _Try_ phase에서 계좌 C의 입금은 성공하지만 계좌 A에 수행되는 NOP가 실패하면 Wallet service는 _Cancel_ phase에  
  들어가야 한다. 이때 계좌 C에 입금되어야 할 $1이 다른 계좌로 들어가는 상황이 발생할 수 있다. 이렇게 되면 이후 Wallet service가  
  계좌 C로부터 $1을 출금하려 할 때 잔액이 없게되고, 이는 분산 트랜잭션이 트랜잭션을 보장하지 못하는 꼴이 된다.

- 3번 선택지에서 만약 계좌 A로부터 $1이 출금되고 계좌 C로 $1이 입금되는 과정이 동시에 수행되면 여러 가지 복잡한 작업을 발생시킨다.  
  예를 들어 계좌 C에게 $1이 입금되긴 했지만 계좌 A로부터의 출금에 실패하면 어떻게 될까? 이러한 경우에는 어떻게 해야 할까?

- 위 문제는 모두 1번 선택지를 사용하면 발생하지 않는다. 즉 출금 작업을 먼저 하고, 입금 작업은 이후에 따로 하는 것이다.

#### Out-of-order execution

- TC/C의 부작용 중 하나는 out-of-order execution이다. 이것이 뭔지는 예시를 보면 쉽게 이해할 수 있을 것이다.

- 계좌 A에서 계좌 C로 $1만큼 송금하는 예시 상황을 다시 사용해보자. 아래 그림에 나타난 것처럼 _Try_ phase에서 계좌 A의 출금 작업이  
  실패해 Wallet service에 실패했음을 알리면 _Cancel_ phase에 들어가 계좌 A, 계좌 C 모두에 대해 취소 작업을 수행한다.

- 이때 계좌 C가 저장된 데이터베이스가 네트워크 이슈로 인해 _Try_ 작업보다 _Cancel_ 작업을 먼저 수신했다고 해보자.  
  이렇게 작업의 순서가 잘못되어 실행되는 것을 out-of-order execution이라고 한다.

- 아래 그림은 out-of-order execution의 예시이다.

  ![picture 11](/images/SDI2_DW_13.png)

- 이렇게 잘못된 순서로 작업이 수행되는 것을 처리하기 위해 각 database node들은 _Try_ 연산을 먼저 수신하지 않아도 TC/C의  
  _Cancel_ 연산을 처리할 수 있도록 허용한다. 기존 로직을 아래 항목들로 갱신하면 된다.

  - 잘못된 순서로 수신된 _Cancel_ 연산이 _Try_ 연산이 오기 전 먼저 도착했음을 나타내는 flag를 데이터베이스에 저장한다.  
    (out-of-order flag)
  - _Try_ 연산은 out-of-order flag의 유무를 확인한다. 만약 이 flag가 있다면 실패 응답을 반환한다.

- 이것이 이전에 phase status table에 out-of-order flag를 저장하도록 한 이유이다.

#### Distributed transaction: Saga

##### Linear order execution

- 분산 트랜잭션의 구현 방식 중 또다른 유명한 방법으로 Saga가 있다. Saga는 사실상 MSA에서 표준처럼 사용되는 방식이다.  
  아래는 Saga의 기반 아이디어이다.

  - (1) 모든 연산은 순차적으로 실행된다. 그리고 각 연산은 수행되는 데이터베이스와의 트랜잭션을 가지므로 독립적이다.
  - (2) 연산은 처음부터 끝의 순서대로 실행된다. 하나의 연산이 끝나면, 다음 연산이 trigger된다.
  - (3) 연산이 실패하면 보상 트랜잭션을 사용해 실패한 연산부터 첫 번째 연산까지, 즉 연산 실행 순서의 역순으로 rollback 처리한다.  
    따라서 만약 분산 트랜잭션이 _n_ 개의 연산을 수행하면 _2n_ 개의 트랜잭션을 준비해야 한다. _n_ 개는 우리가 원하는 연산을 위해,  
    그리고 나머지 _n_ 개는 보상 트랜잭션을 위해 사용된다.

- 예시를 통해 이해해보자. 아래 그림은 계좌 A로부터 계좌 C에게 송금하는 Saga workflow를 나타낸다.  
  가장 위에 있는 수평선은 연산의 정상적인 실행 순서를 나타낸다. 그리고 2개의 수직선은 에러가 발생했을 때 시스템이 수행하는 작업의 흐름을  
  나타낸다. 에러를 마주치면 송금 연산들은 모두 rollback되며 클라이언트는 에러 메시지를 반환받는다.  
  이전에 "Valid operation orders"에서 봤듯이 출금 연산을 먼저 하고 이후에 입금 연산을 수행한다.

  ![picture 12](/images/SDI2_DW_14.png)

- 그렇다면 연산들을 어떻게 조정할까? 2가지 방법이 있다.

  - Choreography: MSA에서 Saga 분산 트랜잭션에 참여하는 모든 서비스들은 다른 서비스들로부터 발생하는 이벤트를 구독해 자신의  
    작업을 수행한다. 따라서 이 방식은 완전히 탈중앙화된 조정 방식(decentralized coordination)이다.

  - Orchestration: 하나의 coordinator가 모든 서비스를 조정해 작업의 순서를 결정한다.

- 위 2개 선택지 중 어떤 coordination model을 사용할지는 비즈니스 요구사항과 목표 사항에 따라 결정된다.  
  Choreography를 사용했을 때의 어려운 점은 서비스들이 완전히 비동기적으로 소통하기 때문에 각 서비스가 다른 서비스의 이벤트를  
  적절히 처리하기 위해 내부적으로 state machine을 유지해야 한다는 것이다. 이는 서비스 개수가 많아지면 꽤나 복잡해질 수 있다.  
  Orchestration은 복잡성을 꽤나 잘 처리하기 때문에 전자 지갑 서비스에서 주로 선호되는 coordination 방식이다.

#### TC/C vs Saga

- TC/C와 Saga는 모두 애플리케이션 레벨에서 구현된 분산 트랜잭션이다. 아래 표는 이 둘의 비슷함과 차이점을 나타낸다.

  | -                                                  | TC/C           | Saga                    |
  | -------------------------------------------------- | -------------- | ----------------------- |
  | 보상 작업 수행 시점                                | _Cancel_ phase | rollback phase          |
  | 중앙화된 조정                                      | yes            | yes(orchestration mode) |
  | 작업 실행 순서                                     | any            | linear                  |
  | 동시 수행 가능성                                   | yes            | no(linear execution)    |
  | 부분적으로 일관적이지 않은 상태를 갖는지 여부      | yes            | yes                     |
  | 애플리케이션 또는 데이터베이스 로직으로 구현되는지 | application    | application             |

- 그렇다면 실전에서는 이 둘 중 어떤 것을 사용해야 할까? 정답은 latency 요구사항에 따라 결정된다.  
  위 표에서 알 수 있듯이 Saga를 사용하면 모든 연산은 순차적으로 실행되어야 하고, TC/C에서 연산들은 동시적으로 수행될 수 있다.  
  따라서 이 둘 중 어떤 것을 사용할지는 아래의 2개 요소에 크게 의존한다.

  - (1) Latency 요구사항이 없거나 송금 기능처럼 연관된 서비스들의 개수가 적다면 이 둘 중 아무거나 사용할 수 있다.  
    MSA의 트렌드를 따라가고 싶다면 Saga를 선택한다.

  - (2) 시스템이 latency에 민감하고 많은 서비스, 연산이 연관되어 있다면 TC/C가 더 좋은 선택지가 될 것이다.

- 기존의 개략적 설계안에서 Redis node들을 RDBMS node로 바꾸고 TC/C 또는 Saga를 사용해 분산 트랜잭션을 구현했다고 해보자.  
  그래도 한 가지 문제가 남는다. 만약 사용자가 애플리케이션 레벨에서 잘못된 연산을 수행하도록 하면 어떻게 될까? 예를 들어 사용자가  
  잘못된 금액을 송금하려 했고, 애플리케이션의 유효성 검증을 통과해 실제로 이 작업이 수행되었다고 해보자.  
  이렇게 되면 문제의 root cause를 추적하고 연관된 계좌에 대해 발생한 모든 작업을 audit(감사)해야 할 것이다. 이를 어떻게 할까?

### Event sourcing을 사용한 방법

#### 배경

- 실세계에서 전자 지갑 provider는 감사 대상이 확률이 높다. 외부 감사원들은 아래와 같은 어려운 요구를 할 수도 있다.

  - 특정 계좌의 특정 시각에 있는 잔액을 알 수 있는가?
  - 현재 계좌 잔액이 이전 기록들과 비교했을 때 정확하다고 할 수 있는가?
  - 코드가 바뀌어도 시스템 로직이 올바르다는 것을 어떻게 증명할 것인가?

- 위 질문들을 모두 해결하는 설계 철학 중 하나는 event sourcing으로 주로 DDD(Domain-Driven Design)에서 많이 사용된다.

#### 정의

- Event sourcing에는 아래 4개의 중요한 용어들이 있다.

  - Command: 외부 세계에서 의도된 행동을 의미한다. 예를 들어 고객 A가 고객 C에게 $1을 송금하려 한다 해보자.  
    이 경우 송금 요청이 command가 된다.

    - Event sourcing에서는 모든 것이 순서를 가진다는 것을 아는 것이 매우 중요하다. 따라서 command들은 주로 FIFO queue에 저장된다.

  - Event: 일부 command는 유효하지 않아 실행되지 않을 수 있기 때문에 command는 _의도(intention)_ 이지, _사실(fact)_ 이 아니다.  
    예를 들어 송금 작업 후 잔액이 음수가 된다면 해당 송금 command는 실패할 것이다.

    - Command는 실행되기 전 반드시 유효성이 검증되어야 한다. Command가 유효성 검증을 통과하면 유효하기에 반드시 실행되어야 한다.  
      그리고 이렇게 유효하게 판단된 command들의 실행 결과를 event라 한다.

    - Command와 event는 아래의 2가지 큰 차이점을 가진다.

      - Event들은 검증된 사실을 나타내기 때문에 반드시 실행되어야 한다. 실전에서는 event를 표현하기 위해 과거형이 사용된다.  
        예를 들어 command가 "A에서 B로 $1 송금" 이라면 event는 "A에서 B로 $1 송금됨" 이 된다.

      - Command들은 randomness(불규칙성)을 갖거나 I/O를 포함할 수 있지만, event들은 deterministic(결정적)이어야 한다.  
        Event는 역사적인 사실을 표현한다.

    - Event를 생성하는 과정에는 아래 2개의 중요한 속성이 존재한다.

      - (1) 하나의 command는 event를 몇 개든 만들 수 있다. 0개 또는 그 이상의 event들을 만들어낼 수 있다.
      - (2) 동일한 command들이 항상 같은 event들을 생성하지 않기 때문에 event의 생성 자체는 불규칙성을 포함할 수 있다.  
        Event를 생성하는 데에는 난수 또는 I/O가 사용될 수 있다.

    - Event들의 순서는 항상 command들의 순서를 따라야 한다. 따라서 event들도 command와 마찬가지로 FIFO queue에 저장된다.

  - State: Event가 적용되면 state가 변경된다. 우리가 설계하는 전자 지갑 시스템으로 봤을 때 state는 모든 고객들의 잔액 정보가  
    되며, 이는 map 자료구조로 표현될 수 있다. Key는 계좌명 또는 ID가 되고, value는 잔액이 된다. Map 자료구조를 저장하기 위해  
    일반적으로 key가 primary key가 되고, value가 table row로 저장되는 key-value store들이 사용된다.

  - State machine: State machine은 event sourcing 프로세스를 실행한다. 아래의 2개 주요 기능이 있다.

    - (1) Command들의 검증 및 event 생성
    - (2) State 갱신을 위해 event 적용

- Event sourcing을 적용하려면 state machine의 동작이 deterministic(결정적)해야 한다. 즉 state machine 자체는 절대로  
  불규칙성을 가지면 안된다. 예를 들어 state machine은 I/O를 사용해 외부에서 난수를 절대 읽어서는 안되고, 난수를 사용해서도 안된다.  
  State에 대해 event를 적용했을 때는 항상 동일한 결과를 내야 한다.

- 아래 그림은 event sourcing 아키텍쳐의 정적 모습을 보여준다. State machine은 command를 event로 변경하는 것과 함께  
  event를 적용하는 책임도 가진다. State machine이 이 2개의 주요 기능을 가지기 때문에 일반적으로 2개의 state machine을  
  그림에 포함시키는데, 하나는 command를 event로 변경하는 state machine이고, 다른 하나는 event를 적용하는 state machine이다.

  ![picture 13](/images/SDI2_DW_15.png)

- 아래 그림은 시간을 적용한 event sourcing의 동적 모습을 나타낸다. 시스템은 지속적으로 command를 수신하고, 이들을 하나씩 처리한다.

  ![picture 14](/images/SDI2_DW_16.png)

#### Wallet service 예시

- 우리가 설계하는 전자 지갑 서비스를 예시로 들어보면 command들은 송금 요청이 된다. 이 command들은 FIFO queue에 저장되고,  
  FIFO queue로는 Kafka가 많이 사용된다. Command queue는 아래 그림과 같다.

  ![picture 15](/images/SDI2_DW_17.png)

- State(잔액 정보)가 RDBMS에 저장되어 있다고 가정해보자. State machine은 각 command들을 FIFO 순서로 처리한다.  
  각 command에 대해 송금인이 출금하기 위해 충분한 잔액이 있는지 확인하고, 만약 있다면 송금 과정에 연관된 2개 계좌에 대해  
  event를 생성한다. 예를 들어 command가 `A => $1 => C`였다면 state machine은 `A: -$1`, `C: +$1`의 2개 event를 생성한다.

- 아래 그림은 state machine의 동작 과정을 5개 단계로 나타낸다.

  ![picture 16](/images/SDI2_DW_18.png)

  - 각 과정은 아래와 같다.

    - (1) Command queue에서 command를 읽어들인다.
    - (2) 데이터베이스로부터 잔액 정보를 읽어들인다.
    - (3) Command를 검증하고, 유효하다면 각 계좌에 대해 event를 하나씩 생성한다.
    - (4) 다음 event를 읽어들인다.
    - (5) 데이터베이스의 잔액 정보를 갱신함으로써 event를 적용한다.

#### Reproducibility(재현성)

- Event sourcing이 다른 아키텍쳐에 비해 갖는 중요한 이점 중 하나로 reproducibility(재현성)가 있다.

- 이전에 본 분산 트랜잭션들을 사용하면 wallet service는 갱신된 잔액 정보를 데이터베이스에 저장하고, 왜 해당 계좌의 잔액이 변경되었는지를  
  파악하기 어렵다. 그리고 갱신 연산에 따라 이전에 있던 잔액 정보들은 덮어씌워지기에 사라진다. Event sourcing 설계에서 모든 변경 사항들은  
  먼저 불변한(immutable) history로 저장된다. 데이터베이스는 오직 특정 시점에 해당 계좌의 잔액이 얼마 있는지를 확인하기 위한 view로써  
  사용된다.

- Event sourcing에서는 특정 시점의 event로부터 시작해 원하는 시점까지의 변경 사항들을 event를 다시 실행함으로써 재현해낼 수 있다.  
  Event들의 목록은 불변성을 가지고 state machine의 로직은 결정적이기 때문에 각 replay로부터 재현된 기록들이 항상 동일함이 보장된다.

- 아래 그림은 wallet service에서 event들을 다시 실행함으로써 이전 상태를 재현하는 과정을 나타낸다.

  ![picture 17](/images/SDI2_DW_19.png)

- Reproducibility(재현성)은 이전에 살펴본 감사원들의 어려운 요구사항을 해결할 수 있다.

  > - 특정 계좌의 특정 시각에 있는 잔액을 알 수 있는가?
  > - 현재 계좌 잔액이 이전 기록들과 비교했을 때 정확하다고 할 수 있는가?
  > - 코드가 바뀌어도 시스템 로직이 올바르다는 것을 어떻게 증명할 것인가?

- 첫 번째 질문에 대한 답은 처음부터 특정 시점까지의 모든 event들을 다시 실행해 알아낼 수 있다.  
  두 번째 질문의 경우, event들의 목록으로부터 잔액을 다시 계산해내 정확성을 보장할 수 있다.  
  마지막으로 세 번째 질문은 event들에 대해 여러 버전의 코드를 실행해 결과가 동일함을 확인함으로써 증명할 수 있다.

- 이렇게 감사도 확실히 지원할 수 있기 때문에 전자 지갑 서비스에서는 event sourcing을 일반적으로 사용한다.

### CQRS(Command Query Responsibility Segregation)

- 지금까지 wallet service가 한 계좌에서 다른 계좌로 돈을 효율적으로 송금하기 위한 설계 방법을 살펴보았다.  
  하지만 클라이언트는 여전히 자신의 잔액이 얼마인지 알 수 없다. 즉 event sourcing 프레임워크의 외부에 있는 클라이언트가 state에 대해  
  알 수 있도록 잔액 정보(state)를 publish할 수 있는 방법이 필요하다.

- 바로 역사적인 state를 보관하는 데이터베이스의 read-only copy를 만들어 이를 외부와 공유하도록 하는 방법이 떠오른다.  
  하지만 event sourcing은 이렇게 처리하지 않는다.

- Event sourcing은 state(잔액 정보)를 publish하는 대신 모든 event를 publish한다. 이렇게 되면 event sourcing 프레임워크의  
  외부에 있는 무언가도 state를 직접 만들어낼 수 있다. 이러한 설계 철학을 CQRS라 한다.

- CQRS에는 state를 write하는 하나의 state machine이 있고, state들의 view를 만들어내는 역할을 하는 여러 개의 read-only  
  state machine들이 존재한다. 이들이 만들어내는 view는 query에 사용될 수 있다.

- Read-only state machine들은 event queue로부터 서로 다른 state를 만들어낼 수 있다. 예를 들어 클라이언트가 자신의 잔액 정보를  
  알고 싶다 한다면 read-only state machine은 state를 만들어내 데이터베이스에 저장하고, query를 처리하도록 할 수 있다.  
  다른 state machine은 감사 목적으로 특정 시점의 잔액 정보를 계산해낼 수도 있을 것이다. 이렇게 state 정보를 사용해 회계 기록을  
  감사 목적으로 만들어낼 수도 있다.

- Read-only state machine들은 항상 최신 정보를 가지지 않을 수 있지만, 언제든지 최신 정보를 갖도록 할 수 있다.  
  이 설계는 eventual consistency(최종 일관성)를 가진다.

- 아래 그림은 전통적인 CQRS 아키텍쳐를 나타낸다.

  ![picture 18](/images/SDI2_DW_20.png)

- 지금까지 event sourcing을 사용해 시스템 전체를 재현할 수 있도록 설계하는 방법을 살펴보았다. 모든 유효한 비즈니스 기록들은  
  불변의 event queue에 담기며, 이후 정확성 검증 등의 목적에 사용될 수 있다.

- 하지만 지금까지 본 event sourcing 아키텍쳐는 한 번에 단 하나의 event만 처리할 수 있으며, 여러 외부 시스템과 소통해야만 한다.  
  더 빠르게 할 수는 없을까?

---

## 상세 설계

- 이번에는 높은 성능, 안정성, 그리고 확장성을 얻는 방법을 살펴보자.

### High-performance event sourcing

- 이전 예시에서 우리는 event store로 Kafka를 사용했고, state store로 데이터베이스를 사용했다.  
  최적화할 수 있는 방법들을 살펴보자.

#### File-based command and event list

- 첫 번째 최적화 방법으로 command와 event들을 Kafka와 같은 외부 시스템이 아닌 local disk에 저장하도록 할 수 있다.  
  이렇게 하면 네트워크 송수신 시간을 없앨 수 있다. Event list는 append-only 자료구조에 저장된다. Appending은 순차적인 write  
  연산이고, 이는 일반적으로 굉장히 빠르다. OS가 순차적인 read, write에 최적화되어 있기 때문에 magnetic hard drive에 대해 수행해도  
  매우 빠르다.

- 다음으로 최근에 발생한 command와 event들을 memory에 caching하도록 할 수 있다. 이전에 봤듯이 command와 event들은 저장되는  
  즉시 처리된다. 따라서 local disk에서 불러오는 시간을 줄이기 위해 memory에 caching해둘 수 있다.

- 구현 세부 사항을 살펴보자. 위에서 본 최적화 방식들은 mmap라는 기술로 쉽게 구현할 수 있다. Mmap는 local disk와 최근 컨텐츠들을  
  memory에 caching하는 작업을 동시에 수행할 수 있다. Mmap은 disk file을 memory로 배열로써 mapping한다. OS는 memory 내의  
  특정 파일들을 caching해 read, write 연산을 더욱 빠르게 수행할 수 있도록 해준다. Append-only file 연산의 경우, 모든 데이터가  
  memory에 보관될 것임이 거의 보장되고, 이는 매우 빠르다.

- 아래 그림은 file-based command, event storage의 모습을 보여준다.

  ![picture 19](/images/SDI2_DW_21.png)

#### File-based state

- 이전 설계에서 state(잔액 정보)는 RDBMS에 저장되었다. 실제 프로덕션 환경에서 데이터베이스는 일반적으로 네트워크를 통해 접근할 수 있는  
  stand-alone server상에서 실행된다. Command와 event에 대해 수행했던 최적화 방법과 비슷하게 state 정보도 local disk에  
  저장시킬 수 있다.

- 더 구체적으로 살펴보면 file-based local RDBMS인 SQLite 또는 local file-based key-value store인 RocksDB 등을 사용할 수 있다.

- 여기서는 write 연산에 최적화된 LSM(Log-Structured Merge-tree) 자료구조를 사용하는 RocksDB를 사용하도록 한다.  
  Read 연산의 성능을 높히기 위해 가장 최근의 데이터들은 caching된다.

- 아래 그림은 file-based command, event, state 설계를 나타낸다.

  ![picture 20](/images/SDI2_DW_22.png)

#### Snapshot

- 이제 command, event, state가 모두 file-based로 변경되었으니 재현(reproduce) 과정을 어떻게 최적화할 수 있는지 살펴보자.  
  처음 reproducibility를 봤을 때, state machine은 처음부터 주어진 시간 까지의 모든 event들을 매번 다시 처리해야 했다.  
  이를 주기적으로 state를 계산해 snapshot으로 보관하는 방식으로 최적화할 수 있다.

- Snapshot은 이전 state에 대한 불변 view이다. Snapshot이 한 번 저장되면 state machine은 이제 가장 처음부터 event들을  
  다시 다 처리할 필요가 없어진다. 대신 snapshot으로부터 데이터를 읽고, 해당 snapshot이 언제까지의 event들을 처리한 것인지를 파악한 후  
  이후의 event들만 다시 처리하면 된다.

- 전자 지갑 서비스와 같이 회계와 관련된 애플리케이션에서 회계 팀은 일반적으로 이전에 발생한 모든 송금들을 검증하기 위해 00:00에 생성된  
  snapshot을 요구한다. 처음 event sourcing의 CQRS를 봤을 당시 이에 대한 해결책은 read-only state machine이 처음부터 원하는  
  시점까지의 모든 event들을 처리하는 방법이었다. 이제 snapshot이 있으므로 read-only state machine은 오직 원하는 데이터를 포함하는  
  snapshot만 찾아 읽으면 된다.

- Snapshot은 거대한 binary file이며, 일반적으로 HDFS 등의 object storage에 보관된다.

- 아래 그림은 file-based event sourcing 아키텍쳐를 나타낸다. 모든 것이 file 기반일 때 시스템은 컴퓨터 하드웨어의  
  최대 I/O 처리량을 적극적으로 활용할 수 있다.

  ![picture 21](/images/SDI2_DW_23.png)

- 이렇게 local file-based 방법을 선택하면 시스템이 더 이상 외부에 있는 Kafka, 데이터베이스와 소통할 필요가 없어진다.  
  하지만 데이터가 local disk에 저장되기 때문에 서버가 stateful하게 되며, SPOF가 될 여지가 발생한다.  
  시스템의 안정성을 어떻게 향상시킬 수 있을까?

### Reliable high-performance event sourcing

- 해결책을 보기 전, 먼저 시스템에서 안정성이 보장되어야 하는 부분들을 살펴보자.

#### Reliability analysis

- 개념적으로 node가 하는 모든 작업은 두 가지로 분류되는데, data 그리고 consumption(소비)이다.  
  데이터가 내구성이 있는 한 동일한 코드를 다른 node에 대해 실행시킴으로써 연산 결과를 쉽게 복구해낼 수 있다. 이는 곧 우리가 데이터의  
  안정성에 대해서만 걱정해야 함을 의미한다. 데이터가 누락되면 해당 데이터는 영원히 누락되는 것이기 때문이다. 우리가 설계하는 시스템의  
  안정성은 대부분 데이터의 안정성에 의해 결정된다.

- 우리 시스템에는 아래의 4가지 데이터가 존재한다.

  - File-based command
  - File-based event
  - File-based state
  - State snapshot

- 위 4가지 데이터 각각에 대해 안정성을 확보하는 방법을 살펴보자.

- State, snapshot은 event 목록을 재실행함으로써 언제든지 다시 만들어낼 수 있다. State와 snapshot의 안정성을 향상시키기 위해  
  우리는 단지 event 목록의 안정성을 보장하면 된다.

- Command 부분을 살펴보자. Event는 command로부터 생성된다. 따라서 command의 안정성을 높이는 것 만으로 충분하다고 생각할 수 있다.  
  하지만 이 생각은 중요한 요소를 놓치고 있다. Event의 생성은 결정적이지 않으며 event는 내부적으로 난수 등의 랜덤한 요소, 외부 I/O 등을  
  포함하고 있을 수 있다. 따라서 command만 있으면 event를 재생성해낼 수 없다.

- 이제 event를 살펴보자. Event는 state(잔액 정보)을 변경시킨 사실적 기록을 나타낸다. Event는 불변이며 state를 다시 만들어내기 위해  
  사용될 수 있다.

- 이 분석 과정을 통해 우리는 높은 안정성을 꼭 보장해야 하는 데이터는 event가 유일함을 알 수 있다.

#### Consensus

- 높은 안정성을 제공하기 위해서는 event 목록을 여러 개의 node들로 replicate(복제)해야 한다. 이 복제 과정 중, 우리는 아래 요소들이 만족함을 보장해야 한다.

  - 데이터 누락 없음
  - 로그 파일 내 데이터의 순서가 모든 node들에 걸쳐 일관되어야 함

- 위의 2가지 속성을 보장하기 위해 consensus-based replication이 좋은 선택지가 된다. Consensus(합의) 알고리즘을 사용하면  
  여러 개의 node들이 동일한 데이터(event 목록)를 가지고 있음을 보장할 수 있다. Raft consensus 알고리즘을 활용한다고 해보자.

- Raft 알고리즘은 node 개수 중 절반 이상이 정상 동작하면 이들이 가지는 append-only list들이 모두 동일함을 보장해준다.  
  예를 들어 5개의 node들이 있고 이들간에 데이터를 동기화하기 위해 Raft 알고리즘을 사용하면 3개 이상의 node들이 정상 동작하면  
  시스템이 정상 동작함을 보장해준다.

  ![picture 22](/images/SDI2_DW_24.png)

- Raft 알고리즘에서 node는 아래 3개 역할 중 하나를 가진다.

  - Leader
  - Candidate
  - Follower

- Raft paper에서 Raft 알고리즘의 구현체를 찾을 수 있다. 여기서는 이 알고리즘의 대략적인 개념만을 짚어보도록 하자.  
  Raft에서 최소 1개의 node는 cluster의 leader로써 동작하며, 나머지 node들은 follower들이다.  
  Leader는 외부로부터 command를 수신하고 데이터를 cluster 내의 follower들에게 복제하는 책임을 가진다.

- Raft 알고리즘을 사용하면 대부분의 node들이 정상 동작하면 시스템 자체의 안정성도 함께 보장된다.  
  예를 들어 cluster 내에 3개 node가 있을 때 1개 node가 장애가 나도 이를 감래할 수 있으며 만약 5개의 node들이 있다면  
  2개 node가 장애가 나도 이를 감래할 수 있다.

#### Reliable solution

- 데이터 복제를 활용하면 위에서 설계한 file-based event sourcing 아키텍쳐에서 SPOF는 없어진다.  
  이제 구현 세부사항을 살펴보자. 아래 그림은 안정성이 보장된 event sourcing 아키텍쳐의 모습이다.

  ![picture 23](/images/SDI2_DW_25.png)

- 위 그림은 3개의 event sourcing node들이 존재한다. 그리고 각 node는 Raft 알고리즘을 사용해 event 목록을 안정적으로 동기화한다.

- Leader node는 외부 사용자로부터 발생한 command들을 수신하고 이를 event로 전환하며 event를 자신의 local event list에  
  append한다. 그리고 Raft 알고리즘이 새로 추가된 event들을 follower들로 복제한다.

- Follower를 포함한 모든 node들은 event 목록을 처리하고 state를 갱신한다. Raft 알고리즘 덕분에 leader와 follower들이 동일한  
  event 목록을 가진다는 것이 보장되고, event sourcing은 event 목록이 동일하다면 모든 state가 동일함을 보장한다.

- 안정적인 시스템은 장애도 잘 처리해낼 수 있어야 한다. 이제 장애가 발생한 node들이 어떻게 처리되는지 살펴보자.

- Leader node에 장애가 발생하면 Raft 알고리즘은 자동으로 남아있는 _healty_ node 들 중 새로운 leader node를 선출한다.  
  이렇게 새롭게 선택된 leader node는 외부로부터 command를 수신할 책임을 갖게 된다. 따라서 node에 장애가 나도 서비스 자체는  
  계속해서 정상 동작할 것임이 보장된다.

- 하지만 leader node가 장애가 나는 시점이 command가 list로 전환되기 직전일 수도 있다. 이 경우, 클라이언트는 timeout이 발생하거나  
  에러 응답을 받는 등 문제를 파악할 수 있다. 따라서 클라이어트는 새롭게 선출된 leader node에게 다시 command를 보내야 한다.

- Leader node와 반대로 follower node의 장애는 처리하기 훨씬 수월하다. Follower node에 장애가 발생하면 해당 node에 전달된  
  요청들은 모두 실패할 것이다. Raft는 이러한 요청 실패를 장애 난 node가 재시작되거나 다른 node로 대체될 때까지 무기한으로 재시도(retry)한다.

- 지금까지 본 방식들을 적용해 시스템은 더욱 안정적이며 장애를 잘 감래할 수 있게 되었다. 하지만 맨 처음 요구사항을 되짚어 보면  
  TPS가 100만이었고, 이는 한 대의 서버가 감당하기 어려운 수치이다. 시스템을 어떻게 더 확장성 있게 구축할 수 있을까?

### Distributed event sourcing

- 바로 직전, 우리는 event sourcing 아키텍쳐의 성능을 높히고 안정적으로 만들기 위한 방법을 살펴보았다.  
  안정성은 확보할 수 있지만, 아래의 2가지 한계점이 나타난다.

  - (1) 전자 지갑이 갱신되었을 때 사용자는 갱신된 결과를 즉각적으로 수신하고 싶어한다. 하지만 CQRS 설계에서 request/response의  
    흐름은 느리게 처리될 수 있다. 이는 클라이언트가 정확히 언제 전자 지갑이 갱신되는지 알지 못하고, 클라이언트가 polling에 의존해야 할 수도 있기 때문이다.

  - (2) 단일 Raft group의 용량은 제한적이다. 규모가 커지면 데이터를 sharding하고 분산 트랜잭션을 구현해야 한다.

- 위 두 가지 문제가 어떻게 해결될 수 있는지 살펴보자.

#### Pull vs push

- Pull model에서 외부 사용자는 주기적으로 read-only state machine을 polling해 실행 상태를 확인한다.  
  이 방식은 실시간이 아니며 polling 주기가 너무 짧으면 wallet service에 부하를 발생시킬 수 있다.  
  아래 그림은 pulling model을 나타낸다.

  ![picture 24](/images/SDI2_DW_26.png)

- 이 _순진한_ pull model은 외부 사용자와 event sourcing node 사이에 reverse proxy를 둠으로써 개선될 수 있다.  
  이 설계에서 외부 사용자는 reverse proxy로 command를 전송하고, reverse proxy는 event sourcing node로 command를  
  forwarding하며 주기적으로 실행 상태를 polling한다. 이 설계는 클라이언트 로직을 단순화시키지만, 상호 작용이 실시간으로 이뤄지지  
  않는다는 단점은 여전하다.

- 아래 그림은 reverse proxy가 추가된 pull model을 나타낸다.

  ![picture 25](/images/SDI2_DW_27.png)

- Reverse proxy가 있다면 read-only state machine을 수정해 응답을 더욱 빠르게 주도록 할 수 있다.  
  이전에 봤듯이 read-only state machine은 각각 수행하는 일이 다를 수 있다. 예를 들어 어떤 read-only state machine은  
  event를 수신하는 즉시 실행 상태를 reverse proxy로 push하도록 할 수 있다. 이는 실시간에 근접한 UX를 제공할 수 있다.

- 아래 그림은 push-based model을 나타낸다.

  ![picture 26](/images/SDI2_DW_28.png)

### Distributed transaction

- 모든 event sourcing node group이 동기적으로 작동하면 TC/C 또는 Saga와 같은 분산 트랜잭션을 다시 활용할 수 있게 된다.  
  데이터를 hash 값을 2로 나눈 나머지를 기준으로 partitioning했다고 가정해보자.

  ![picture 27](/images/SDI2_DW_29.png)

- 최종적인 distributed event sourcing 아키텍쳐에서 송금 과정이 어떻게 이뤄지는지 살펴보자.  
  이해하기 쉽도록 하기 위해 Saga 분산 트랜잭션 모델을 사용하고 rollback이 없는 _"happy path"_ 만 살펴보자.

- 송금 과정은 2개의 분산된 작업을 필요로 한다.

  - `A: -$1`, `C: +$1`

- 아래 그림처럼 Saga coordinator는 실행 과정을 조정한다.

  ![picture 28](/images/SDI2_DW_30.png)

- 각 과정을 하나씩 살펴보자.

  - (1) 사용자 A가 Saga coordinator에게 분산 트랜잭션을 포함하는 요청을 보낸다. `A: -$1`, `C: +$1`의 2개 연산을 포함한다.

  - (2) Saga coordinator는 트랜잭션의 상태를 파악하기 위해 Phase status table에 새로운 record를 추가한다.

  - (3) Saga coordinator는 연산의 순서를 관찰하고 `A: -$1`을 먼저 처리해야 겠다고 판단한다.  
    이후 coordinator는 `A: -$1` command를 A의 계좌 정보가 저장되어 있는 Partition 1로 보낸다.

  - (4) Partition 1의 Raft leader node가 `A: -$1` command를 수신하고 이를 command list에 저장한다.  
    이후 command를 검증하는데, 검증에 성공하면 event로 전환시킨다. 다음으로 Raft 합의 알고리즘에 의해 데이터가 follower node들로 복제된다.  
    `A 계좌에서 $1 출금됨` 이벤트는 동기화가 완료된 후 처리된다.

  - (5) 이벤트가 동기화되면 Partition 1의 event sourcing 프레임워크가 CQRS를 사용해 read path로 데이터를 동기화한다.  
    Read path는 이후 실행 상태와 state를 재건해낸다.

  - (6) Patition 1의 read path가 상태를 event sourcing 프레임워크의 호출자, 즉 Saga coordinator에게 push한다.

  - (7) Saga coordinator는 성공 상태를 Partition 1으로부터 수신한다.

  - (8) Saga coordinator는 Partition 1의 작업이 성공했음을 나타내는 새로운 record를 Phase status table에 기록한다.

  - (9) 첫 번째 연산(`A: -$1`)이 성공했으므로 Saga coordinator는 두 번째 연산(`C: +$1`)을 수행한다.  
    이전과 마찬가지로 계좌 C의 정보가 저장되어 있는 Partition 2로 `C: +$1` command를 보낸다.

  - (10) Partition 2의 Raft leader node가 `C: +$1` command를 수신하고 command list에 저장한다.  
    유효성을 검증한 후 유효하다고 판단되면 이벤트로 전환시킨다. Raft 합의 알고리즘에 의해 데이터가 follower node들로 복제되며,  
    `C 계좌에 $1 입금됨` 이벤트가 복제가 완료된 후 처리된다.

  - (11) 이벤트가 동기화되면 Partition 2의 event sourcing 프레임워크가 CQRS를 사용해 read path로 데이터를 동기화한다.  
    Read path는 이후 실행 상태와 state를 재건해낸다.

  - (12) Partition 2의 read path가 상태를 event sourcing 프레임워크의 호출자, 즉 Saga coordinator에게 push한다.

  - (13) Saga coordinator는 성공 상태를 Partition 2로부터 수신한다.

  - (14) Saga coordinator는 Partition 2의 작업이 정상적으로 완료되었음을 나타내는 새로운 record를 Phase status table에 기록한다.

  - (15) 이 시점에 모든 연산은 성공적으로 완료되었으므로 분산 트랜잭션이 종료된다. Saga coordinator는 자신의 호출자에게 결과를 반환한다.

---

## 마무리

- 이번 장에서는 초당 백만개가 넘는 송금 command를 처리할 수 있는 전자 지갑 서비스를 설계해보았다.  
  개략적으로 추정치를 계산함으로써 해당 사용량을 원활하게 처리하기 위해 수천개의 node들이 필요함을 알 수 있었다.

- 첫 번째 설계에서는 Redis와 같은 in-memory key-value store를 사용했다. 하지만 이 방식의 문제점은 데이터의 내구성이 보장되지 않는다는 것이었다.

- 두 번째 설계에서 in-memory cache가 RDBMS로 대체되었다. 여러 개의 node들을 원자적으로 다루기 위해 2PC, TC/C, Saga와 같은  
  분산 트랜잭션을 구현하는 방법을 살펴보았다. 하지만 트랜잭션에 기반한 해결책들은 감사 요구사항을 쉽게 만족시킬 수 없다는 단점이 존재했다.

- 다음으로 event sourcing을 살펴보았다. 처음에는 event sourcing을 외부 데이터베이스와 queue를 사용하는 방식으로 구현했지만,  
  이 방식은 성능이 그닥 좋지 못했다. 이 문제를 command, event, state를 node의 local disk에 저장시킴으로써 해결했다.

- 하나의 node가 있다는 것은 SPOF가 존재함을 의미한다. 시스템의 안정성을 향상시키기 위해 Raft 합의 알고리즘을 사용해 event 목록을  
  여러 개의 node들로 동기화하기로 결정했다.

- 마지막 개선점은 event sourcing에 CQRS를 도입하는 것이었다. 외부 사용자 관점에서 비동기로 동작하는 event sourcing 프레임워크를  
  외부 사용자와 event sourcing node 사이에 reverse proxy를 둠으로써 동기 통신을 하도록 변경했다. 이후 여러 개의 node들로  
  command의 처리를 조정하기 위해 TC/C 또는 Saga를 활용했다.

---
