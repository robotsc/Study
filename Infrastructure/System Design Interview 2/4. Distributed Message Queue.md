# Distributed Message Queue

- 이번 장에서는 시스템 디자인 면접에서 자주 등장하는 질문인 분산 message queue를 설계해보자. 현대 아키텍쳐에서 시스템은 작고 서로 독립된  
  컴포넌트들로 나눠지고, 이들은 잘 정의된 인터페이스를 두고 소통한다. Message queue는 이 컴포넌트들이 서로 상호작용하는 방법을 제공한다.  
  그렇다면 message queue가 어떤 이점을 가져다 줄까?

  - Decoupling: Message queue는 서로 다른 컴포넌트가 강결합되는 것을 막아 서로 독립적으로 개발될 수 있게 한다.
  - Improved scalability: 트래픽량에 따라 producer와 consumer 각각을 확장할 수 있다. 예를 들어 사용량이 많은 시간대에는 consumer만  
    확장해 늘어난 트래픽을 처리하도록 할 수 있다.
  - Increased availability: 시스템의 일부가 장애가 난다면, 다른 컴포넌트들은 queue를 사용해 계속해 상호작용할 수 있다.
  - Better performance: Message queue는 비동기 통신을 쉽게 하도록 한다. Producer들은 queue에 consumer가 메시지를 소비하기를 기다리지  
    않고, 단순히 메시지를 전달하고 다른 일을 할 수 있다. 즉 consumer, producer가 서로의 작업이 완료되기를 기다릴 필요가 없다.

- 다음은 유명한 분산 message queue들이다.
  - Apache Kafka, Apache RocketMQ, RabbitMQ, Apache Pulsar, Apache ActiveMQ, ZeroMQ

#### Message queues vs Event streaming platforms

- 위의 유명한 message queue들 중 따지고 보면 Apache Kafka와 Pulsar는 event streaming platform이지, message queue는 아니다.  
  하지만 message queue와 event streaming platform을 구분짓는 경계를 허무는 다양한 기능들을 제공한다. 예를 들어 일반적인 message queue인  
  RabbitMQ는 선택적으로 stream 기능을 사용할 수 있도록 한다. 이 stream 기능은 event streaming platform처럼 append-only log 방식으로  
  구현되어 있고, 메시지의 반복적인 소비와 메시지의 생명주기를 길게 다룰 수 있도록 해준다. 또한 Apache Plusar는 Kafka의 대조군인데,  
  일반 분산 message queue로 사용해도 될 정도로 유연하고 성능이 좋다.

- 이번 장에서는 long data retention, repeated consumption of messages 등 event streaming platform에서 제공할 만한 추가적인  
  기능을 가진 분산 message queue를 설계해볼 것이다. 이 기능들은 설계를 더욱 복잡하게 한다.

## 문제 이해 및 설계 범위 확정

- 기본적으로 message queue의 기능은 매우 간결하고 단순한데, producer는 메시지를 queue에 보낼 수 있어야 하고, consumer들은 이 메시지들을  
  가져와 사용(소비)할 수 있어야 한다. 이 기능 외에도 성능, 메시지 전달 방식, 메시지 retention 등의 고려 요소들이 많다.  
  이번에 디자인할 분산 message queue가 제공하는 기능 요구사항은 아래와 같다.

  - Producer들은 메시지를 message queue에 보낼 수 있다.
  - Consumer들은 message queue로부터 메시지를 소비한다.
  - 메시지는 한 번, 또는 여러 번 소비될 수 있다.
  - 메시지의 크기는 KB 단위이다.
  - Queue에 메시지가 전달된 순서대로 consumer에게 전달된다.
  - 데이터 전달 방식(at-least once, at-most once, exactly once)는 사용자들이 설정할 수 있다.

- 아래는 비 기능적 요구사항들이다.

  - Use case에 따라 높은 처리량 또는 낮은 latency를 제공해야 한다.
  - Scalability: 시스템은 분산 처리되어 있어야 한다. 따라서 전송되는 메시지가 갑자기 치솟아도 이를 감당할 수 있어야 한다.
  - Persist, durable: 데이터는 disk에 저장되어야 하고, 여러 개의 node들에 복제되어야 한다.

### 일반적인 message queue

- 위 요구사항들은 일반적인 message queue에서는 제공하지 않는 기능들을 포함한다. 예를 들어 일반적인 message queue는 메시지를 소비될 때까지  
  메모리에만 보관하고, 일반적으로 메시지의 순서를 보장하지 않는다. 이 기능이 없으면 설계가 훨씬 쉬워진다.

---

## 개략적 설계안 제시 및 동의 구하기

- 첫 번째로 message queue의 기본적인 요구사항을 다뤄보자.

- 아래 그림은 message queue의 중요한 컴포넌트들과 이들간의 상호작용을 나타낸다.

  ![picture 48](/images/SDI2_MQ_1.png)

  - Producer는 message queue에 메시지를 보낸다.
  - Consumer는 queue를 구독하고, 해당 queue의 메시지를 소비한다.
  - Message queue는 producer와 consumer의 가운데에 위치해 이들을 decouple함으로써 각각 독립적으로 운영 및 확장할 수 있도록 한다.
  - Producer와 consumer 모두는 client/server model에서 client에 해당하고, message queue가 server에 해당한다.  
    Client, server는 네트워크를 통해 상호작용한다.

### Messaging Models

- 가장 유명한 messaging model은 point-to-point와 publish/subscribe 이다.

#### Point-to-point

- 이 모델은 전통적인 message queue에서 많이 사용된다. Point-to-point model에서는 메시지가 queue에 보내지고, 단 하나의 consumer에 의해  
  소비된다. 물론 queue로부터 메시지를 소비하기 위해 많은 consumer들이 있을 수 있지만, 각 메시지는 오직 단 하나의 consumer에 의해 소비된다.  
  아래 그림에서 메시지 A는 consumer 1에 의해 소비된다.

![picture 49](/images/SDI2_MQ_2.png)

- Consumer가 메시지를 소비하면 해당 메시지는 queue에서 제거된다. 즉 point-to-point model에서는 data retention이 없다.  
  이와 반대로 우리가 이번에 설계할 message queue는 메시지를 2주 동안 보관하기 위한 persistence layer가 있고, 이를 통해 메시지가 반복적으로 소비되게 한다.

#### Publish-subscribe

- 이 모델을 알기 전에 우선 topic이 무엇인지 보자. Topic은 메시지를 분류하기 위한 카테고리이다. 각 topic은 모든 message queue service에서  
  unique한 이름을 가진다. 그리고 메시지는 특정 topic을 갖고 message queue에 보내지고, 읽어진다.

- Publish-subscribe model에서 메시지는 topic으로 보내지고, 해당 topic을 구독하고 있는 consumer들에 의해 소비된다.  
  아래 그림에 나타난 것처럼, message A는 consumer1, consumer2 모두에 의해 소비된다.

  ![picture 50](/images/SDI2_MQ_3.png)

- 우리가 설계할 message queue는 위의 두 모델 모두를 지원한다. Publish-subscribe model은 **topic** 을 사용해 구현되고, point-to-point  
  model은 **consumer group** 을 사용해 구현된다.

### Topics, partitions, and brokers

- 이전에 봤듯이 메시지들은 topic으로 분류되고 저장된다. 그렇다면 단 하나의 서버가 처리하기에 topic의 데이터량이 너무 많으면 어떻게 될까?

- 이 문제를 해결하기 위한 하나의 접근법으로 **partition(sharding)** 을 사용할 수 있다. 아래 그림에 나타난 것처럼 topic을 partition들로  
  나누고, 각 partition에 고르게 메시지들을 전달한다. 여기서 partition을 _topic에 해당하는 메시지의 부분집합_ 으로 생각해도 된다.  
  Partition들은 message queue cluster 내의 서버들에 고르게 분포된다. 그리고 partition들을 갖는 서버를 **broker** 라 한다.  
  Broker들로 partition을 분배하는 것은 높은 확장성을 지원하기 위해 중요한 요소다. 특정 topic의 처리량을 partition의 개수를 늘림으로써  
  처리할 수 있다.

  ![picture 51](/images/SDI2_MQ_4.png)

- Topic의 각 partition은 queue와 함께 FIFO mechanism으로 운영된다. 즉 partition 내에 메시지의 순서를 그대로 보관한다는 뜻이다.  
  그리고 partition 내에 있는 메시지의 위치를 **offset** 이라 한다.

- 메시지가 producer에 의해 전달되면, 실제로는 해당 메시지의 topic의 partition들 중 하나로 전달된다. 각 메시지는 선택적으로 message key를  
  가지며, 같은 message key를 가진 메시지들은 모두 동일한 partition으로 전달된다. 만약 message key가 없다면 메시지는 partition들 중  
  하나로 랜덤으로 전달된다.

- Consumer가 topic에 구독하게 되면, 해당 topic의 하나 이상의 partition들로부터 데이터를 pull하게 된다. 특정 topic에 구독하는  
  consumer들이 여러 개 있다면, 각 consumer는 topic의 partition의 일부를 처리하는 책임이 부여된다. 이러한 consumer들은 topic에 대해 하나의  
  **consumer group** 을 형성하게 된다.

- Broker와 partition들로 구성된 Message queue cluster는 아래와 같이 구성되어 있다.

  ![picture 52](/images/SDI2_MQ_5.png)

### Consumer group

- 이전에 언급했듯이 우리가 설계하려는 message queue는 point-to-point와 publish-subscribe model 모두를 지원해야 한다.

- **Consumer group** 은 consumer들의 집합으로, 이 consumer group 내의 consumer들은 특정 topic의 메시지들을 소비하기 위해 협업한다.

- Consumer들은 group들로 구성될 수 있다. 각 consumer group은 여러 개의 topic을 구독할 수 있고, 각자 자신만의 소비 offset을 유지한다.  
  예를 들어 consumer들을 하나는 billing, 다른 하나는 accounting 처럼 use case별로 grouping할 수 있다.

- 동일한 consumer group 내의 consumer들은 트래픽을 병렬적으로 처리할 수 있다. 아래 그림을 보자.

  ![picture 53](/images/SDI2_MQ_6.png)

  - Consumer group 1은 topic A를 구독한다.
  - Consumer group 2는 topic A와 topic B를 구독한다.
  - Topic A는 consumer group 1과 consumer group 2 모두에 의해 구독되고 있다. 즉 동일한 메시지가 여러 개의 consumer들에 의해 소비될 수  
    있다. 이 패턴이 publish-subscribe model을 지원하도록 한다.

- 하지만 위 그림대로 진행하면 문제점이 하나 생긴다. 병렬적으로 데이터를 처리하는 것은 처리량을 높여주지만, 같은 partition내의 메시지가 소비되는 순서가  
  메시지가 전달된 순서와 같다는 것이 보장되지 못한다. 예를 들어 만약 consumer 1과 consumer 2가 모두 partition 1의 메시지를 소비해가면,  
  partition-1의 메시지 소비 순서를 보장하지 못하게 될 것이다.

- 이 문제는 하나의 partition은 같은 group에 속한 단 하나의 consumer에 의해 소비되어야 한다는 규칙을 추가함으로써 해결할 수 있다.  
  만약 consumer group내의 consumer의 개수가 partition의 개수보다 크다면, 일부 consumer는 아무런 데이터를 소비하지 못하게 될 것이다.  
  예를 들어 위 그림에서 consumer group 2에 속한 consumer 3는 이미 같은 consumer group에 속한 consumer 4가 topic B를 구독하고  
  있으므로 topic B로부터 메시지를 소비하지 못한다.

- 이 규칙을 적용해 만약 하나의 consumer group에 모든 consumer들을 배치하게 된다면 동일한 partition에 있는 메시지들을 각각 단 하나의  
  consumer에 의해 소비될 것이고, 이는 곧 point-to-point model과 같다. Partition이 가장 작은 storage unit이니 사전에 미리 충분한  
  수의 partition들을 배치해놔 동적으로 partition의 개수를 늘리지 않도록 하면 좋다. 확장이 필요하다면 오로지 consumer만 추가하면 된다.

### 개략적 설계안

- 아래 그림은 위에서 다룬 모든 내용을 포함한 개략적 설계안이다.

  ![picture 54](/images/SDI2_MQ_7.png)

  - Clients

    - Producer: 메시지를 특정 topic으로 push한다.
    - Consumer group: Topic을 구독하며 메시지를 소비한다.

  - Core service and storage

    - Broker: 여러 개의 partition을 가진다. Partition은 특정 topic 내의 메시지들의 부분 집합을 가진다.
    - Storage:
      - Data storage: 메시지는 partition 내의 data storage에 보관된다.
      - State storage: Consumer의 상태를 보관한다.
      - Metadata storage: 설정 값 및 topic의 속성들을 보관한다.

  - Coordination service:

    - Service discovery: 어떤 broker들이 사용 가능한지를 알려준다.
    - Leader election: Broker들 중 하나가 active controller로 선택된다. Cluser 내에는 단 하나의 active controller 만이 존재한다.  
      Active controller는 partition을 할당하는 작업에 대한 책임을 가진다.
    - 주로 controller를 선정하기 위해 Apache ZooKeeper, etcd 등이 사용된다.

---

## 상세 설계

- 높은 data retention 요구사항과 높은 처리량을 동시에 만족하기 위해 우리는 설계 시 아래의 중요한 3개 선택을 했다.

  - 현대 OS에서 순차적 접근에 대한 성능을 보장하고 disk caching이 적용 가능한 on-disk data structure를 사용한다.
  - 메시지가 변경되지 않고 producer로부터 받아져 queue에 보관되고, consumer에게 전달되도록 설계했다. 이렇게 하면 메시지를 복사할 필요가 없어진다.  
    복사하는 작업은 높은 처리량과 많은 데이터를 다루는 시스템에서 꽤나 비싼 작업이다.
  - 시스템을 batch 작업이 수월하도록 설계했다. 작은 I/O는 높은 처리량을 지원하는 시스템의 적과도 같다. 따라서 가능한 한 우리의 설계는 batch를  
    권장한다. Producer들은 메시지를 batch로 전달한다. Message queue는 더 큰 batch로 메시지들을 보관하고, consumer들은 원한다면  
    메시지를 batch로 가져갈 수도 있다.

### Data storage

- 이제 메시지를 보관하는 방법들을 자세히 살펴보자. 가장 좋은 선택을 하기 위해 message queue의 트래픽 패턴을 생각해보자.

  - Write-heavy, read-heavy
  - Update, delete 연산이 없다.
  - 순차적인 read/write 요청이 매우 많다.

#### 선택지 1번: 데이터베이스

- 첫 번째 선택지로 데이터베이스를 사용할 수 있다.

  - RDB: Topic table을 만들고, 하나의 메시지는 해당 topic table의 row로 저장된다.
  - NoSQL: Topic을 위한 collection을 만들고, 메시지를 document로 저장한다.

- Database는 메시지를 보관해야 한다는 요구사항은 만족시키지만, 큰 규모에서 write-heavy, read-heavy한 트래픽을 모두 원활히 처리하기 위해  
  설계하는 것이 어렵기 때문에 이상적인 선택지는 아니다. 데이터베이스를 사용하는 방법은 message queue의 데이터 사용 패턴을 잘 만족시키지 못한다.

- 이는 데이터베이스가 최선의 선택지가 아니며, 시스템의 병목점이 될 수 있음을 의미한다.

#### 선택지 2번: WAL(Write-ahead log)

- 두 번째 선택지로 WAL(Write-ahead log)를 사용할 수 있다. WAL은 단순히 하나의 파일로, 새로운 내용들이 append-only log에 append된다.  
  WAL은 MySQL의 redo log, ZooKeeper의 WAL 등 많은 시스템에서 사용된다.

- 이 설계안에서는 메시지를 디스크에 WAL log file로 저장하는 것이 좋다. WAL은 순차적인 read/write 접근 방식을 가진다.  
  디스크의 순차 접근 성능은 매우 좋다. 또한 rotational disk는 용량이 크며 비용도 적게 든다.

- 아래 그림에 나타난 것처럼 새로운 메시지는 하나씩 증가하는 offset과 함게 partition의 tail에 append된다. 가장 쉬운 구현 방법으로  
  log file의 line number를 offset으로 사용할 수도 있다. 하지만 하나의 파일이 무한정 커질 수 없다는 점을 감안하면, segment로  
  나누는 것이 좋을 것이다.

- Segment를 사용할 때 새로운 메시지들은 오직 활성 segment file에만 append된다. 활성 segment file이 특정 크기게 도달하면, 새로운  
  활성 segment file이 생겨 새로운 메시지를 저장하고, 기존에 있던 활성 segment file은 비활성 상태가 된다. 비활성 segment들은 오직  
  read request만 처리한다. 오래된 비활성 segment file들은 retention이 지나거나 용량의 한계에 의해 제거될 수 있다.

  ![picture 55](/images/SDI2_MQ_8.png)

- 동일한 partition 내의 segment file들은 아래 그림과 같이 `Partition-{:partition_id}`의 이름을 갖는 폴더로 정리된다.

  ![picture 56](/images/SDI2_MQ_9.png)

##### Disck performance에 대한 이야기

- 높은 data retention 요구사항을 만족시키기 위해 이 설계안은 많은 양의 데이터를 저장하기 위해 disk에 많이 의존한다. Rotational disk가  
  느리다는 잘못 알려진 사실이 있는데, 이는 오직 random access를 할 때만 느린 것이다. 우리가 설계하는 시스템은 순차적 접근을 하기 때문에  
  rotational disk를 사용해도 충분히 빠른 read/write 처리량, 그리고 성능을 보장받을 수 있다.

- 또한 현대 OS는 disk data를 메모리에 caching하기 때문에 접근 속도도 빠를 것이다. WAL도 마찬가지로 OS disk caching을 많이 사용한다.

### Message data structure

- 메시지의 자료 구조는 높은 처리량을 달성하기 위해 중요한 요소이다. 메시지 자료구조는 producer, message queue, 그리고 consumer 사이의  
  일종의 _약속_ 을 정의한다. 이 설계안은 메시지가 producer에서 queue로, 그리고 queue에서 consumer로 전달되는 과정에서 데이터의 불필요한  
  복사를 하지 않음으로써 높은 처리량을 달성한다. 만약 시스템의 어느 부분에서도 이 동작을 어긴다면, 메시지는 변경되야 할 것이고, 연산량이 많은  
  복사 작업이 필요하게 된다. 따라서 시스템의 성능을 심각하게 떨어뜨릴 수 있다.

- 아래는 메시지 자료구조의 스키마 예시이다.

| Field name | Data type |
| ---------- | --------- |
| key        | byte[]    |
| value      | byte[]    |
| topic      | string    |
| partition  | integer   |
| offset     | long      |
| timestamp  | long      |
| size       | integer   |
| crc        | integer   |

#### Message Key

- 메시지의 key는 해당 메시지가 어떤 partition으로 전달될지 결정하는 역할을 한다. 만약 key가 지정되지 않았다면, 무작위의 partition에 저장된다.  
  어떤 partition에 저장될지는 `hash(key) % numberOfPartitions`로 결정할 수 있다. 만약 더 높은 유연성을 제공하고 싶다면, producer가  
  직접 mapping 알고리즘을 정의해 partition을 고르도록 할 수도 있다. 참고로 key와 partition number은 절대 같은 것이 아님을 알아두자.

- Key는 string이나 number가 될 수 있고, 보통 business information을 담는다. Partition number는 message queue 내에서 사용되는  
  개념이므로 클라이언트들에게 직접적으로 노출되어서는 안된다.

- 적절한 mapping 알고리즘이 있으면 만약 partition의 개수가 바뀌더라도 메시지들이 여전히 모든 partition에 고르게 분배되도록 할 수 있다.

#### Message value

- Message value는 메시지의 payload이다. Plain text일 수도 있고, binary block일 수도 있다.

> 메시지의 key, value는 key-value store의 key-valye pair와는 다르다. Key-value store에서 key는 unique하며, key를 통해  
> value를 찾을 수 있다. 반면 메시지에서 key는 unique할 필요가 없다. 심지어 필수 값도 아닐 수도 있으며, key를 사용해 value를 찾을 필요도 없다.

#### Other field of message

- Topic: 메시지가 소속된 topic
- Partition: 메시지가 소속된 partition의 ID
- Offset: Partition 내 해당 메시지의 위치. Topic, partition, offset을 이용해 특정 메시지를 찾아낼 수 있다.
- Timestamp: 메시지가 보관된 시점의 timestamp
- Size: 메시지의 크기
- CRC(Cyclic redundancy check): Raw data의 무결성을 보장하기 위해 사용된다.

- 다른 기능들을 지원하기 위해 선택적인 field들이 메시지에 추가되도록 할 수 있다. 예를 들어 메시지를 tag로 필터링하고 싶다면, 메시지가 tag를  
  optional field로 갖도록 하면 된다.

### Batching

- Batching은 이 설계에 구석구석 스며들어 있다. Producer, consumer, 그리고 message queue 모두에서 메시지를 batch로 다룰 수 있다.  
  Batching은 시스템의 성능을 좌우하는 중요한 요소다. 이번 장에서는 message queue 내에서의 batching에 대해 집중적으로 다뤄보자.

- Batching이 시스템의 성능을 향상시키는 데 중요한 이유는 아래와 같다.

  - OS가 메시지를 grouping 해 하나의 네트워크 요청으로 처리할 수 있도록 한다. 따라서 비싼 네트워크 왕복 비용을 줄일 수 있다.
  - Broker가 큰 chunk 단위로 log를 append하는데, 이는 OS에 운영되는 disk cache에 더욱 큰 block들이 순차적으로 write되게 한다.  
    작은 단위가 아니라 큰 단위로 순차적 접근을 하기에 disk에 대한 순차적 접근 처리량이 높아진다.

- 처리량과 latency는 tradeoff 관계를 가진다. 만약 시스템이 latency가 더 중요한 전통적인 message queue로 배포되었다면, 해당 시스템은  
  더 작은 batch size를 사용할 것이다. 이 경우 Disk의 성능은 안 좋아질 것이다. 또한 순차적 disk 접근 처리량이 낮은 것을 보완하기 위해  
  topic 별로 더 많은 수의 partition을 사용할 것이다.

- 지금까지 main disk storage subsystem과 연관된 disk 자료구조에 대해 다뤄보았다. 이번에는 producer와 consumer 각각의 흐름에 대해  
  다뤄보자. 이후 다시 돌아와 message queue의 나머지 부분들을 깊게 다뤄볼 것이다.

### Producer flow

- 만약 producer가 메시지를 partition에 보내고 싶다면, 어떤 broker에 연결해야 할까?  
  첫 번째 선택지로 routing layer를 둘 수 있다. Routing layer로 전달된 모든 메시지들은 _"적절한"_ broker로 전달된다.  
  만약 broker들이 복제되었다면, _"적절한"_ broker는 leader replica가 된다.

![picture 57](/images/SDI2_MQ_10.png)

- 위 그림에 나타난 것처럼, producer가 메시지를 Topic-A의 partition-1에 전달하고 싶다고 해보자. 과정은 아래와 같다.

  - (1) Producer는 routing layer에 메시지를 보낸다.
  - (2) Routing layer는 metadata storage로부터 partition-1을 가지는 replica들 읽고, local에 caching한다.  
    메시지가 도착하면 routing layer는 메시지를 Broker-1에 있는 Partition-1의 leader replica에 전달한다.
  - (3) Leader replica는 메시지를 수신하고, 나머지 follower replica들은 leader replica로부터 데이터를 pull한다.
  - (4) _"충분한"_ 수의 follower replica들이 메시지를 동기화했다면, leader replica는 data를 commit(disk에 저장)하고,  
    이후 데이터가 소비될 수 있도록 한다. 그리고 producer에게 메시지가 전달되었음을 알린다.

> Leader replica: Primary, Follower replica: Read Replica

- Leader, follower replica가 왜 모두 필요할까? 이유는 **fault tolerance**를 위함이다.

- 이 방식은 작동하지만, 몇 가지 단점들이 있다.

  - Routing layer가 있다는 것은 network hop이 증가한다는 뜻이고, 이는 network hop의 증가로 인한 overhead로 인해 network latency를  
    증가시킨다.
  - 이 방식은 요청의 batching을 고려하지 않았다. (Request batching은 효율성을 위한 중요한 요소다.)

- 아래 그림은 향상된 설계안이다.

![picture 58](/images/SDI2_MQ_11.png)

- 위 설계안에서 routing layer는 producer의 일부가 되었고, buffer 컴포넌트가 producer에 추가되었다. 이 둘은 모두 producer client의  
  라이브러리의 일부로써 producer에 설치될 수 있다. 이 변화된 방식은 아래의 이점들을 가져온다.

  - Network hop이 적어져 latency가 감소한다.
  - Producer가 메시지가 전달될 partition을 결정하는 로직을 직접 가질 수 있다.
  - 메모리를 buffer에 두어 한 번의 요청으로 더 많은 양의 데이터를 batching할 수 있게 된다. 따라서 처리량이 증가하게 된다.

- Batch size를 결정할 때는 처리량과 latency의 tradeoff를 고려해야 한다.

  ![picture 59](/images/SDI2_MQ_12.png)

- Batch size가 커지면 처리량은 증가히지만 batch size를 만족시키기 위해 batch를 모아야 하므로 latency는 증가한다.  
  반면 batch size가 작아지면 요청이 더 일찍 발생하므로 latency는 줄어들지만, 처리량은 감소한다.  
  Producer들은 use case에 따라 batch size를 조절할 수 있다.

### Consumer flow

- Consumer는 partition 내의 offset을 명시하고, 해당 위치로부터 발생한 이벤트들의 chunck를 받아간다.

![picture 60](/images/SDI2_MQ_13.png)

#### Push vs pull

- 중요한 요소 중 하나로 broker가 consumer들에게 데이터를 push 할지, 아니면 consumer들이 broker들로부터 데이터를 pull할지 결정해야 한다.

#### Push model

- 장점: 낮은 latency. Broker는 메시지를 수신하자마자 consumer들에게 해당 메시지를 그대로 전달할 수 있다.
- 단점:
  - Broker가 받는 메시지량이 consumer의 메시지 처리량보다 많다면 consumer에게 부하가 발생한다.
  - Broker가 데이터가 전송되는 것을 평가하기 결정하기 때문에 서로 다른 processing power를 가진 consumer들이 모두 원활하게 작동하도록 하기 어렵다.

#### Pull model

- 장점:

  - Consumer가 소비량을 결정한다. 예를 들어 한 집합의 consumer들은 메시지를 실시간으로 처리하고, 다른 집합의 consumer들은 메시지를 batch로  
    처리하도록 할 수 있다.
  - Consumer의 소비량이 메시지가 전달되는 양을 따라잡지 못하면 consumer를 scale out하거나 다시 상황이 괜찮아지면 처리하도록 할 수 있다.
  - Pull model은 batch processing을 위해 더 적합하다. Push model에서 broker는 consumer가 메시지를 즉각적으로 소비할 수 있는 여건인지를  
    알지 못한다. 만약 broker가 consumer에게 메시지를 1번에 1개씩 보내고 consumer가 장애가 났다면, 새로운 메시지들은 buffer에서 대기하게 될  
    것이다. 반면 Pull model은 log에서 consumer의 position 뒤로 가능한 만큼 메시지를 pull해가게 된다. 따라서 data batching에 더  
    적합할 수 밖에 없다.

- 단점: Broker에 메시지가 없더라도 consumer가 계속 데이터를 pulling할 수 있고, 이는 리소스 낭비로 이어진다.  
  이 문제를 해결하기 위해 많은 message queue들은 새로운 메시지를 위해 특정 시간 동안 pull을 대기시키는 polling mode를 지원한다.

- 위의 push, pull model의 비교를 통해 대부분의 message queue들은 pull model을 사용한다.

- 아래 그림은 consumer pull model의 흐름이다.

![picture 61](/images/SDI2_MQ_14.png)

- (1) 새로운 consumer가 consumer group 1에 등록하고 Topic A를 구독하고 싶어 한다. 우선 해당 consumer는 group name을 hashing해서  
  연관된 broker를 찾는다. 즉 동일한 consumer group 내의 모든 consumer들은 동일한 broker에 연결하게 되고, 이는 _coordinator of_  
  _consumer group_ 이라고도 한다. 여기서 conumser group의 coordinator와 처음 본 개략적 설계안의 coordinator service는 다르다는  
  것을 인지하자. 여기서 말하는 coordinator는 consumer group을 관리하고, 개략적 설계안의 coordination service는 broker cluster를  
  관리한다.

- (2) Coordinator가 새로운 consumer가 consumer group 1에 등록되었음을 확인하고, 해당 consumer가 partition-2를 구독하게 한다.  
  구독할 partition을 결정하는 방법에는 round-robin, range 등 다양한 전략들이 있다.

- (3) Consumer는 마지막으로 소비된 offset 이후의 메시지들을 소비한다. 이 offset은 state storage에 의해 관리된다.

- (4) Consumer가 메시지를 처리하고 처리된 마지막 메시지의 offset을 broker에게 전달해 저장시킨다.  
  데이터가 처리되는 순서와 offset을 기록하는 것은 message delivery semantic에 영향을 끼친다. 자세한 설명은 뒤에서 보자.

### Consumer rebalancing

### State storage

### Metadata storage

#### ZooKeeper

### Replication

#### In-sync replicas

##### ACK=all

##### ACK=1

##### ACK=0

### Scalability

#### Producer

#### Consumer

#### Broker

#### Partition

##### Decrease the number of partitions

### Data delivery semantics

#### At-most once

#### At-least once

#### Exactly once

### Advanced features

#### Message filtering

#### Delayed messages & scheduled messages

---
