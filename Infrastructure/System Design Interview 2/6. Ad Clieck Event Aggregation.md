# Ad Click Event Aggregation

- Facebook, YouTube, TickTock 등 온라인 미디어 생태계가 활성화되면서 이러한 플랫폼에 사용되는 광고 비용이 거의 매일 최고치를 찍고  
  있다. 따라서 특정 광고의 클릭 이벤트 등을 추적하는 일이 매우 중요해졌다. 이번 장에서는 Facebook, Google의 규모에서 사용되는 광고 클릭  
  이벤트를 취합하는 시스템을 설계해보자.

- 기술적 설계에 들어가기 전에 이 주제를 조금 더 잘 이해하기 위해 우선 온라인 광고에 대해 살펴보자.  
  온라인 광고의 가장 큰 이점 중 하나는 실시간성 데이터로 관측이 가능하다는 것이다.

- 디지털 광고는 RTB(Real-Time Bidding)이라는 핵심 프로세스가 있으며, 이 프로세스는 디지털 광고가 사고 팔리는 것을 의미한다.  
  아래 그림은 온라인 광고의 프로세스를 나타낸다.

  ![picture 1](/images/SDI2_ACEA_1.png)

- RTB는 주로 1초 내로 완료되기 때문에 RTB의 속도는 매우 중요하다.

- 데이터의 정확도 또한 중요하다. 광고 클릭 이벤트 취합은 온라인 광고의 효율성을 나타내는 핵심 지표로 활용되고, 이 지표에 따라 광고주들이 지불하는 금액이  
  달라지기 때문이다. 광고 클릭 이벤트의 취합 결과에 따라 광고 캠패인 관리자는 광고 대상이나 키워드 및 예산이나 bidding 전략을 조정한다. 온라인 광고에서  
  사용되는 핵심 metric들인 CTR(click-through rate), CVR(conversion rate)는 모두 취합된 광고 클릭 데이터에 의존한다.

## 문제 이해 및 설계 범위 확정

- 아래는 기본적인 요구사항이다.

  - Input data는 여러 개의 서로 다른 서버에 위치한 log file이며, 최신 클릭 이벤트들은 log file의 마지막에 append된다.  
    그리고 이벤트는 `ad_id`, `click_timestamp`, `user_id`, `ip`, `country`의 속성을 갖는다.
  - 매일 10억 번의 광고 클릭이 발생하며, 총 200만개의 광고가 있다.  
    광고 클릭은 매년 30%씩 증가한다.
  - 가장 핵심적으로 지원해야 하는 query는 아래와 같다.
    - 특정 광고의 최근 _M_ 분 내의 클릭 이벤트 회수 조회
    - 최근 _M_ 분 동안 가장 많이 클릭된 광고 _N_ 개 조회
    - 위의 두 query에 대해 데이터가 `ip`, `user_id`, `country`로 필터링될 수 있어야 한다.
  - 예상보다 늦게 도착하는(late-arriving) 이벤트에 대한 처리
  - 중복 이벤트에 대한 처리
  - 시스템의 다양한 컴포넌트에 문제가 생겨 down time이 발생하는 경우에 대한 처리
  - RTB Latency는 1초 미만이어야 한다.
  - 광고 클릭 이벤트의 취합 과정 latency는 수 분 내로 이뤄져야 한다.

- 이번에 설계할 시스템의 요구사항은 아래와 같다.

  - 특정 `ad_id`를 가진 광고에 대해 최근 _M_ 분 동안 발생한 클릭 횟수 취합
  - 최근 _M_ 분 동안 가장 많이 클릭된 _N_ 개의 광고 취합
  - 다양한 속성들로 취합된 결과에 대한 필터링 지원
  - 데이터의 volume은 Facebook 또는 Google의 규모이다.

- 추가적으로 아래의 요구 사항들이 있다.

  - 취합된 데이터가 billing, RTB에 사요되므로 데이터의 정확도가 매우 중요하다.
  - 지연되거나 중복된 이벤트를 적절히 처리해야 한다.
  - 시스템은 부분 장애를 감내할 수 있어야 한다.
  - End-to-end latency는 수 분 내여야 한다.

- 이제 위의 요구사항들에 기반해 필요한 정보들을 수치화해보자.

  - DAU: 10억 명
  - 평균적으로 한 사용자가 광고를 1회 클릭한다 가정했을 때, 매일 10억 건의 광고 클릭 이벤트가 발생한다.
  - Ad click QPS = `10^9 events / 10^5 seconds in a day = 10000`
  - Peak ad click QPS가 평균치의 5배라 가정했을 때 Peak QPS = 50000
  - 한 건의 클릭 이벤트가 0.1KB의 저장 공간을 차지한다고 가정했을 때 매일 소모되는 저장 공간은 `0.1KB * 10억 = 100GB`이다.  
    그리고 매달 필요한 저장 공간은 `100GB * 30 = 3TB`이다.

---

## 개략적 설계안 제시 및 동의 구하기

- 이번에는 query를 위한 API, 데이터 모델, 그리고 개략적 설계안을 다뤄보자.

### Query API 설계

- API 설계의 목적은 클라이언트와 서버 사이의 일종의 약속을 만들기 위함이다. 소비자 앱에서 일반적으로 클라이언트는 해당 프로덕트를 사용하는 end-user이다.  
  하지만 이 시스템의 경우, 클라이언트는 dashboard 사용자(data scientist, product manager, 광고주 등)이며, 이들은 모두 aggregation  
  service에 query를 수행시키게 된다.

- API 설계를 수월히 하기 위해 관련 기능적 요구사항을 다시 보자.

  - 특정 `ad_id`를 가진 광고에 대해 최근 _M_ 분 동안 발생한 클릭 횟수 취합
  - 최근 _M_ 분 동안 가장 많이 클릭된 _N_ 개의 광고 취합
  - 다양한 속성들로 취합된 결과에 대한 필터링 지원

- 데이터에 대한 filtering은 요청에 query parameter를 추가하는 것으로 처리할 수 있기 때문에, 2개의 API만 설계하면 된다.

#### API 1: 최근 _M_ 분 동안 발생한 `ad_id`의 클릭 횟수 취합

| API                                    | 설명                                       |
| -------------------------------------- | ------------------------------------------ |
| GET `/v1/ads/{ad_id}/aggregated_count` | 주어진 `ad_id`의 이벤트 횟수를 취합해 반환 |

- 아래는 이 API의 요청 파라미터 목록이다.

  | Field  | 설명                                                                                               | Type |
  | ------ | -------------------------------------------------------------------------------------------------- | ---- |
  | from   | 시작 시간(기본값: -1분)                                                                            | long |
  | to     | 끝 시간(기본값: 현재 시각)                                                                         | long |
  | filter | 다양한 필터링을 위해 사용된다. 예를 들어 `filter=001`은 US 외부에서 발생한 클릭 이벤트를 제외한다. | long |

- 응답은 아래와 같다.

  | Field | 설명                                  | Type   |
  | ----- | ------------------------------------- | ------ |
  | ad_id | 광고의 identifier                     | string |
  | count | 시작 시간부터 끝 시간까지 취합된 횟수 | long   |

#### API 2: 최근 _M_ 분 동안 가장 많이 클릭된 _N_ 개의 광고 취합

| API                       | 설명                                                          |
| ------------------------- | ------------------------------------------------------------- |
| GET `/v1/ads/popular_ads` | 최근 _M_ 분 동안 가장 많이 클릭된 _N_ 개의 광고를 취합해 반환 |

- 아래는 이 API의 요청 파라미터 목록이다.

  | Field  | 설명                           | Type    |
  | ------ | ------------------------------ | ------- |
  | count  | 검색할 광고 개수               | integer |
  | window | 취합 window의 크기(M), 분 단위 | integer |
  | filter | 다양한 필터링을 위한 파라미터  | long    |

- 응답은 아래와 같다.

  | Field  | 설명                              | Type  |
  | ------ | --------------------------------- | ----- |
  | ad_ids | 가장 많이 클릭된 광고들의 id 목록 | array |

### Data model

- 이 시스템에는 raw data, aggregated data의 2개 데이터 타입이 있다.

#### Raw data

- 아래는 log file에서 raw data가 어떻게 저장되는지를 나타낸다.

```
[AdClickEvent] ad001, 2022-03-04 00:00:01, user 1, 207.148.22.22, USA
```

- 아래 테이블은 구조화된 방식으로 data field가 어떻게 저장되는지를 나타낸다. 이런 데이터는 여러 다른 애플리케이션 서버에 분산되어 저장된다.

  | ad_id | click_timestamp     | user_id | ip            | country |
  | ----- | ------------------- | ------- | ------------- | ------- |
  | ad001 | 2021-03-03 00:00:01 | user1   | 207.148.22.22 | USA     |
  | ad001 | 2021-03-03 00:00:02 | user1   | 207.148.22.22 | USA     |
  | ad002 | 2021-03-03 00:00:03 | user2   | 209.153.56.11 | USA     |

#### 취합된 데이터(Aggregated data)

- 광고 클릭 이벤트가 매 분마다 취합된다고 가정해보자. 아래는 취합된 결과의 한 예시이다.

  | ad_id | click_minute | count |
  | ----- | ------------ | ----- |
  | ad001 | 202103030000 | 5     |
  | ad001 | 202103030001 | 7     |

- 결과 데이터에 대한 필터링을 지원하기 위해 `filter_id`라는 필드를 테이블에 추가하도록 하자.  
  아래 표처럼 동일한 `ad_id`와 `click_minute`를 갖는 레코드들은 `filter_id`로 grouping된다.

  | ad_id | click_minute | filter_id | count |
  | ----- | ------------ | --------- | ----- |
  | ad001 | 202103030000 | 0012      | 2     |
  | ad001 | 202103030000 | 0023      | 3     |
  | ad001 | 202103030001 | 0012      | 1     |
  | ad001 | 202103030001 | 0023      | 6     |

- 아래는 filter 들의 목록이다.

  | filter_id | region | ip   | user_id   |
  | --------- | ------ | ---- | --------- |
  | 0012      | US     | 0012 | \*        |
  | 0013      | \*     | 0023 | 123.1.2.3 |

- 그리고 최근 _M_ 분동안 가장 많이 클릭된 _N_ 개의 광고를 조회하는 query를 지원하기 위해, 아래의 구조가 사용된다.

  | 이름               | 타입      | 설명                                   |
  | ------------------ | --------- | -------------------------------------- |
  | window_size        | integer   | _M_ , aggregation window size(분 단위) |
  | update_time_minute | timestamp | 갱신된 timestamp (1분 단위)            |
  | most_clicked_ads   | array     | JSON 형태로 담긴 id들의 목록           |

#### 비교

- Raw data를 저장하는 것과 취합한 데이터를 저장하는 것의 차이는 아래와 같다.

  - Raw data만 저장했을 때

    - 장점: 전체 data aset이 저장되며 데이터의 filtering 및 재 계산을 지원한다.
    - 단점: 데이터 스토리지가 거대해지며, query 속도가 느리다.

  - 취합된 데이터만 저장했을 때
    - 장점: 데이터 set이 작아지며 query 속도가 빠르다.
    - 단점: 계산된 데이터이므로 데이터의 누락이 발생할 수 있다. 예를 들어 10개 entry가 1개의 entry로 취합될 수 있다.

- 그렇다면 raw data를 저장해야 할까, 아니면 취합된 데이터를 저장해야 할까? 둘 다 저장하면 좋을 것 같다. 이 이유에 대해 알아보자.

  - Raw data를 저장하는 것은 좋은 선택지이다. 만약 뭔가가 잘못된다면 raw data를 사용해 디버깅을 할 수도 있고, 취합된 데이터가 버그로 인해  
    손상되었다면 버그를 고친 후 취합된 데이터를 다시 raw data로부터 계산해낼 수 있다.

  - 취합된 데이터 또한 저장되어야 한다. Raw data의 크기는 매우 커대하다. 크기가 커지면 query의 성능이 굉장히 나빠진다.  
    따라서 read query를 크기가 비교적 작은 취합된 데이터에 대해 수행하면 좋다.

  - Raw data를 backup data로 활용할 수 있다. 일반적으로 다시 계산해내는 일 외에 raw data에 대해 query를 할 일이 많지 않다.  
    오래된 raw data는 비용 절감을 위해 cold storage로 이전시킬 수 있다.

  - 취합된 데이터는 active data로 사용될 수 있다. 그리고 query 성능을 위해 tuning될 수 있다.

#### 적절한 데이터베이스 고르기

- 적절한 데이터베이스를 고르기 위해, 아래의 사항들을 고려해야 한다.

  - 데이터가 어떻게 생겼는지? 데이터가 관계가 있는지? 데이터가 document인지, blob인지?
  - 주로 사용 패턴이 read-heavy한지, write-heavy 한지, 아니면 둘 다 인지?
  - 트랜잭션에 대한 지원이 필요한지?
  - SUM, COUNT 등 OLAP(Online Analytical Processing) function이 필요한 query들이 많은지?

- 먼저 raw data에 대해 생각해보자. 일반적인 상황에서 raw data에 대해 직접 query할 일은 없겠지만, data scientist 또는 ML 엔지니어에게  
  있어서 raw data를 활용해 사용자의 응답 예측, 행동 패턴 분석 등 관련된 인사이트를 도출해내기 좋을 것이다.

- 이전에 봤듯이 평균 write QPS는 10000이며, peak QPS는 50000이다. 따라서 시스템은 write-heavy하다고 볼 수 있다.  
  반면 read 측에서 raw data는 backup 및 재 계산을 위해 사용되기 때문에 read volume은 낮다고 볼 수 있다.

- 이 요구사항을 관계형 데이터베이스가 만족시킬 수 있지만, write 연산을 확장하는게 어려울 수 있다. Cassandra, InfluxDB 등의 NoSQL이 write 및  
  time-ranging query에 최적화되어 있기에 더 좋은 선택지가 될 것이다.

- 또다른 선택지로 데이터를 ORC, Parquet, AVRO 와 같은 columnar data format으로 Amazon S3에 저장할 수 있다.  
  각 파일에 대해 최대 크기를 지정하고 stream processor를 활용해 파일 크기가 일정 크기 이상이 되었을 때 다른 파일로 저장하는 등의 프로세스를  
  처리할 수 있을 것이다. 이 설정은 다른 선택지에 비해 자주 사용되지 않으므로, 이번 설계에서는 Cassandra를 사용하도록 하자.

- 다음으로 취합된 데이터에 대해 다뤄보자. 이 데이터는 기본적으로 time-series의 성격을 가지며 write, read 모두 heavy하다.  
  이는 각 광고에 대해 매 분마다 database에 query해 최근 취합된 결과를 알려줘야 하기 때문이다. 이 기능은 dashboard를 자동으로 새로고침하거나  
  시간에 따라 알림을 trigger하는 데에 좋다. 요구사항에서 총 200만 개의 광고가 있다고 했기에 이 기능은 read-heavy하다. 데이터는 aggregation  
  service에 의해 매분 취합되고 데이터베이스 write되기 때문에 write-heavy하기도 하다. 위에서 raw data를 다루기 위한 Cassandra 등의  
  NoSQL 데이터베이스를 취합된 데이터를 저장하기 위해서도 동일하게 사용할 수 있을 것이다.

- 지금까지 query API 및 데이터 모델에 대해 다뤄보았으니, 이를 모두 개략적 설계안에 녹여 넣어보자.

### 개략적 설계

- 실시간 big data processing에서 일반적으로 데이터는 data stream의 형태로 processing stream내로 들어갔다가 나온다.  
  Aggregation service도 동일한 방법으로 동작하는데, input이 raw data이고 output이 취합된 결과가 된다.

  ![picture 2](/images/SDI2_ACEA_2.png)

#### 비동기 처리 방식

- 현재 있는 설계안은 동기 처리된다. 이는 producer와 consumer의 개수가 항상 동일하지 않기 때문에 좋지 않다. 만약 트래픽이 갑자기 치솟아서  
  consumer들이 처리하지 못할 양의 이벤트가 발생한다면 consumer들은 아마 out-of-memory 에러가 발생하거나 장애가 발생할 수 있을 것이다.  
  이렇게 동기 시스템에서 하나의 컴포넌트가 장애가 난다면, 전체 시스템이 장애가 나게 된다.

- 이를 해결하기 위한 일반적인 해결책으로 Kafka와 같은 message queue를 도입해 producer와 consumer를 decouple할 수 있다.  
  이렇게 하면 전체 프로세스가 비동기가 되고, producer 및 consumer는 독립적으로 확장될 수 있다.

- 이제 이를 반영해 모든 프로세스를 비동기적으로 변경한 설계안을 살펴보자. 아래 그림에서 볼 수 있듯이 Log Watcher, Aggregation Service,  
  Database는 2개의 message queue를 사용해 decoupling되었다. Database Writer는 message queue로부터 데이터를 poll하며  
  데이터를 데이터베이스의 형식에 맞춰 변환하고, 마지막으로 데이터베이스에 write 한다.

  ![picture 3](/images/SDI2_ACEA_3.png)

- 그렇다면 첫 번째 message queue에는 무엇이 저장될까? 아래 표처럼 광고 클릭 이벤트 데이터가 저장된다.

  | ad_id | click_timestamp | user_id | ip  | country |
  | ----- | --------------- | ------- | --- | ------- |

- 두 번째 message queue에는 무엇이 저장될까? 이 message queue는 2가지의 데이터를 저장한다.

  - (1) 매 분마다 취합된 광고 클릭 이벤트 결과
    | ad_id | click_minute | count |
    | ----- | ------------ | ----- |

  - (2) 분 단위로 저장된 가장 많이 클릭된 상위 _N_ 개의 광고
    | update_time_minute | most_clicked_ads |
    | ------------------ | ---------------- |

- 취합된 데이터들을 바로 데이터베이스에 저장하지 않는 이유가 궁금할 수도 있을 것이다. 간단히 말하면, end-to-end로 exactly once semantics를  
  만족시키기 위해 Kafka와 같은 두 번째 message queue가 필요하기 때문이다.(atomic commit)

  ![picture 4](/images/SDI2_ACES_4.png)

#### Aggregation service

- 광고 클릭 이벤트를 취합하는 데에 사용할 수 있는 기술로 MapReduce 프레임워크가 매우 좋은 선택지이다. 그리고 이를 DAG Model을 도입해 구현할 수  
  있다. DAG Model의 핵심은 아래 그림처럼 시스템을 Map/Aggregate/Reduce node 들과 같이 작은 computing unit들로 쪼개는 것이다.

  ![picture 5](/images/SDI2_ACES_5.png)

- 각 node는 하나의 task를 처리할 책임이 있고, 결과를 downstream node들에게 전달한다.  
  각 node들에 대해 다뤄보자.

##### Map node

- Map node는 데이터를 data source로부터 읽고 이를 filtering하고 변환(transform)한다.  
  예를 들어 Map node는 아래 그림처럼 `ad_id % 2 = 0`인 광고를 node 1에 전달하고, 다른 광고들을 node 2에 전달할 수 있다.

  ![picture 6](/images/SDI2_ACES_6.png)

- Map node가 굳이 왜 필요할까? 이를 위한 대안으로 Kafka partition 또는 tag를 도입해 aggregate node가 Kafka에 직접 구독하도록 할 수도  
  있다. 이렇게 하면 동작은 하지만 input data가 clean되거나 정규화되어야 할 경우가 있을 것이다. Map node는 이를 처리하기 위해 존재한다.  
  또다른 이유로 data가 어떻게 발생하는지에 대해 알지 못할 수 있으므로 동일한 `ad_id`를 갖는 데이터가 서로 다른 Kafka partition에 적재될 수도  
  있을 것이다.

##### Aggregate node

- Aggregate node는 메모리에 매 분마다 `ad_id` 별로 발생한 클릭 이벤트의 횟수를 기록한다. MapReduce 패러다임에서 Aggregate node는  
  Reduce의 일부에 해당한다. 따라서 map-aggregate-reduce 프로세스는 사실상 map-reduce 프로세스와 동일하다.

##### Reduce node

- Reduce node는 Aggregate node들에 의해 계산된 취합 데이터를 바탕으로 최종 결과를 만들어낸다. 예를 들어 아래 그림처럼 3개의 aggregate  
  node들이 있고 각각이 해당 node 내에서 가장 많이 클릭된 광고 3개를 갖고 있다고 해보자. 그러면 reduce node는 이들을 다시 취합해 이들 모두에서  
  가장 많이 클릭된 3개의 광고를 계산해낸다.

  ![picture 7](/images/SDI2_ACES_7.png)

- DAG Model은 잘 알려진 MapReduce 패러다임을 정확히 나타낸다. DAG Model은 큰 데이터를 받아 분산 및 병렬 연산을 수행해 big data를  
  작거나 regular-sized data로 변환하기 위해 설계되어 있다.

- DAG Model에서 중간 데이터는 메모리에 저장될 수 있으며 서로 다른 node들은 TCP(node들이 다른 프로세스에서 실행되는 경우) 또는 공유 메모리  
  (node들이 서로 다른 스레드에서 실행되는 경우)를 사용해 소통할 수 있다.

### 주요 use case

- 개략젹으로 MapReduce가 어떻게 동작하는지 이해했으니, MapReduce가 아래 3개의 주요 use case들에 어떻게 사용될 수 있는지 알아보자.

  - 클릭 횟수 취합
  - 최근 _M_ 분동안 가장 많이 클릭된 _N_ 개의 광고 조회
  - 데이터 필터링

#### Use case 1: 클릭 횟수 취합

- 아래 그림에 나타난 것처럼 input 이벤트들은 Map node들에 의해 `ad_id(ad_id % 3)`으로 partition되며 aggregate node들에 의해 취합된다.

  ![picture 8](/images/SDI2_ACES_8.png)

#### Use case 2: 가장 많이 클릭된 _N_ 개의 광고 조회

- 아래 그림은 가장 많이 클릭된 3개의 광고를 조회하는 개략적인 과정을 나타낸다. 3개라는 값은 _N_ 으로, 설정 가능한 값이다.  
  Input 이벤트들은 `ad_id`로 매핑되어 있으며 각 aggregate node는 heap 자료구조를 활용해 해당 node 내의 가장 많이 클릭된 3개의 광고를  
  효율적으로 저장한다. 마지막 단계에서 Reduce node는 9개의 광고를(각 aggregate node가 전달한 3개의 광고들) 3개로 reduce한다.

  ![picture 9](/images/SDI2_ACES_9.png)

#### Use case 3: Data filtering

- "ad001의 미국 내의 광고 클릭 횟수" 등과 같은 data filtering을 지원하기 위해 사전에 filtering 조건들을 정의하고, 이들을 기반으로  
  취합할 수 있다. 예를 들어 ad001, ad002의 취합 결과가 아래처럼 나타났다고 해보자.

  | ad_id | click_minute  | country | count |
  | ----- | ------------- | ------- | ----- |
  | ad001 | 202103030001  | USA     | 100   |
  | ad001 | 202103030001  | GPB     | 200   |
  | ad001 | 202103030001  | others  | 3000  |
  | ad002 | 202103030001  | USA     | 10    |
  | ad002 | 2021203030001 | GPB     | 25    |
  | ad002 | 2021203030001 | others  | 12    |

- 이러한 기술을 star schema라고 하며, 주로 data warehouse에서 많이 사용된다.  
  필터링을 위한 필드들은 dimension이라 하며, 이 접근 방법은 아래의 이점들을 가져다준다.

  - 이해하기 쉽고, 구현하기 쉽다.
  - 추가적인 컴포넌트 없이 기존 aggregation service를 재사용해 star schema에서 더 많은 dimension들을 만들 수 있다.
  - 결과들이 미리 계산되므로 filtering 조건에 따른 결과를 조회하기가 빠르다.

- 물론 이 접근법에도 문제가 있는데, 더 많은 bucket과 record들을 생성한다는 것이다. 이는 filtering 조건이 많으면 많아질수록 더 많아질 것이다.

---

## 상세 설계

- 이번에는 아래의 내용들을 각각 깊이 있게 다뤄보자.

  - Streaming vs batching
  - Time and aggregation window
  - Delivery guarantees
  - Scale the system
  - Data monitoring and correctness
  - Final design diagram
  - Fault tolerance

### Streaming vs batching

- 이전에 본 개략적 설계안은 stream processing system에 속한다.  
  아래 표는 서로 다른 3가지 시스템에 대한 비교를 나타낸다.

  | -         | Services(online system)  | Batch system(offline system)             | Streaming system(near real-time system)      |
  | --------- | ------------------------ | ---------------------------------------- | -------------------------------------------- |
  | 응답성    | 클라이언트에게 빠른 응답 | 클라이언트에게 응답할 필요가 없다.       | 클라이언트에게 응답할 필요가 없다.           |
  | input     | 사용자의 요청            | 유한한 크기를 가진 input, 큰 양의 데이터 | Input이 크기에 제약이 없다(infinite streams) |
  | output    | 사용제에게 보내는 응답   | materialized view, 취합된 metric 등      | materialized view, 취합된 metric 등          |
  | 성능 측정 | 가용성, latency          | 처리량                                   | 처리량, latency                              |
  | 예시      | 온라인 쇼핑몰            | MapReduce                                | Flink                                        |

- 개략적 설계에서는 stream processing과 batch processing이 모두 사용되었다. 데이터가 도착하자마자 처리하고 취합된 결과 데이터를 실시간에  
  근접하게 만들어내기 위해 stream processing을 사용했고, 이전 데이터의 backup 과정을 위해 batch processing을 사용했다.

- 이렇게 batch, stream processing을 모두 처리할 수 있는 시스템 아키텍쳐를 lambda라고 한다. Lambda 아키텍쳐의 단점은 2개의 processing  
  path를 가지기에 유지보수해야할 코드 베이스가 2개가 된다는 것이다. 반면 Kappa 아키텍쳐는 batch와 stream processing을 하나의 processing  
  path로 다루는 아키텍쳐이며, lambda의 단점을 해결해준다. 핵심 아이디어는 단 하나의 stream processing engine만을 사용해 실시간 데이터  
  처리와 끝없는 reprocessing을 수행한다는 것이다.

- 아래 그림은 lambda와 kappa 아키텍쳐의 차이점을 보여준다.

![picture 10](/images/SDI2_ACES_10.png)

- 우리가 이전에 본 개략적 설계안은 이전 데이터를 reprocess함과 동시에 실시간 취합 기능을 제공하기에 Kappa 아키텍쳐를 사용한다는 것을 볼 수 있다.  
  참고로 아래의 "Data recalculation"을 보도록 하자.

#### Data recalculation

- 가끔은 _"historical data replay"_ 라고도 하는 이전 데이터를 재 계산해야하는 상황이 발생할 수 있다.  
  예를 들어 aggregation service에서 심각한 버그를 발견했다면, 버그가 발생한 시점부터 raw data로부터 데이터 취합을 다시 해야 할 것이다.  
  아래 그림은 이러한 데이터의 재계산 과정을 보여준다.

  ![picture 11](/images/SDI2_ACES_11.png)

- 각 과정을 살펴보자.

  - (1) Recalculation service가 raw data 저장소로부터 필요한 데이터를 가져온다. 참고로 이 작업은 batch로 수행된다.
  - (2) 가져온 데이터는 재계산만을 수행하는 aggregation service에게 전달되어 실시간 데이터 취합에 영향을 주지 않도록 한다.
  - (3) 취합된 데이터들이 2번째 message queue에 보내지고, 이후 aggregation database에 update를 수행한다.

### Time

- 데이터의 취합을 진행하려면 timestamp가 무조건 필요하다. Timestamp은 아래의 두 부분에서 만들어질 수 있다.

  - 이벤트 시각: 광고 클릭 이벤트가 발생한 시각
  - Processing time: 클릭 이벤트를 처리하는 aggregation server의 시스템 시각에 의존

- 네트워크 지연과 비동기 처리 방식으로 이벤트가 발생한 시각과 processing time 사이의 간극은 꽤나 클 수 있다.  
  아래 그림에서는 event 1이 aggregation service에 클릭 발생 5시간 후에 도착한다.

  ![picture 12](/images/SDI2_ACES_12.png)

- 만약 취합의 기준이 event time이 된다면 늦게 aggregation service에 도착한 이벤트에 대한 처리를 할 수 있어야 한다.  
  반대로 만약 processing time이 취합의 기준이 된다면 취합 결과의 정확도가 떨어질 수 있다. 따라서 이 둘 사이에 완벽한 해답은  
  없으며, 이 둘의 tradeoff 관계를 다뤄봐야 한다.

  - Event time을 기준으로 처리할 때
    - 장점: 클라이언트가 정확히 언제 광고가 클릭되었는지를 알기에 취합 결과가 정확하다.
    - 단점: 클라이언트 측에서 생성된 timestamp에 의존한다. 클라이언트들은 잘못된 시간 정보를 갖고 있을 수도 있고, 악성 사용자들에 의해  
      잘못된 timestamp를 사용하게 될 수도 있다.
  - Processing time을 기준으로 처리할 때
    - 장점: 서버의 timestamp는 더 안전하다.
    - 단점: 이벤트가 aggregation service에 늦게 도착하는 등의 문제가 있어 취합 결과의 정확도가 떨어질 수 있다.

- 우리가 설계하는 시스템의 요구사항에서 데이터의 정확도가 매우 중요하다고 했으니, 취합의 기준을 event time으로 정했다고 해보자.  
  그렇다면 늦게 도착하는 이벤트들을 어떻게 적절히 처리해야 할까? 이때 이를 _"watermark"_ 라 불리는 기술로 처리할 수 있다.

- 아래 그림에서 광고 클릭 이벤트들은 1분 단위의 window 내에서 취합된다. Event time이 해당 event가 어떤 window 내에 들어갈지를  
  결정하게 된다고 해보자. 그러면 아래 그림에서 event 1과 event 5가 적절한 aggregation window 내에 들어가지 못하게 늦게 도착하므로  
  window 1은 event 2를 놓치고, window 3은 event 5를 놓치게 된다.

  ![picture 13](/images/SDI2_ACES_13.png)

- 이렇게 늦게 도착하는 이벤트들을 더 정확히 처리해내기 위해서 watermark를 사용할 수 있다. Watermark는 단지 aggregation window의  
  연장선에 불과하다. 이렇게 watermark를 사용하면 취합 결과의 정확도를 향상시킬 수 있다. Watermark가 얼마 만큼의 시간을 가질지는 설정 가능하며  
  아래 그림처럼 watermark에 15초를 부여하면 window 1은 이전과 달리 event 2를 처리할 수 있고, window 3도 event 5를 처리할 수 있게 된다.

  ![picture 14](/images/SDI2_ACES_14.png)

- Watermark에 부여된 시간은 비즈니스 요구사항에 의존한다. 부여된 시간이 길다면 더 늦게 도착하는 event까지 처리해 정확도를 향상시킬 수 있지만,  
  시스템의 latency를 증가시키게 된다. 반면 부여된 시간이 짧다면 정확도가 떨어지지만, 시스템의 latency를 낮게 유지할 수 있다.

- 이때 watermark 기술이 지연 시간이 꽤나 긴 이벤트들은 처리하지 못한다는 것을 유의해야 한다. 그리고 watermark를 사용하는 것은 데이터의 정확도를  
  향상시킬 수 있지만, 정확도와 시스템의 latency 사이에 tradeoff 관계가 생긴다는 점도 유의해야 한다.

### Aggregation window

- Window function은 아래의 4가지가 존재한다.

  - Tumbling window(fixed window)
  - Hopping window
  - Sliding window
  - Session window

- 위 4가지 중 tumbling window와 sliding window에 대해 다뤄보자.  
  (나머지 2가지는 이 시스템과 연관성이 상대적으로 떨어져 다루지 않는다.)

- 아래 그림은 tumbling window의 예시인데, 시간이 동일한 길이로 분할되며 각 분할돤 영역은 겹치는 부분이 존재하지 않는다.  
  이러한 tumbling window는 매 분마다 발생한 광고 클릭 이벤트를 취합하는 데에 사용하기 좋다.

  ![picture 15](/images/SDI2_ACES_15.png)

- 아래 그림은 sliding window를 나타낸다. Sliding window에서 이벤트들은 data stream을 가로지르는 window에 의해 grouping되고,  
  특정 주기를 갖는다. Sliding window는 겹치는 부분이 존재할 수도 있다. 이는 우리가 설계하는 서비스에서 최근 _M_ 분동안 가장 많이 클릭된  
  광고 _N_ 개를 조회하는 use case에 사용하기 적합하다.

  ![picture 16](/images/SDI2_ACES_16.png)

### Delivery guarantees

### Scale the system

### Data monitoring and correctness

### Final design diagram

### Fault tolerance
