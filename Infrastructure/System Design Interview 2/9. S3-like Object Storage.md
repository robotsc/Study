# S3-like Object Storage

- 이번 장에서는 Amazon S3와 비슷한 object storage 서비스를 설계해보자.  
  S3는 AWS에 의해 제공되는 object storage이며 RESTful API를 기반으로 하는 인터페이스를 제공한다.  
  아래는 S3에 대한 사실 중 일부이다.

  - 2006년 6월: GA
  - 2010년: Versioning, Bucket Policy, Multipart upload 지원
  - 2011년: SSE, Multi-object Delete, Object Expiration 지원
  - 2013년: S3에 저장된 object 개수 2조개 달성
  - 2014, 2015년: Life Cycle Policy, Event Notification, Cross-Region Replication 지원
  - 2021년: S3에 저장된 object 개수 200조개 달성

- Object storage를 설계하기 전, 우선 몇 가지의 storage system들을 살펴보고, 용어들을 정의하자.

## Storage System 101

- 개략적으로 봤을 때 storage system들은 아래의 3개 카테고리로 나뉜다.

  - Block storage
  - File storage
  - Object storage

### Block storage

- Block storage는 1960년대에 탄생했다. HDD, SSD와 같이 서버에 물리적으로 연결된 storage device를 block storage라 한다.

- Block storage는 저장된 raw block들을 서버에게 volume으로 제공한다. 이렇게 함으로써 storage를 유연하고, 다양한 용도로 사용할 수  
  있게 했다. 서버는 raw block들을 format해 file system으로 사용할 수도 있고, block들에 대한 제어권을 특정 애플리케이션에게  
  넘길 수도 있다. 데이터베이스나 VM engine같은 몇 가지 애플리케이션들은 성능을 쥐어짜내기 위해 block들을 직접 관리한다.

- Block storage는 단지 물리적으로 연결된 storage는 아니다. 빠른 속도를 가진 네트워크로 서버에게 연결될 수도 있고, FC(Fibre Channel),  
  iSCSI 등의 표준 프로토콜로 서버에게 연결될 수도 있다. 개념적으로 네트워크로 연결된 block storage는 여전히 raw block을 다루며  
  서버 입장에서는 물리적으로 연결된 block storage와 동일하게 사용할 수 있다.

### File storage

- File storage는 block storage에 기반해 만들어진 storage system이며 파일과 디렉토리를 더 쉽게 다루기 위해 추상화되어 있다.  
  데이터는 file의 형식으로 계층적 디렉토리 구조 내에 저장된다. 일반적인 용도로 가장 많이 사용되는 storage solution은 file storage이다.  
  File storage는 SMB/CIFS, NFS 등의 일반적인 file-level 네트워크 프로토콜을 사용해 동시에 수많은 서버들로 연결되어질 수 있다.  
  그리고 file storage에 접근하는 서버들은 block 관리, volume formatting 등 복잡한 과정을 관리할 필요가 없다.  
  이러한 file storage의 단순함과 간편함은 특정 organization 내에서 여러 개의 파일, 디렉토리를 공유하기에 너무나 좋은 선택지가 된다.

### Object storage

- Object storage는 block storage, file storage에 비하면 최신 기술에 속한다. 그리고 항상 성능과 안전성, 확장성, 비용에 대해  
  tradeoff 관계가 형성된다. 주로 상대적으로 archiving이나 backup을 위해 사용되는 상대적으로 _"cold"_ 한 데이터를 저장하기 위해  
  많이 사용된다. Objec tsotrage는 모든 데이터를 flat한 구조에 object로 저장한다. 따라서 계층적인 디렉토리 구조가 없다.  
  데이터에 대한 접근은 일반적으로 RESTful API로 제공된다. 그리고 다른 storage들에 비해 상대적으로 처리 속도가 느린 편이다.  
  대부분의 public cloud provider는 Amazon S3, Google Object Storage, Azure Blob Storage와 같이 object storage를  
  제공한다.

#### 3개 storage system들의 비교

![picture 45](/images/SDI2_SOS_1.png)

- 아래 표는 block storage, file storage, 그리고 object storage를 비교한 내용을 가진다.

| 특성            | Block storage                                              | File storage                        | Object storage                        |
| --------------- | ---------------------------------------------------------- | ----------------------------------- | ------------------------------------- |
| Mutable content | Y                                                          | Y                                   | N(object versioning을 사용해야 한다.) |
| Cost            | High                                                       | Medium to high                      | Low                                   |
| Performance     | Medium to high, very high                                  | Medium to high                      | Low to medium                         |
| Consistency     | Strong consistency                                         | Strong consistency                  | Strong consistency                    |
| Data access     | SAS/iSCSI/FC                                               | Standard file access, CIFS/SMB, NFS | RESTful API                           |
| Scalability     | Medium                                                     | High                                | Vast                                  |
| Good for        | VMs(Virtual Machines), Database와 같은 고성능 애플리케이션 | 일반적인 용도의 file system access  | Binary data, unstructured data        |

### 용어 정의

- S3-like object storage를 설계하려면 우선 object storage의 핵심 개념을 정립해야 한다.  
  여기서는 관련 용어들과 그 내용을 정의해보자.

- **Bucket** : Object들의 논리적인 container. Bucket의 이름은 전역적으로 unique해야 하며 데이터를 S3에 upload하기 전, 먼저  
  bucket을 생성해야 한다.

- **Object** : Object는 bucket에 저장할 독립적인 데이터 조각을 말한다. Payload라고도 불리는 object data와 metadata를 가진다.  
  Object data는 저장하고 싶은 순차적인 byte가 될 수도 있고, meatadata는 object에 대한 정보를 담은 name-value pair들로 구성된다.

- **Versioning** : 같은 bucket 내의 한 object의 여러 변형을 저장하도록 하는 기능이다. Bucket-level에서 활성화시킬 수 있다.  
  이 기능은 의도치 않게 삭제되거나 갱신된(overwritten) object를 사용자들이 복구할 수 있게 한다.

- **URI(Uniform Resource Identifier)** : Object storage는 bucket, object에 접근하기 위해 RESTful API들을 제공한다.  
  이를 위해 각 리소스들은 URI로 구별된다.

- **SLA(Service-Level Agreement)** : SLA는 서비스 프로바이더와 클라이언트 사이의 계약이다.  
  예를 들어, Amazon S3 Standard-IA storage class는 아래의 SLA를 제공한다.

  - 여러 개의 AZ에 걸쳐 99.999999999%의 내구성을 위해 설계되었다.
  - 데이터는 하나의 AZ가 장애 나더라도 회복할 수 있다.
  - 99.9%의 가용성을 위해 설계되었다.

---

## 문제 이해 및 설계 범위 확정

- 이번에 설계할 시스템의 요구사항들은 아래와 같다.

### 기능적 요구사항

- Bucket 생성
- Object upload, download
- Object versioning
- Bucket 내의 object들의 목록 조회. `aws s3 ls` 명령과 비슷하다.
- 대용량의(GB 단위 또는 그 이상) 데이터와 작은 데이터(수십 KB)를 모두 효율적으로 저장할 수 있어야 한다.
- 1년에 저장되는 총 데이터량은 100PB이다.

### 비기능적 요구사항

- 데이터 내구성은 99.9999%, 가용성은 99.99%를 만족해야 한다.
- Storage는 효율적이어야 한다. 높은 안정성과 성능을 제공함과 동시에 storage 비용은 최소화해야 한다.

### 추정치 계산

- Object storage는 일반적으로 디스크의 용량이나 IOPS(Input Output Per Second)가 병목 지점이 된다. 간단히 살펴보자.

  - Disk 용량: Object들이 아래와 같이 분포되어 있다고 가정해보자.

    - 1MB 미만의 object들: 20%
    - 1MB~64MB의 object들: 60%
    - 64MB 이상의 object들: 20%

  - IOPS: 하나의 hard disk(SATA interface, 7200rpm)가 random seek를 1초에 100~150번 수행할 수 있다 하자.(100 ~ 150 IOPS)

- 위의 수치들에 기반해 시스템이 보관할 수 있는 object들의 개수를 추정해볼 수 있다. 계산 과정을 단순화하기 위해 small object는 0.5MB,  
  medium object는 32MB, 그리고 large object는 200MB의 크기를 가진다고 가정하자.

- Storage를 40% 정도 사용한다 가정하면, 아래와 같은 수치가 나온다.

  - 100PB = `100 * 1000 * 1000 * 1000MB = 10^11MB`
  - `10^11MB * 0.4 / (0.2 * 0.5MB + 0.6 * 32MB + 0.2 * 200MB) = 6억 8000`개의 object들
  - Object의 metadata가 1KB라 가정하면 metadata 정보들을 위한 공간은 0.68TB가 된다.

- 위에서 나온 수치들을 직접 사용하지 않을 수도 있지만, 설계하려는 시스템의 규모와 제약 조건들을 파악하기에는 좋은 방법이다.

---

## 개략적 설계안 제시 및 동의 구하기

- 설계를 다루기 전, 먼저 object storage의 재미있는 특징들을 먼저 살펴보자.

- **Object immutability** : Object storage가 file storage, block storage와 달리 가지는 주요한 차이점 중 하나는 object  
  storage 내에 저장된 object는 불변하다는 것이다. 기존 object를 삭제하거나 완전히 대체할 수는 있어도, 갱신할 수는 없다.

- **Key-value store** : Object data를 가져오기 위해 object URI를 사용할 수 있다. Object URI는 key가 되며, object data가  
  value가 된다.

```
Request:
GET /bucket1/object1/txt HTTP/1.1

Response:
HTTP/1.1 200 OK
Content-Length: 4567

[4567 bytes of object data]
```

- **Write once, read many times** : Object data에 대한 접근 패턴은 한 번 쓰고 여러번 읽는 특징을 가진다.  
  LinkedIn에 따르면 요청의 95%가 read 연산을 수행한다고 한다.

- **Support both small and large objects** : Object의 크기는 각각 다르며 크거나 작은 데이터를 모두 지원해야 한다.

- Object storage의 설계 철학은 UNIX file system과 매우 유사하다. UNIX에서 local file system에 파일을 저장할 때  
  filename과 file data를 함께 저장하지 않는다. 대신 filename은 inode라는 자료구조에 저장되고 file data는 disk의 다른 위치에  
  저장된다. inode는 file data를 가리키는 file block pointer들을 가진다. 특정 local file에 접근할 때 먼저 inode로부터  
  metadata를 가져오게 된다. 그 후 file block pointer를 따라 디스크에 file data가 저장된 위치를 파악해 데이터를 읽어오게 된다.

- Object storage도 위와 비슷하게 동작한다. inode는 meatdata store가 되며, metadata store는 모든 object metadata를  
  보관한다. Hard disk는 object data가 저장되는 data store가 된다. UNIX file system에서 inode가 file block pointer를  
  통해 hard disk에 저장된 데이터의 위치를 찾아가는 것처럼 object storage에서는 네트워크 요청을 사용해 metadata store가  
  object의 ID를 통해 data store에 저장된 object data를 찾아간다. 아래 그림은 이 둘의 개랴적인 과정을 나타낸다.

  ![picture 47](/images/SDI2_SOS_2.png)

- 이렇게 metadata와 object data를 분리해 저장하는 것은 설계를 단순하게 해준다. Data store는 불변(immutable) 데이터를 저장하는  
  반면, metadata store는 가변(mutable) 데이터를 저장한다. 이러한 분리는 서로 다른 컴포넌트를 독립적으로 구현하고 최적화할 수 있도록 한다.

- 아래 그림은 bucket과 object가 어떤 모습인지 간단히 나타낸다.

  ![picture 48](/images/SDI2_SOS_3.png)

### 개략적 설계안

- 아래 그림은 개략적 설계안이다.

![picture 49](/images/SDI2_SOS_4.png)

- 각 컴포넌트들을 하나씩 살펴보자.

  - Load balancer: 여러 개의 API server들로 RESTful API 요청들을 분산시킨다.
  - API service: RPC들을 IAM service, Metadata service, Date store들로 적절히 orchestrate한다.  
    이 컴포넌트는 수평적 확장을 쉽게 하기 위해 stateless하다.
  - IAM(Identity & Access Management): 이 서비스는 인증, 인가, 그리고 접근 제어를 수행하는 중앙 서비스이다.  
    인증은 요청자의 신원을 파악하고, 인가는 신원에 따라 특정 작업을 수행할 권한이 있는지를 검증한다.
  - Data store: 실제 데이터를 저장하고 반환한다. 데이터와 관련된 모든 연산은 Object ID(UUID)를 기반으로 수행된다.
  - Metadata store: Object들의 metadata들을 저장한다.

- 이제 개략적 설계안에 대해 기본적인 이해가 되었으니, 아래의 object storage의 가장 주된 workflow들을 다뤄보자.

  - Object upload
  - Object download

#### Object upload

![picture 50](/images/SDI2_SOS_5.png)

- Object는 특정 bucket내에 저장된다. `bucket-to-share`라는 이름의 bucket을 만들고, 이 bucket에 `script.txt`라는 파일을  
  저장하는 과정을 살펴보자.

  - (1) 클라이언트는 `bucket-to-share`의 이름을 가지는 bucket을 생성하는 HTTP PUT request를 보낸다.  
    이 요청은 load balancer에서 API service로 전달된다.
  - (2) API service는 IAM을 호출해 해당 사용자가 WRITE 작업에 대한 권한을 가지는지 확인한다.
  - (3) API service는 bucket에 대한 새로운 entry를 metadata store에 생성한다. 이 entry가 생성되면, 성공 응답이 클라이언트에게 보내진다.
  - (4) Bucket이 생성된 후, 클라이언트는 `script.txt`라는 파일을 업로드하는 HTTP PUT request를 보낸다.
  - (5) 다시 IAM으로 권한이 있는지 확인하고, 성공하면 API service는 HTTP PUT request의 payload를 data store에 저장시킨다.  
    Data store는 payload를 object로 저장하며, 저장된 object의 UUID를 반환한다.
  - (6) API service가 metadata database에 새로운 entry를 저장시킨다. `object_id(UUID)`와 `bucket_id`,  
    `object_name` 등 중요한 metadata를 포함한다. 아래 표는 저장된 entry의 예시이다.

  | object_name | object_id                        | bucket_id                         |
  | ----------- | -------------------------------- | --------------------------------- |
  | script.txt  | 239D8443-0122-44F5-043E-C13E345S | 82AA1B2E-F599-4593-B353-1F5FA4FS1 |

- Object를 upload하는 API는 아래와 같다.

  ```
  PUT /bucket-to-share/script.txt HTTP/1.1
  Host: foo.s3example.org
  Date: Mon, 1 Aug 2022 17:03:00 KST
  Authorization: authorization string
  Content-Type: text/plain
  Content-Length: 4567
  x-amz-meta-author: Sangwoo

  [4567 bytes of object data]
  ```

#### Object download

- Bucket은 계층적인 디렉토리 구조를 갖지 않는다. 하지만 bucket 이름과 object 이름을 연결시켜 논리적으로 폴더 구조를 따라할 수 있다.  
  예를 들어 object 이름을 `script.txt`에 대신 `bucket-to-share/script.txt`로 짓는 것이다.

- 특정 object를 가져오기 위해서는 GET 요청에 object이름을 지정해야 한다. 이 API는 아래와 같이 생겼다.

  ```
  GET /bucket-to-share/script.txt HTTP/1.1
  Host: foo.s3example.org
  Date: Mon, 1 Aug 2022 17:03:00 KST
  Authorization: authorization string
  ```

- 이전에 언급했듯이 data store는 object의 이름을 저장하지 않고, 오직 `object_id(UUID)`를 기반으로 한 object 관련 작업만을  
  수행한다. 따라서 object를 download하기 위해서는 먼저 object의 이름을 UUID로 매핑시켜야 한다.

- Object를 다운로드하는 과정은 아래 그림과 같다.

![picture 51](/images/SDI2_SOS_6.png)

- 각 과정을 살펴보자.

  - (1) 클라이언트는 load balancer로 HTTP GET 요청을 보낸다. (`GET /bucket-to-share/script.txt`)
  - (2) API service는 IAM을 통해 요청을 보낸 자가 bucket에 대한 READ 권한이 있는지 검증한다.
  - (3) 검증이 성공하면 API service는 metadata store를 통해 해당 object의 UUID를 가져온다.
  - (4) API service는 data store로부터 UUID를 통해 해당 object data를 가져온다.
  - (5) API service는 object data를 HTTP GET의 응답으로 보낸다.

---

## 상세 설계

- 이번에는 아래의 영역들을 깊게 다뤄보자.

  - Data store
  - Metadata data model
  - Bucket 내의 object들 조회
  - Object versioning
  - 대용량 파일 업로드 최적화
  - Garbage collection

### Data store

- Data store의 설계를 더 깊게 보자. 이전에 봤듯이 API service는 사용자들로부터 발생한 외부 request들을 처리하며, 적절한 내부 서비스가  
  해당 요청을 처리하도록 한다. Object를 저장하고 가져오기 위해 API service는 data store를 사용한다.  
  아래 그림은 object를 upload, download할 때 API service와 data store의 상호작용을 나타낸다.

  ![picture 52](/images/SDI2_SOS_7.png)

#### Data store의 개략적 설계

- Data store는 아래 그림과 같이 3개의 주요 컴포넌트를 가진다.

![picture 53](/images/SDI2_SOS_8.png)

- 각 컴포넌트에 대해 알아보자.

#### Data routing service

- Data routing service는 data node cluster에 접근하기 위한 RESTful 또는 gRPC API를 제공한다.  
  Stateless 서비스이며 서버를 추가함으로써 확장할 수 있다. 이 서비스는 아래의 책임을 가진다.

  - 데이터를 저장하기 위한 최적의 data node를 가져오기 위해 Placement service에 요청
  - Data node들로부터 데이터를 읽고, API service로 반환
  - Data node에 데이터 저장

#### Placement service

- Placement service는 어떤 data node(primary, replicas)에 object를 저장할 것인지 결정한다.  
  이 과정은 cluster의 물리적 정보를 제공하는 virtual cluster map을 사용하여 구현된다. Virtual cluster map은 placement
  service가 사용하는 data node들 각각의 위치 정보를 가져 replica들이 물리적으로 분리되어 있도록 한다. 이러한 분리(separation)는  
  높은 내구성을 제공하기 위한 핵심 속성이다.

- 아래는 virtual cluster map의 예시이다.

  ![picture 54](/images/SDI2_SOS_9.png)

- Placement service는 heartbeat를 사용해 모든 data node들을 지속적으로 모니터링한다. 만약 특정 data node가 설정된 주기 내에  
  heartbeat를 보내지 않으면, placement service는 해당 node를 virtual cluster map에 "down"으로 표기한다.

- 이 서비스는 굉장히 중요하기에 Paxos 또는 Raft 합의 프로토콜을 사용해 5개에서 7개의 placement service node들로 구성된 cluster를  
  운영하는 것이 좋다. 합의(consensus) 프로토콜은 절반 이상의 node들이 healthy하면 전체 서비스가 작동함을 보장해준다.  
  예를 들어 만약 placement service cluster가 7개의 node들로 구성되어 있다면 3개의 node가 장애나도 이를 감래할 수 있다.

#### Data node

- Data node는 object의 실제 데이터를 저장하며 여러 개의 data node들(replication group)으로 데이터를 복제함으로써 안정성과  
  내구성을 보장한다.

- 각 data node에는 data service daemon이 실행되는데, 이 data service daemon은 계속해서 placement service로 heartbeat를  
  전송한다. Heartbeat message는 아래의 중요한 정보들을 포함한다.

  - Data node가 관리하고 있는 disk drive(HDD 또는 SSD)의 개수
  - 각 disk drive에 저장된 데이터량

- Placement service는 data node로부터 heartbeat를 처음 수신하면, 해당 heartbeat를 보낸 data node에 ID를 부여하고,  
  virtual cluster map에 추가하고, 아래의 정보들을 반환한다.

  - Data node의 고유한 ID
  - Virtual cluster map
  - 해당 data node의 데이터가 복제될 data node

#### Data persistence flow

![picture 55](/images/SDI2_SOS_10.png)

- 데이터가 data node로 저장되는 과정을 살펴보자.

  - (1) API service는 object data를 data store로 전달한다.
  - (2) Data routing service는 해당 object를 위한 UUID를 생성하고, 해당 object를 어떤 data node에 저장할지 알아내기 위해  
    placement service에 질의한다. Placement service는 virtual cluster map을 검사하고, primary data node를 반환한다.
  - (3) Data routing service가 primary data node에 UUID와 함께 object data를 저장시킨다.
  - (4) Primary data node는 전달받은 데이터를 저장하고, 2개의 secondary data node들에게 복제시킨다.  
    Primary node는 모든 secondary node들에 데이터의 복제가 완료되었을 때 data routing service에 데이터 저장 성공 응답을 보낸다.
  - (5) API service로 object의 UUID가 반환된다.

- (2)번 단계에서 placement service는 object의 UUID를 받아 해당 object가 저장될 replication group을 찾아 반환한다.  
  이를 어떻게 하는 걸까? 이러한 lookup은 결정적(deterministic)해야 하며, replication group가 추가 또는 삭제 되는 등의 경우도  
  잘 처리해야 한다. 주로 consistent hashing이 이러한 lookup 기능의 구현을 위해 사용된다.

- (4)번 단계에서 parimary data node는 응답을 보내기 전, 모든 secondary node들로 데이터를 복제시킨다. 이렇게 하면 모든 data node들에  
  걸쳐 데이터의 높은 일관성을 보장할 수 있다. 하지만 이러한 일관성은 가장 느린 replica도 복제가 될 때까지 기다려야 하므로 latency와의  
  tradeoff 관계를 형성한다. 아래 표는 일관성과 latency의 tradeoff 관계를 보여준다.

  ![picture 56](/images/SDI2_SOS_11.png)

  - Option 1: 데이터는 모든 3개의 node에 데이터가 저장되면 성공적으로 저장되었다고 간주된다. 이 방식은 가장 일관성이 높지만, latency가 길다.
  - Option 2: 데이터는 primary node에 저장되고, secondary node들 중 하나만 저장되어도 성공적으로 저장되었다고 간주된다.  
    이 방식은 일관성도 중간, latency도 중간 정도를 가진다.
  - Option 3: 데이터는 primary에만 저장되어도 성공적으로 저장되었다고 간주된다. 가장 낮은 일관성을 갖지만, latency가 가장 짧다.
  - Option 2, Option 3는 eventual consistency를 제공한다.

#### 데이터 구성 방식

- 이번에는 각 data node가 어떻게 데이터를 관리하는지 살펴보자. 가장 간단한 방법으로 각 object를 하나의 파일로 저장할 수 있을 것이다.  
  이는 동작하지만, 작은 파일이 많으면 성능이 떨어질 것이다. File system에 작은 파일이 너무 많아지면, 아래의 2개 문제가 발생한다.

  - 많은 양의 data block을 낭비하게 된다. File system은 파일을 별도의 disk block에 저장한다. Disk block들은 모두 동일한  
    크기를 가지며, 이 크기는 volume이 초기화될 때 고정된다. 일반적인 block size는 4KB 정도이다. 4KB보다 작은 파일은 여전히  
    disk block이 4KB이기에 4KB의 공간을 차지할 것이다. 따라서 만약 시스템이 작은 파일을 대량으로 가진다면 많은 양의 data block을  
    낭비하게 될 것이다.

  - 시스템의 inode 용량을 초과할 수 있다. File system은 파일의 위치와 관련된 다른 정보들을 inode라고 불리는 특수한 block에 저장한다.  
    대부분의 file system에서 inode의 개수는 디스크가 초기화될 때 고정된다. 작은 파일이 수백만개가 되면 모든 inode를 소비해버릴 수 있다.  
    또한 OS도 file system metadata를 아무리 caching하더라도 많은 개수의 inode를 잘 처리하지 못한다.

  - 이러한 이유로 작은 object들을 하나의 파일로 저장하는 것은 좋지 못한 선택이다.

- 위의 문제들을 해결하기 위해 작은 object들은 이들끼리 합쳐 하나의 큰 파일로 관리하도록 할 수 있다. 개념적으로는 WAL(Write-Ahead Log)와  
  유사하게 동작한다. Object를 저장하면, 해당 object는 이미 존재하는 read-write 파일에 append된다. 만약 이 read-write 파일이  
  용량 제한(일반적으로 몇 GB)을 초과하면, 해당 read-write 파일은 read-only로 변경되고 새로운 read-write 파일이 생성되어 새롭게  
  저장될 object들을 저장한다. 파일이 read-only로 변경되는 순간부터는 오직 read 요청만을 처리할 수 있다.  
  아래 그림은 이 방식이 동작하는 과정을 보여준다.

  ![picture 57](/images/SDI2_SOS_12.png)

- Read-write 파일에 대한 write 접근은 직렬화되어야 함에 유의하자. 위 그림에 나타난 것처럼 object들은 read-write 파일에 순서대로 하나씩  
  저장된다. 이러한 on-disk layout을 유지하려면 병렬적으로 write 요청을 처리하는 여러 개의 core들은 read-write 파일에 write하기 위해  
  순서를 지켜야 한다. 현대 서버는 core수가 많으며 이렇게 요청을 동시적으로 수행할 때 순서를 지키게끔 하는 제약은 처리량을 현저히 떨어뜨리게 된다.  
  이를 고치고 싶다면 요청을 처리하는 core 각각 만을 위한 read-write 파일을 제공해 각 core가 자신만의 read-write 파일을 사용하게끔 할 수 있다.

#### Object lookup

- 이제 각 파일은 여러 개의 작은 object들을 가지게 된다. 이 경우, data node가 UUID를 기반으로 특정 object의 위치를 어떻게 알아낼까?  
  Data node는 이를 위해 아래의 정보들을 필요로 한다.

  - Object를 포함하고 있는 data file
  - Data file 내의 해당 object의 start offset
  - Object의 크기

- 아래 표는 이러한 lookup을 지원하기 위한 데이터베이스 스키마이다.(테이블명: `object_mapping`)

  | 필드          | 설명                          |
  | :------------ | ----------------------------- |
  | object_id(PK) | object의 UUID                 |
  | file_name     | object를 포함하는 파일의 이름 |
  | start_offset  | 파일 내의 object의 시작 주소  |
  | object_size   | Object의 byte 크기            |

- 위의 mapping 정보를 보관할 장소로 RocksDB와 같은 파일 기반의 key-value store와 RDBMS를 비교해보자.

  - RocksDB: SSDTable을 기반으로 하기에 write는 빠르지만, read가 상대적으로 느리다.
  - RDBMS: 일반적으로 B+ tree 기반의 storage engine을 사용하기에 read는 빠르지만, write는 느리다.

- 위 비교를 토대로 RDBMS가 더 나은 read 성능을 제공하기에, RDBMS를 사용하는 것이 좋아보인다.

- 그렇다면 이 RDBMS를 어떻게 배포해야 할까? 우리가 설계하는 시스템의 규모에서 mapping 테이블의 크기는 매우 크다.  
  수많은 data node들을 가지는 하나의 큰 cluster로 배포할 수 있지만, 이는 관리하기 어렵다. 그리고 mapping data는 각 data node마다  
  서로 격리되어 있다는 점에 유의해야 한다. 즉, 여러 data node들이 데이터를 공유할 필요가 없다. 이러한 속성을 최대한 살리기 위해  
  각 data node마다 RDBMS를 배포하도록 할 수 있다. 파일 기반으로 되어있는 SQLite가 좋은 선택지일 것이다.

#### 수정된 data persistence flow

- Data node에 관련해 몇 가지 수정 사항을 이야기했으니, 이 변경 사항들을 반영한 새로운 object를 저장하는 workflow를 살펴보자.

![picture 58](/images/SDI2_SOS_13.png)

- 각 과정은 아래와 같다.

  - (1) API service가 object 4라는 이름을 가진 object를 저장하는 요청을 보낸다.
  - (2) Data node service가 /data/c의 이름을 가진 read-write 파일에 object 4를 append한다.
  - (3) object 4의 mapping 내용을 담은 row가 `object_mapping` 테이블에 저장된다.
  - (4) Data node가 저장된 object의 UUID를 API service에게 반환한다.

#### Durability(내구성)

- Data storage system에 있어 데이터의 안전성은 매우 중요하다. 99.9999%의 내구성을 제공하는 storage system을 어떻게 만들 수 있을까?  
  장애가 날 수 있는 상황 각각에 대한 고려가 필요하며 데이터가 올바르게 복제되어야 한다.

##### 하드웨어 장애 및 그 외의 장애 요소

- 하드웨어 장애는 어떠한 매개체를 사용하더라도 사실상 불가피하다. 일부 storage media는 다른 것들에 비해 더 나은 내구성을 제공하기도 하지만,  
  99.9999%의 높은 내구성을 위해서는 하나의 hard drive에 의존할 수 없다. 내구성을 높힐 수 있는 증명된 방식 중 하나는 하나의 disk가  
  장애가 나더라도 데이터의 가용성에 영향을 주지 않기 위해 데이터를 여러 개의 hard drive들로 복제하는 것이다.  
  이전에 본 설계안에서, 이 시스템은 데이터를 총 3번 복제한다.

- 1년에 장애가 날 확률이 0.81%인 hard drive를 사용한다 해보자. 이 장애 확률 수치는 모델과 어떤 hard drive를 사용하는지에 따라 달라진다.  
  이 상황에서 데이터를 3번 복제하는 것은 `1 - 0.0081^3 = 0.999999`의 안전성을 제공한다. 이 수치는 매우 개략적인 추정치이다.

- 내구성을 완벽히 측정하기 위해서는 다른 영역에서 발생할 수 있는 장애에 또한 감안해야 한다. 이를 failure domain이라 하는데, 정의는 아래와 같다.

  > Failure domain: 주요 서비스가 문제가 생겼을 때 부정적인 영향을 받는 물리적, 또는 논리적 환경

- 현대 data center에서 서버는 주로 rack에 있으며, rack들은 rows/floors/rooms로 grouping된다.  
  각 rack이 네트워크 스위치와 전원을 공유하기 때문에 하나의 rack 안에 있는 모든 서버들은 rack-level의 failure domain이 된다.  
  현대 서버는 motherboard, processor, 전원 공급 장치, HDD drive 등의 컴포넌트들을 서로 공유한다. 이러한 컴포넌트들은  
  node-level의 failure domain에 속한다.

- 대규모 환경에서 failure domain을 격리하는 좋은 예시를 살펴보자. 일반적으로 data center는 서로 다른 AZ끼리 그 어떠한 것도  
  공유하지 않도록 격리되어 있다. 따라서 장애의 영향 범위를 줄이기 위해 아래와 같이 데이터를 다른 AZ로 복제하도록 할 수 있다.  
  이때 failure domain을 격리하는 것은 직접적으로 데이터의 내구성을 향상시키지는 않지만 자연 재해, 정전 등의 상황 발생 시  
  더 높은 안정성을 제공할 수 있다는 점에 유의하자.

  ![picture 59](/images/SDI2_SOS_14.png)

#### Erasure coding

- 위에서 봤듯이 데이터를 3번 복제하면 대충 99.9999%의 내구성을 제공할 수 있다. 이에 더해 내구성을 더욱 높이는 방법은 없을까?  
  Erasure coding이 한 가지 방법이 될 수 있다. Erasure coding은 데이터 내구성에 대해 조금 다른 방법으로 접근한다.  
  데이터를 서로 다른 서버에 작은 단위(chunk)로 분산시키고, 중복 제거를 위해 parity를 생성한다.

> Parity: 데이터가 한 storage에서 다른 storage로 옮겨지거나 다른 컴퓨터로 옮겨지는 등의 상황이 발생했을 때,  
> 데이터의 누락이 생겼거나 변경 사항이 생겼는지를 검사하는 기술

- 장애가 발생하면 chunk data와 parity를 사용해 데이터를 재건(reconstruct)할 수 있다.  
  아래 그림은 4+2 erasure coding을 보여준다.

  ![picture 60](/images/SDI2_SOS_15.png)

- 각 과정을 하나씩 살펴보자.

  - (1) 데이터가 같은 크기를 가진 data chunk(d1, d2, d3, d4)들로 분리된다.
  - (2) 특정 수식을 사용해 parity p1, p2가 계산된다. 아래는 매우 단순화시킨 예시이다.
    - `p1 = d1 + 2 * d2 - d3 + 4 * d4`
    - `p2 = -d1 + 5 * d2 + d3 - 3 *d4`
  - (3) d3, d4가 node의 장애로 인해 삭제된다.
  - (4) Parity를 만들 때 사용된 수식을 적용해 누락된 데이터인 d3, d4를 d1, d2, p1, p2를 이용해 다시 만들어낸다.

- Erasure coding이 어떻게 동작하는지 확실히 이해하기 위해 또다른 예시를 보자.  
  아래는 8+4 erasure coding의 예시이다.

  ![picture 61](/images/SDI2_SOS_16.png)

- 8+4 erasure coding은 데이터를 동일한 크기를 가지는 8개의 chunk들로 분할하고, 4개의 parity들을 계산해 만들어낸다.  
  이렇게 생긴 12개의 데이터는 모두 동일한 크기를 가진다. 그리고 12개의 데이터는 모두 서로 다른 12개의 failure domain들로 분산된다.  
  Parity를 만들어내는(erasure coding에 사용되는) 수식은 최대 4개의 node들이 장애가 나 데이터가 누락되더라도 남아있는 데이터들로부터  
  누락된 데이터를 다시 만들어낼 수 있도록 보장한다.

- Data router가 object를 조회하기 위해 단 하나의 healty한 node에만 질의하면 되었던 것에 비해 erasure coding에서는 data  
  router가 최소 8개의 healthy한 node들로부터 데이터를 읽어와야 한다. 이는 설계 상에 생기는 tradeoff이다.  
  이렇게 더 높은 내구성과 적은 storage 비용 vs 데이터 접근 속도의 tradeoff가 발생하는 것이다.  
  우리가 설계하는 object storage에서 대부분의 비용은 storage에서 발생하기에 이렇게 erasure coding을 사용하는 것도 좋은 선택이다.

- 그렇다면 erasure coding을 사용할 때 얼마 만큼의 추가 저장 공간이 필요할까? 2개의 data chunk에 대해 하나의 parity block이  
  생성되므로 storage overhead는 50%이다. 반면, 3번 데이터를 복제하는 설계에서 storage overhead는 200%이다.

  ![picture 62](/images/SDI2_SOS_17.png)

- 그렇다면 erasure coding이 데이터 내구성을 어떻게 향상시키는 것일까? 전과 같이 1년 동안 장애 확률이 0.81%인 node가 있다 해보자.  
  Backblaze에 의해 진행된 연구 결과에 따르면, erasure coding을 적용하면 99.999999999%의 내구성에 도달할 수 있다고 한다.

- 아래 표는 데이터 복제와 erasure coding의 장단점을 나타낸다.

  |       특성       |                                          데이터 복제                                           |                                                                        erasure coding                                                                        |
  | :--------------: | :--------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------: |
  |      내구성      |                                  99.9999%(3번의 데이터 복제)                                   |                                                             8+4 erasure coding 시 99.999999999%                                                              |
  | 저장 공간 효율성 |                                     200% storage overhead                                      |                                                                     50% storage overhead                                                                     |
  |  컴퓨팅 리소스   |                                              없음                                              |                                                                  parity 계산을 위해 사용됨                                                                   |
  |    write 성능    |                              복제만 하면 되고, 계산 과정이 없다.                               |                                               disk에 데이터를 쓰기 전 parity를 계산해야 하므로 latency가 증가                                                |
  |    read 성능     | 일반적으로 read는 replica에서 처리된다. 장애가 나더라도 read는 다른 replica에서 수행 가능하다. | 일반적으로 모든 read 요청은 처리되기 위해 cluster 내의 여러 node로부터 데이터를 읽어와야 한다. Node에 장애가 나면 먼저 데이터를 재건해야 하기 때문에 느리다. |

- 요약하자면 latency가 중요한 요인인 애플리케이션에서는 데이터 복제가 자주 사용되며 storage cost를 줄이는게 중요한 상황에서는  
  erasure coding이 자주 사용된다. Erasure coding은 비용 효율성과 내구성 덕분에 굉장히 매력적이지만, data node의 설계를  
  매우 복잡하게 만든다. 따라서 이번 설계에서는 데이터 복제를 사용하도록 한다.

#### 정확성 검증

- Erasure coding은 데이터 내구성을 향상시키면서 storage cost를 줄인다.  
  이제 다른 문제를 살펴보자. 바로 데이터 오염이다.

- 만약 디스크 전체가 장애가 발생하고 이 장애가 파악 가능하다면, data node에 장애가 발생했다고 판단해도 된다.  
  이 경우 erasure coding을 사용해 누락된 데이터를 재건할 수 있다. 하지만 대규모 시스템에서는 disk가 아니라 in-memory 데이터의 오염이  
  빈번하게 발생한다.

- 이 문제는 process 영역 사이에 checksum을 사용해 검증함으로써 해결할 수 있다.  
  Checksum은 데이터 에러를 파악하기 위해 사용되는 작은 data block이다. 아래 그림은 checksum이 만들어지는 과정을 나타낸다.

  ![picture 63](/images/SDI2_SOS_18.png)

- 만약 원본 데이터의 checksum을 알고 있다면, 데이터가 이동된 후 checksum을 계산해내 기존 checksum과 비교할 수 있다.

  - 이 둘이 다르다면 데이터는 오염된 것이다.
  - 이 둘이 동일하다면 데이터가 오염되지 않았을 확률이 매우 높다. 100%로 확신할 수는 없지만, 일반적으로는 오염되지 않았다고 판단할 수 있다.

    ![picture 64](/images/SDI2_SOS_19.png)

- Checksum 알고리즘은 MD5, SHA1, HMAC 등 많이 있다. 좋은 checksum 알고리즘은 데이터가 작게 변경되어도 checksum 값이 매우 다르게  
  계산되도록 한다. 이번 장에서는 MD5와 같이 단순한 checksum 알고리즘을 선택하자.

- 이 설계에서는 각 object의 끝에 checksum을 append하도록 한다. 파일이 read-only로 변경되기 전, 파일 전체에 대한 checksum을 끝에  
  append한다. 아래 그림은 이를 보여준다.

  ![picture 65](/images/SDI2_SOS_20.png)

- 8+4 erasure coding과 더불어 checksum 검증을 수행한다면 데이터를 읽을 때 아래의 과정이 수행된다.

  - (1) Object data와 checksum을 가져온다.
  - (2) 받아온 데이터의 checksum을 계산한다.
    - 가져온 checksum과 계산한 checksum이 일치하면 데이터는 에러가 없는 것으로 간주한다.
    - 두 checksum 값이 다르다면 데이터가 오염된 것으로 간주하고, 다른 failure domain들로부터 관련 데이터를 읽어와 데이터를 복구한다.
  - (3) 8개의 data chunk가 생길 때까지 (1), (2)를 반복한다. 이후 8개 data chunk로 데이터를 만들어내 클라이언트에게 반환한다.

### Metadata data model

- 이번에는 데이터베이스 스키마를 먼저 보고, 데이터베이스의 확장에 대해 다뤄보자.

#### Schema

- 데이터베이스 스키마는 아래의 3개 query를 지원해야 한다.

  - Object 이름을 토대로 Object ID 조회
  - Object 이름을 토대로 object 저장, 삭제
  - 특정 bucket 내에 동일한 prefix를 가지는 object들 조회

- 아래 그림은 스키마 설계를 나타낸다. `bucket`, `object`의 2개 테이블이 필요하다.

  ![picture 66](/images/SDI2_SOS_21.png)

#### `bucket` 테이블 확장하기

- 일반적으로 한 사용자가 생성할 수 있는 bucket 수에는 제한이 있으므로 `bucket` 테이블의 크기는 작다.  
  백만명의 사용자가 있고, 각 사용자가 10개의 bucket을 가지며 각 record가 1KB를 차지한다고 하자. 이는 곧 `100만 * 10 * 1KB = 10GB`의  
  저장공간이 필요하다는 뜻이다. 현대 데이터베이스 서버에서 이 정도 크기는 하나의 서버로 감당할 수 있다. 하지만 데이터베이스 서버를 한 대만  
  사용하면 모든 read 연산을 처리하기 위해 CPU 부족, 네트워크 bandwidth 부족 등의 문제가 생길 수 있다. 따라서 여러 개의 데이터베이스  
  replica로 read 요청을 분산시키는 것이 좋을 것이다.

#### `object` 테이블 확장하기

- `object` 테이블은 object metadata를 저장한다. 우리가 설계하는 규모에서 이 데이터는 한 대의 데이터베이스 인스턴스에서 감당하기에는  
  버거울 것이다. 이때 sharding을 수행해 `object` 테이블을 확장할 수 있다.

- 한 가지 선택지로 `bucket_id`를 기준으로 sharding을 수행해 같은 bucket 내의 모든 object들이 동일한 shard에 저장되도록 할 수 있다.  
  하지만 bucket은 수억개의 object들을 가질 수 있기에 hotspot shard가 발생할 수 있으므로 적절하지 않다.

- 또다른 선택지로 `object_id`로 sharding을 수행할 수 있다. 이 방식은 데이터를 고르게 분산한다는 장점을 가진다. 하지만 object 이름으로  
  object id를 조회하는 query와 object name으로 object를 저장, 삭제하는 query가 모두 URI를 기반으로 하기에 효율적으로 수행되지 못할 것이다.

- 한 가지 해결책으로 `bucket_name`과 `object_name`을 조합해 sharding할 수 있다. 이렇게 하는 이유는 대부분의 metadata 작업들이  
  object URI를 기반으로 진행되기 때문이다. 예를 들어 URI를 통해 object ID를 찾아내거나 object를 upload하게 된다.  
  데이터를 고르게 분산시키기 위해 `<bucket_name, object_name>`의 hash 값을 sharding key로 사용할 수 있다.

- 이러한 sharding 스키마를 토대로 (1), (2)번 query를 수행할 수 있다는 것은 명확하지만 마지막 (3)번 query는 조금 의아하다.

  > (3): 특정 bucket 내에 동일한 prefix를 가지는 object들 조회

#### Bucket 내의 object 목록 조회

- Object storage는 file system과 같이 계층 구조를 사용하지 않고 flat한 구조에 데이터를 저장한다.  
  Object는 `s3://bucket-name/object-name`의 형식으로 접근할 수 있다.  
  예를 들어 `s3://mybucket/abc/d/e/f/file.txt`는 아래의 내용을 가진다.

  - bucket name: `mybucket`
  - object name: `abc/d/e/f/file.txt`

- S3는 사용자들이 bucket 내의 object들을 더 수월하게 정리할 수 있도록 하기 위해 _"prefixes"_ 라는 개념을 제공한다.  
  Prefix는 object의 이름 앞에 붙는 문자열을 말한다. S3는 prefix를 사용해 디렉토리와 유사한 형식으로 object들을 정리할 수 있게끔 한다.  
  하지만 prefix가 디렉토리가 아님에 유의해야 한다. Bucket 내의 object를 prefix로 검색하면, 해당 prefix로 시작하는 object name을  
  가지는 object들만 반환한다.

- 위에서 본 예시의 `s3://mybucket/abc/d/e/f/file.txt`에서 prefix는 `abc/d/e/f/`이다.

- AWS S3 listing command는 일반적으로 아래 3개가 자주 사용된다.

  - (1) 사용자의 모든 bucket 조회: `aws s3 list-buckets`
  - (2) 특정 bucket 내에 동일한 prefix를 갖는 모든 object들 조회: `aws s3 ls s3://mybucket/abc/`  
    이 명령어에서 object name의 prefix 뒤에 더 많은 `/`를 갖는 이름은 common prefix로 처리된다.  
    예를 들어 `CA/cities/la.txt`, `CA/cities/sf.txt`, `NY/cities/ny.txt`, `federal.txt`가 있다 해보자.  
    이 상황에서 `aws s3 ls s3://mybucket/`를 수행하면 `CA/`, `NY/`, `federal.txt`가 반환된다.
  - (3) 재귀적으로 동일한 prefix를 공유하는 object 조회: `aws s3 ls s3://mybucket/CA/ --recursive`  
    (2)번과 동일한 상황이라 하면, 이 명령의 결과는 `CA/cities/la.txt`, `CA/cities/sf.txt`가 반환된다.

#### 단일 데이터베이스

- 우선 단일 데이터베이스를 사용할 때 조회 명령어를 처리하는 query를 알아보자.  
  먼저 특정 사용자의 모든 bucket을 조회하는 query는 아래와 같다.

  ```sql
  SELECT * FROM bucket WHERE owner_id={id};
  ```

- Bucket 내에 같은 prefix를 공유하는 모든 object들을 조회하는 query는 아래와 같다.

  ```sql
  SELECT * FROM object WHERE bucket_id = "123" AND object_name LIKE `abc/%`;
  ```

- 위 query를 사용하면 (2)번 command가 추가적인 `/`가 붙는 것을 common prefix로 처리하는 방식을 애플리케이션 코드에서  
  처리할 수 있다. 또한 애플리케이션 코드를 통해 (3)번 command도 처리할 수 있다.

#### 분산 데이터베이스

- Metadata 테이블이 sharding되면 어떤 shard들이 검색할 데이터를 가지는지를 모르기 때문에 조회 기능을 구현하기 어렵다.  
  가장 직관적인 해결책은 모든 shard들을 검색하고 각 결과를 취합하는 것이다. 이를 위해 아래처럼 할 수 있다.

  - (1) Metadata service가 아래의 query를 모든 shard에 대해 수행한다.
    ```sql
    SELECT * FROM bucket WHERE bucket_id = "123" AND object_name LIKE `abc/%`;
    ```
  - (2) Metadata service가 위 query 결과를 취합해 사용자가 원하는 결과를 반환한다.

- 이 해결책은 동작은 하지만 pagination을 구현하기 까다롭다. 왜 이게 어려운지 알아보기 전에 단일 데이터베이스를 사용할 때  
  pagination을 어떻게 구현하는지 살펴보자. 각 페이지마다 10개의 object들을 보여주도록 할 때, 아래와 같은 query를 사용할 수 있다.

  ```sql
  SELECT * FROM object WHERE bucket_id = "123" AND object_name LIKE `abc/%` LIMIT 10 OFFSET 0;
  ```

- `OFFSET`과 `LIMIT`절이 첫 10개의 object들만 가져오도록 결과를 제한한다. 다음 페이지를 보고 싶다면 OFFSET을 10으로 설정하면 된다.  
  그리고 클라이언트는 자신이 원하는 페이지를 request에 담아 전달하게 된다.

- 이러한 client-server request loop는 cursor가 모든 결과들의 마지막에 도달할 때까지 반복된다.

- 이제 왜 분산 데이터베이스(sharded databases)에서 pagination을 지원하기 어려운지 다뤄보자. Object들이 여러 shard들로 분산되어  
  있기 때문에, 각 shard의 조회 결과 개수는 상이할 것이다. 어떤 shard들은 10개의 object들을 꽉 채워 반환할 수도 있는 반면, 어떤  
  shard들은 일부 또는 결과가 없을 수도 있다. 애플리케이션 코드는 모든 shard에서 데이터를 조회하고 취합하고, 정렬하고, 오직 10개만  
  반환할 것이다. 이 10개에 포함되지 않는 object들은 다음 페이지를 위해 남겨져야 한다. 이는 곧 각 shard가 서로 다른 offset을 가지게  
  될 것이라는 뜻이고, 서버는 모든 shard의 offset을 유지하고 이를 cursor와 연관시켜야 한다는 뜻이다. 수백개의 shard들이 있으면  
  애플리케이션 코드가 추적해야 하는 offset도 수백개가 된다.

- 이 문제를 해결할 수 있는 해결책이 존재하지만, tradeoff가 발생한다. Object storage는 광활한 규모, 그리고 높은 내구성을 제공하는 데에  
  초점이 맞춰져 있기 때문에 object들을 조회하는 성능은 그렇게 중요하지 않다. 실제로 상용 object storage는 object listing 기능의  
  성능을 그렇게 높게 제공하지 않는다. 만약 그래도 object listing을 수월하게 하고 싶다면 `bucket_id`로 sharding된 object listing만을  
  위한 테이블을 생성하면 수십억개의 object들이 있는 bucket이 있더라도 object listing의 성능을 괜찮게 낼 수 있을 것이다.

### Object versioning

- Versioning은 bucket 내의 object의 여러 버전을 유지할 수 있도록 하는 기능이다. Versioning이 있으면 실수로 삭제되거나  
  overwrite된 object를 원하는 상태로 복구할 수 있다. 예를 들어, 특정 파일을 수정하고 기존과 같은 bucket에 같은 object name으로  
  저장했다 해보자. Versioning이 없다면 해당 파일의 이전 버전은 metadata store에서 새로운 버전으로 대체된다. 이번 버전은 삭제되기에  
  해당 버전의 파일은 garbage collector에 의해 삭제될 것이다. 반면 versioning이 있다면 object storage가 해당 파일의 모든  
  버전을 metadata store에 가지고 있기 때문에 이전 버전이 삭제되지 않아 복구가 가능하다.

- 아래 그림은 버전이 있는 object를 upload하는 과정을 보여준다.

  ![picture 67](/images/SDI2_SOS_22.png)

- 각 과정을 살펴보자.

  - (1) 클라이언트가 `script.txt`라는 이름을 가진 object를 HTTP PUT request를 통해 upload한다.
  - (2) API service는 Identity service를 통해 사용자의 신원을 파악하고 bucket에 write 권한이 있는지 확인한다.
  - (3) 확인이 완료되면 API service는 데이터를 data store에 저장한다. Data store는 데이터를 새로운 object로 저장하고  
    API service에게 저장된 object의 UUID를 반환한다.
  - (4) API service는 metadata store에 해당 object의 정보를 저장하기 위해 metadata store를 호출한다.
  - (5) Versioning을 지원하기 위해 metadata store의 테이블이 `object_version`이라는 column을 갖는지 확인한다.  
    이 column은 object versioning이 활성화되어 있을 때만 존재한다. Column이 존재하는 것이 확인되면 기존 record를  
    overwrite하지 않고 기존과 같은 `bucket_id`, `object_name`을 갖지만 `object_id`, `object_version`은 다른 새로운  
    row가 저장된다. `object_id`는 (3)번에서 반환된 저장된 object의 UUID이다. `object_version`은 새로운 row가 삽입될 때  
    생성되는 TIMEUUID이다. Metadata store를 위해 어떤 데이터베이스를 선택하든, 해당 데이터베이스는 특정 object의 최신 버전을  
    검색하는 작업을 효율적으로 수행할 수 있어야 한다. 최신 버전은 같은 `object_name`을 가지는 record들 중 `object_version`,  
    즉 TIMEUUID 값이 가장 큰 record이다. 아래 그림은 versioned metadata가 저장되는 방식을 보여준다.

  ![picture 68](/images/SDI2_SOS_23.png)

- Versioned object를 upload하는 것에 더해 object는 삭제될 수도 있다. 이 경우를 살펴보자.

- Object를 삭제하면 해당 object의 모든 version은 bucket 내에 유지되며, 아래와 같이 _Delete Marker_ 가 저장된다.

  ![picture 69](/images/SDI2_SOS_24.png)

- 위 그림과 같이 _Delete Marker_ 는 object의 새로운 version으로 저장되며, 저장되는 순간 해당 object의 최신 버전으로 취급된다.  
  이후 최신 버전이 _Delete Marker_ 인 object에 GET request를 보내면 `404 Object Not Found` 에러를 반환받게 된다.

### 대용량 파일 업로드 최적화

- 앞 부분에서 추정치들을 계산할 때, object들 중 20% 가량이 대용량이라고 가정했다. 일부는 몇 GB보다 더 클 수도 있다.  
  이러한 대용량의 파일을 직접 upload할 수도 있겠지만, 시간이 오래 걸리게 될 것이다. 만약 upload 도중 네트워크 connection이 실패하면  
  처음부터 다시 시작해야 한다.

- 위와 같은 불편함을 해결하기 위해 큰 object를 작은 부분들로 쪼개고, 작은 부분들 각각을 upload하도록 할 수 있다.  
  모든 작은 부분들의 upload가 완료되면 object store는 작은 부분들로부터 object를 만들어낼 수 있다. 이를 multipart upload라 한다.

- 아래 그림은 multipart upload가 동작하는 과정을 보여준다.

  ![picture 70](/images/SDI2_SOS_25.png)

- 각 과정은 아래와 같다.

  - (1) 클라이언트의 요청은 object storage에 도달해 multipart upload가 시작된다.
  - (2) Data store는 upload를 식별할 수 있는 `uploadID`를 반환한다.
  - (3) 클라이언트가 큰 파일을 작은 부분들로 쪼개고, upload를 시작한다. 클라이언트가 upload하려는 파일이 1.6GB이고, 이를 총 8개의  
    부분들로 쪼개어 각 부분이 200MB의 크기를 갖는 상황이라 해보자. 클라이언트는 첫 번째 작은 부분을 (2)에서 받은 `uploadID`와 함께  
    data store에 upload한다.
  - (4) 특정 부분의 upload가 완료되면 data store는 해당 부분의 MD5 checksum 값을 가지는 `ETag`를 반환한다.  
    `ETag`는 multipart upload를 검증하기 위해 사용된다.
  - (5) 모든 부분들의 upload가 완료되면 클라이언트는 multipart upload request를 마치는 요청을 보내고, 이 요청은 `uploadID`,  
    작은 부분들의 개수, 그리고 `ETag`들을 포함한다.
  - (6) Data store가 각 부분의 번호에 기반해 object를 만들어낸다. Object가 매우 크기에 이 과정은 몇 분이 걸릴 수도 있다.  
    Object의 생성이 완료되면 클라이언트에게 성공 응답이 반환된다.

- 위와 같은 과정의 한 가지 잠재적인 문제점은 작은 부분들은 object가 생성된 이후 더 이상 사용될 일이 없다는 것이다.  
  이는 곧 불필요한 저장 공간을 차지하게 된다. 이 문제를 해결하기 위해 더 이상 필요 없는 작은 부분들을 제거하는 책임을 가지는  
  garbage collection service를 사용한다.

### Garbage collection

- Garbage collection은 더 이상 사용되지 않는, 무의미하게 저장 공간을 차지하는 데이터들을 제거하는 과정을 말한다.  
  아래와 같은 경우에 데이터가 garbage(쓰레기)가 될 수 있다.

  - Lazy object deletion: Object는 삭제 시 실제로 제거되지 않고 _Delete marker_ 가 적용된다.
  - Orphan data: 실행되다가 취소된 multipart upload에서 이미 upload된 작은 부분들이 이에 해당한다.
  - Corrupted data: Checksum 검증에 실패한 데이터들을 말한다.

- Garbage collector는 곧바로 이러한 object들을 data store에서 제거하지 않는다. 삭제 처리된 object들은 주기적으로 compaction(압축)  
  알고리즘에 의해 제거될 것이다.

- Garbage collector는 replica들에서 사용되지 않는 공간도 정리하는 책임을 가진다. 복제를 하는 경우, object가 삭제되면 해당  
  object를 primary 및 replica들 모두에서 제거해야 한다. Erasure coding을 사용하면 만약 8+4 erasure coding을 사용한다면  
  12개의 node 모두에서 해당 object를 제거해야 한다.

- 아래 그림은 compaction이 동작하는 과정을 보여준다.

  ![picture 71](/images/SDI2_SOS_26.png)

- 각 과정은 아래와 같다.

  - (1) Garbage collector가 `/data/b` 내의 object들을 `/data/d`라는 새로운 파일로 복사한다.  
    이때 delete flag가 object 2, object 5에 대해 true로 설정되어 있기에 이 두 object는 복사하지 않는다는 점에 유의하자.
  - (2) 모든 object들의 복사가 완료되면 garbage collector는 `object_mapping` 테이블을 갱신한다.  
    예를 들어 object 3의 `object_id`, `object_size` 값은 그대로 있겠지만 `file_name`과 `start_offset`은 새로운  
    위치대로 갱신된다.

- 위 그림에서 알 수 있듯이 compaction이 수행된 이후의 파일 크기는 이전 파일의 크기보다 작다. 작은 파일들이 많이 생기는 것을 방지하기 위해  
  일반적으로 garbage collector는 compact할 read-only 파일들이 많아질 때까지 기다린다. 이후 compaction 과정에서는 대량의  
  read-only 파일들 내의 object들을 새로운 큰 파일들로 append한다.

---
