# Designing for High Availability, Fault Tolerance and Cost Efficiency

## Backup and Disaster Recovery strategies

- RTO(Recovery Time Objective): Defined as the maximum amount of time in which a service can remain unavailable  
  before it's classed as damaging to the business.

> RTO 내에 Disaster Recovery가 완료되어야 한다.

- RPO(Recovery Point Objective): Defined as the maximum amount of time for which a data could be lost for a service.

![picture 1](/images/AWS_SAA_DHFTCE_1.png)

- Backup and DR Strategies

![picture 2](/images/AWS_SAA_DHFTCE_2.png)

---

## High Availability vs Fault Tolerance

- High Availability: High Availability can be defined by maintaining a percentage of uptime which maintains  
  operational performance, and so this can closely be aligned to an SLA.

- Fault Tolerance: Fault Tolerance expands on High Availability to offer a greater level of protection should components  
  begin to fail in your infrastructure, however, there are usually additional cost implications due to the greater level  
  of resiliency offered.

- Fault Tolerance는 High Availability보다 상위 개념이다. (즉, Fault Tolerance가 높으면 당연히 High Availability를 갖는다.)

- Fault Tolerance를 갖추기 위해 AWS에서 생각해볼 만한 점은 2개 이상의 Region을 사용하는 것이다.  
  물론 하나의 region 내의 다른 2개 이상의 AZ를 사용하는 것도 가능하지만, 이는 High Availability를 위한 것이다.  
  Fault Tolerance는 이보다 한 단계 더 나아가 Region Level의 장애를 감내할 수 있다.

---

## Considerations when planning an AWS DR storage solution

### How to get data in/out of AWS?

- Direct Connect: Supports in/out activity up to 10 Gbps.
- VPN Connection
- Internet Connection

### Large Data Transfer

- Transfer of large data can be done by using physical disk appliances.

- AWS Snowball

  - Physical appliance sent to customer site.
  - 50TB or 80TB in size.
  - Use multiple Snowball appliances to scale to Petabyte scale.

- Snowmobile

  - Exabyte-scale transfer service.
  - Transfer up to 100PB per Snowmobile(pulled by a semi-trailer truck).

- AWS Storage Gateway

  - Another solution which acts as a gateway between your data center and your AWS environment.

### How quickly do you need you data back?

- Things to consider

  - Dependend on your RTO requirements.
  - How critical is the data to the business?
  - Different storage services offer different accessibility options.
  - Your network infrastructure to AWS will impact retrieval rates.

### How much data do you need to import/export?

- The amount of data you have to transfer can affect your storage solution.
- Calculate your target transfer rate
  - Target transfer rate: Length of time it would take you to perform a copy over your connection to AWS.
- Check limitations of storage services used, minimums and maximums, this varies between services,  
  such as S3, Storage Gateway and Snowball.

### Durability

![picture 3](/images/AWS_SAA_DHFTCE_3.png)

### Security

- Do you have to comply with specific governance and compliance controls?
- What encryption methods are available for sensitive data, at rest and in transit?
- It is essential to understand all security controls available.
- Poor security could devastate and damage your business.

### Compliance

- AWS Artifact
  - Allows customers to access and review AWS Compliance Reports.

---

## Using Amazon S3 as a Data Backup Solution

- Should consider S3 classes.

  - Standard Class

    - Durability: 99.999999999%
    - Availability: 99.99%

  - Infrequent Access(IA)

    - Durability: 99.999999999%
    - Availability: 99.9%
    - Used for data that is accessed less frequently than data held in Standard class.
    - Commonly used for backups as you still have immediate access when needed.
    - Use the same SLA as the Amazon S3 Standard class.
    - Uses the same security and data management capabilies as Amazon S3 Standard class.
    - Primary difference is the cost!

  - Amazon Glacier
    - Data is stored in Archives(Not Buckets)
    - Archives can be up to 40TB in size.
    - Archives are stored within Vaults.
    - Different security measures to that of Amazon S3
    - Primarily used for data archiving.
    - To move data into Glacier:
      - Use lifecycle rules from with Amazon S3 to move data from Standard and IA class to Amazon Glacier
      - AWS SDKs
      - Amazon Glacier API
    - Durability: 99.999999999%
    - Encryption in rest and transit
    - Vault Lock security enabling WORM control and vault access policies.
    - Amazon Glacier can be used in overall solution for HIPAA and PCI compliancy.
    - Data Retrieval Options
      - Expedited:
        - Used for urgent access to a subset of an Archive
        - Less than 250MB
        - Data available within 1~5 minutes.
      - Standard
        - Used to retrieve any of your Archives.
        - Data available within 3~5 hours.
        - Cheaper than Expedited.
      - Bulk
        - The cheapest option for data retrieval.
        - Used to retrieve petabytes of data
        - Data available within 5~12 hours.

- S3 CRR(Cross-Region-Replication)

  - By default, S3 does not copy your data across multiple regions.
  - Reducing latency of data retrieval: 가까운 region에 secondary workload를 둬야 latency가 minimize된다.

- S3 Performance - Multipart upload

  - Should be used for any object larger than 100MB.
  - Can increase performance when uploading.
  - Allows an object to be broken down into smaller contiguous parts for an upload.
  - Parts can be uploaded in any order.
  - If errors are received for specific parts, that part will be resent.
  - Amazon S3 reassembles all parts once the upload is complete.
  - Can be done in same time, in parallel.
  - Benefits
    - Speed and throughput
    - Interruption recovery(Only the errornous parts will be re-sent)
    - Management(ex. Multipart upload does not have an expiry time)

- S3 Security

  - IAM Policies
    - Used to allow and restrict access to S3 buckets and objects at a very granular level depending on  
      identities and permissions.
  - Bucket Policies
    - JSON policies assigned to individual S3 buckets. These bucket policies define who or what has access  
      to that Bucket's contents.
  - ACLs(Access Control Lists)
    - Allows you to control which user or AWS account can access a Bucket or object using a range of permissions  
      such as read, write, full control etc.
  - Lifecycle Policies
    - Allows you to automatically manage and move data between classes, allowing specific data to be relocated  
      based on compliance and governance controls.
  - MFA Delete
    - Mult-Factor-Authentication Delete ensures that a user has to enter a 6 digit MFA code to delete an object,  
      preventing accidental deletion and misuse.
  - Versioning
    - Ensures you can recover from misuse of an object or accidental deletion, allowing you to revert back to an  
      older version of the data object.

---

## Using AWS Snowball for Data Transfer

- AWS Snowball

  - Used to securely transfer large amounts of data in and out of AWS in petabyte scale.
  - The Snowball Appliance comes as either a 50TB or 80TB device.

- By default, all data transfered into Snowball Appliance is automatically encrypted using 256-bit encryption keys  
  generated from KMS.

- If your data retrival will take longer than a week using your existing connection method such as Direct Connect,  
  VPN, Internet Connection... , then consider using AWS Snowball.

---

## Using AWS Storage Gateway for On-premise Data Backup

- AWS Storage Gateway allows you to provide a gateway between your own data center's storage systems such as your  
  SAN, NAS or DAS and Amazon S3 and Glacier on AWS.

- The Storage Gateway itself is a software appliance that can be stored within your own data center  
  which allows integration between your on-premise storage and that of AWS.  
  This connectivity can allow you scale your storage requirements both securely and cost efficiently.

- 3 different configurations available.

  - File Gateway
  - Volume Gateway
  - Tape Gateway

- File Gateway

  - Allows you to securely store your files as objects within S3.
  - Ability to mount or map drives to an S3 Bucket as if it was a share held locally.
  - When storing files using the File Gateway, they are sent to S3 over HTTPS and are also encrypted with  
    S3's own server side encryption, SSE-S3.

- Stored Volume Gateways

  - Used to backup your local storage volumes to Amazon S3.
  - Your entire local data set remains on-premise, ensuring low latency data access.
  - Volumes can be between 1GiB ~ 16TiB.
  - Each Storage Gateway can hold maximum 32 volumes. -> Maximum 512 TiB.
  - A storage buffer using on-premise storage is used as a staging point for data.
  - Data is uploaded across an SSL channel and stored in encrypted form.
  - Snapshots can be taken of volumes at any point, and stored as EBS snapshots on S3.
    - Snapshots are incremental, ensuring that only the data that's changed since the last backup is copied,  
      helping to minimize the storage costs on S3.

- Cached Volume Gateways

  - Primary data storaged is Amazon S3, rather than than your own local storage solution.
  - Local data storage is used for buffering and a local cache for recently accessed data.
  - Each volume can be up to 32TiB.
  - The Storage Gateway can support 32 Cached Volume Gateways. -> Maximum capacity of 1024TiB per Cached Volume Gateway.
  - Snapshots of volumes stored on S3 as EBS snapshots(also incremental)
    - Can be attached to EC2 instances as EBS volumes.

- Tape Gateway(Gateway-Virtual Tape Library, Gateway VTL)

  - Allows you to backup data to S3 from your on-premise data center.
  - Leverage Amazon Glacier for data archiving.
  - VTL(Virtual Tape Library) is a Cloud based tape backup solution, replacing physical components with virtual ones,  
    while utilizing your existing tape backup application infrastructure.

---
