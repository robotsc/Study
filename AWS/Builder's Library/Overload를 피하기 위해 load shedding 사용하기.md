# Overload를 피하기 위해 load shedding 사용하기

> - [Amazon Builder's Library - Using load shedding to avoid overload](https://aws.amazon.com/builders-library/using-load-shedding-to-avoid-overload/)

## Introduction

- 저자는 Amazon의 Service Frameworks 팀에서 수년간 근무했다. 저자가 있던 팀은 Amazon Route 54, Elastic Load  
  Balancing 등의 팀들이 서비스를 더욱 빠르게 개발하고, 서비스 클라이언트가 다른 서비스를 더욱 쉽게 호출하도록 돕는 도구를  
  개발하고 있었다. 거기에 더해 다른 팀이 제공하는 수치 측정, 인증, 모니터링, 클라이언트 라이브러리 생성 등의 다양한 설정을  
  손쉽게 많은 팀들이 사용하도록 하는 기능도 개발했다.

- 이때 저자가 맞이한 한 가지 문제점은 성능과 가용성과 관련된 기본적인 수치를 어떻게 제공하느냐에 대한 것이었다.  
  예를 들어, 저자의 팀은 API call의 latency 특성을 알 수 없었기 때문에 클라이언트단의 기본 timeout을 마음대로  
  설정할 수 없었다.

- 이를 해결하는 도중, 각 서비스가 클라이언트들에게 동일한 시점에 맺을 수 있는 connection의 개수를 결정해야하는 문제점이  
  발생했다. 이 설정은 서버에 과부하가 걸리거나, overloading되는 것을 방지하기 위해 필요했다. 더 자세히 말하자면,  
  load balancer의 최대 connection 개수에 알맞게 서버의 max connection을 설정하고 싶어했다.  
  이 당시에는 Elastic Load Balancing이 출시되기 전이었기 때문에, 하드웨어단의 load balancing이 널리 사용되고 있었다.

- Max connection을 결정하는 것은 매우 어려운 문제였다. Max connection이 너무 낮게 설정되어 있으면 서비스는 충분히  
  처리할 수 있음에도 불구하고 load balancer 단에서 요청을 끊어버리게 되고, 반대로 너무 높게 설정되어 있으면 서버가  
  느려지고 응답 불가 상태가 되기 때문이다.

- 이에 대한 대안으로 저자의 팀은 load shedding 기법을 적용했고, 결과적으로 훌륭하게 문제를 해결할 수 있었다.

---

## The anatomy of overload

- Amazon은 시스템이 overloading되기 전 미리 확장할 수 있도록 시스템을 설계한다. 하지만 시스템을 보호한다는 것은 곧  
  protection layer들이 추가될 것임을 나타낸다. 이 과정은 자동 확장으로부터 시작해 우아하게 excess load shedding을  
  하고, 이 메커니즘을 모니터링하고, 지속적으로 테스트하는 과정을 포함한다.

- 부하 테스트를 하면 시스템 활용도에 비례해 latency가 높아지는 경우가 많다. 부하가 많은 상황에서는 thread 경쟁, context  
  switching, GC, 그리고 I/O 경쟁 등의 문제점들이 더욱 명확하게 드러난다. 결과적으로 서비스들은 성능이 더욱 가파르게  
  나빠지는 inflection point를 지나게 된다.

- 이러한 관측의 뒤에는 Universal Scalability Law라는 이론이 있다. 이 이론은 시스템의 처리량이 동시성을 활용하면  
  증가할 수 있지만, 결과적으로는 직렬화할 수 밖에 없는 작업들에 의해 한계가 있음을 나타낸다.

- 불행히도 처리량은 시스템의 자원 한계 뿐만 아니라 시스템의 부하에 의해 더 나빠질 수 있다. 즉, 만약 시스템에 주어진  
  자원이 처리할 수 있는 처리량보다 더 많은 작업을 해야 한다면, 느려진다. 컴퓨터는 과부하된 상태에도 작업을 지속할 수는 있지만,  
  상당 수의 시간을 context switching에 소모하게 되고, 결국 매우 느려지게 된다.

- 클라이언트가 서버와 상호작용하는 분산 시스템에서 일반적으로 클라이언트는 timeout을 두게 되는데, 즉 서버로부터 일정 시간이  
  지날 때까지 응답이 오지 않으면 서버로부터의 응답 대기를 중지하게 된다. 서버에 과부하가 발생해 latency가 클라이언트의  
  timeout보다 길어지게 되면, 요청들은 실패하게 된다. 아래 그래프는 TPS가 증가함에 따라 inflection point를 지나면  
  처리량이 급격히 떨어지면서 latency가 증가하는 모습을 나타낸다.

  ![picture 1](/images/AWS_BL_ULSO_1.png)

- 위 그래프에서 클라이언트의 timeout보다 응답 시간이 길어지면 상황이 나쁜 것임은 알 수 있지만, 정확히 어떻게 얼마나 나쁜지는  
  표현되어 있지 않다. 이를 표현하기 위해 클라이언트 측에서의 가용성과 latency를 함께 표현한 아래 그래프를 살펴보자.  
  일반적인 response time 단위 대신, median response time을 사용했다. Median response time은 전체 요청의  
  50%가 해당 시간 이내에 응답을 받았음을 의미한다. 만약 서버의 median latency가 클라이언트의 timeout과 동일하다면  
  절반 가량의 요청은 timeout이 발생했을 것이고, 클라이언트가 봤을 때의 가용성은 50%가 된다. 즉, latency의 증가가  
  가용성에도 영향을 끼칠 수 있음을 보여준다.

  ![picture 2](/images/AWS_BL_ULSO_2.png)

- 위 그래프는 해석하기 어려울 수 있다. 이를 더 명료하게 표현하기 위해 _처리량(throughput)_ 에서 _goodput_ 을 별도로  
  나타내보자. 처리량(throughput)은 서버에게 전달되는 초당 요청의 개수를 나타내고, goodput은 에러 없이, 적절한 latency  
  이내에 처리된 요청들을 나타낸다.

  ![picture 3](/images/AWS_BL_ULSO_3.png)

### Positive feedback loops

- 과부하된 상태에서 문제는 계속해서 악화된다. 먼저 클라이언트가 timeout을 맞았다는 것은 이미 서버 상태가 좋지 못하다는 것을  
  나타낸다. 거기에 더해 서버가 요청을 처리하기 위해 수행하던 작업들이 낭비된다.

- 문제를 더욱 악화하는 것은 클라이언트들이 종종 요청을 재시도한다는 것이다. 이는 시스템에 부하를 훨씬 더 많이 발생시키게 된다.  
  만약 SoA 구조에서 call graph가 굉장히 크고, 각 layer가 자신만의 재시도를 수행한다면, 가장 하단의 layer에서 발생한  
  부하는 재시도를 다른 layer들로 전이시켜 부하를 기하 급수적으로 증가시킨다.

### Preventing work from going to waste

- 표면적으로 봤을 때 load shedding은 매우 단순하다. 서버가 과부하 상태에 도달하게 되면, 추가적인 요청을 거절함으로써  
  처리하기로 한 요청들에 대해서만 집중할 수 있어야 한다. Load shedding의 목표는 서버가 처리하기로 결정한 요청들의  
  latency를 낮게 유지해 클라이언트 timeout이 발생하지 않도록 하는 것이다. 이 접근법을 사용하면 서버는 수락한 요청들에  
  대한 가용성을 높게 유지할 수 있고, 오직 초과된 요청들의 가용성만 낮아지게 된다.

  ![picture 4](/images/AWS_BL_ULSO_4.png)

- 과부하된 요청을 shedding하고 수락한 요청들의 latency를 낮게 유지하는 것은 시스템의 가용성을 높일 수 있다.  
  하지만 이 방식의 장점을 위 그래프만으로는 제대로 파악할 수 없다. Overall availability는 여전히 아래로 내려가고,  
  이는 나쁘게 보이기 때문이다. 하지만 요점은 서버가 수락한 요청들이 빠르게 처리되기 때문에 높은 가용성을 유지할 수 있다는 것이다.

- Load shedding은 goodput을 유지하고 처리할 수 있는 요청을 최대한 많이 처리할 수 있게 해주며, 이는 서버가 제공하는  
  기본 처리량이 증가하더라도 동일하다. 하지만 load shedding을 수행하는 것 자체도 비용이 들기 때문에, 결과적으로  
  load shedding을 사용하더라도 트래픽이 매우 많아지면 goodput도 함께 낮아진다.

  ![picture 5](/images/AWS_BL_ULSO_5.png)

---

## Testing

- 부하 테스트에는 상당한 시간을 써야 한다. 서비스가 장애가 날 정도로, 그리고 장애를 내고도 남을 정도로 테스트해야 한다.  
  그리고 이를 토대로 위에서 본 그래프와 같은 지표를 그리게 되면 과부하 상태의 성능과 이후 개선점들을 파악할 수 있게 된다.

- 부하 테스트에는 다양한 종류가 있다. 어떤 부하 테스트들은 부하가 증가함에 따라 fleet이 자동으로 확장함을 검증한다.  
  만약 부하 테스트 결과 트래픽이 증가함에 따라 서비스의 가용성이 0에 수렴하게 된다면, 추가적인 load shedding  
  메커니즘을 도입해볼 필요가 있다. 가장 이상적인 부하 테스트 결과는 서비스의 자원 활용도가 최대치에 도달하거나  
  트래픽이 급증해도 안정적으로 서비스가 동작하는 것이다.

- Chaos monkey와 같은 도구들은 서비스를 대상으로 카오스 엔지니어링 테스트를 하도록 도와준다.  
  예를 들어, CPU 과부하를 유발하거나 packet 손실을 유발하는 등 과부하 상황에서 발생할 수 있는 문제점들을 발생시킬  
  수 있다. 또다른 테스트 방식으로 점진적으로 트래픽을 증가시키는 대신, 트래픽은 일정하게 보내면서 서버 개수를 하나씩  
  줄이는 방식이 있다. 이렇게 하면 서버 인스턴스마다의 처리량이 증가하기 때문에, 실질적으로 하나의 서버 인스턴스가  
  처리할 수 있는 트래픽을 알아낼 수 있다. 마지막으로 완전한 end-to-end 부하 테스트는 요청을 처리하기 위해 서비스가  
  의존하는 데이터베이스 등의 다양한 컴포넌트들도 포함하기 때문에 병목 지점들을 알아낼 수 있게 한다.

- 테스트를 할 때는 서버단의 가용성과 latency에 더해 클라이언트 기준에서의 가용성과 latency도 함께 측정해야 한다.  
  클라이언트 측에서 바라본 서버의 가용성이 감소하면 부하를 더욱 증가시켜보자. 만약 load shedding이 동작하고 있다면  
  트래픽이 서비스가 제공하는 처리량보다 훨씬 더 많아지더라도 goodput은 안정적인 수치를 유지할 것이다.

- 과부하를 피할 수 있는 메커니즘을 알아보기 전 부하테스트가 먼저 선행되어야 한다. 먼저 테스트를 함으로써 병목 지점을  
  발견하고, 어떤 보호 메커니즘을 적용해야 하는지 알 수 있기 때문이다.

---

## Visibility

- AWS는 부하로부터 보호하기 위해 어떤 기술을 사용하는지와 무관하게 메트릭과 가시성을 충분히 확보하고, 관찰함으로써 어떤 기술이 적합한지를 지속적으로 검사한다.

- 만약 잘못된 보호 메커니즘이 요청을 거절하면, 거절된 요청들에 의해 서비스의 가용성이 저하된다.  
  만약 서비스가 처리할 수 있는 리소스가 충분히 있음에도 불구하고 max connection이 너무 낮게 설정되는 등의 이유로 요청을 거절하게 되면  
  false positive 수치가 증가하게 된다. 이 false positive 수치는 0이어야 한다. 만약 이 수치가 주기적으로 0이 아닌 값을  
  가지게 된다면, 서비스가 너무 민감하게 tuning되어 있지 않은지, 지속적으로 과부하되는 인스턴스가 있는지, 확장 또는 load balancing에  
  문제가 있는지 등을 확인해야 한다. 이와 같은 경우, 애플리케이션 성능 튜닝을 해야하거나 더 큰 인스턴스 타입으로 바꿔  
  부하를 더 우아하게 처리할 수 있도록 해야 한다.

- 가시성의 경우, 만약 load shedding이 발생해 요청들이 거절되면 어떤 클라이언트의 요청이었는지, 어떤 작업을 호출했는지, 그리고 이 외에  
  보호 메커니즘을 튜닝하는 데에 도움이 될만한 정보들을 항상 함께 확인할 수 있어야 한다. 또한 load shedding에 의해 거절된 트래픽의  
  양도 항상 모니터링해야 한다. 만약 문제가 있다면 우선적으로 capacity를 추가하고 현재 병목 지점을 해결해야 한다.

In terms of visibility, when load shedding rejects requests, we make sure that we have proper instrumentation to know who the client was, which operation they were calling, and any other information that will help us tune our protection measures. We also use alarms to detect whether the countermeasures are rejecting any significant volume of traffic. When there is a brownout, our priority is to add capacity and to address the current bottleneck.

- Load shedding에는 가시성과 관련해 미묘하지만, 중요한 고려 사항이 하나 더 있다. 실패한 요청들의 latency가 서비스가 실제로 처리하는  
  요청들의 latency 관련 메트릭에 영향을 미치지 않도록 해야한다. 요청을 거절하는 latency는 실제로 처리되는 요청들의 latency에 비해  
  현저히 낮다는 것은 자명하다. 예를 들어, 만약 서비스가 받는 트래픽의 60%가 load shedding되어 거절된다면, 서비스가 실제로  
  처리하는 요청들의 latency는 매우 높더라도 median latency는 꽤나 좋아보일 수도 있다.

### Load shedding effects on automatic scaling and AZ failure

- 설정이 잘못된 경우, load shedding에 의해 auto scaling이 동작하지 않게될 수 있다. 다음 예시를 살펴보자.

  - 서비스에 CPU 사용량 기반의 auto scaling 정책이 적용되어 있고, 그와 동시에 비슷한 CPU 사용량에 도달했을 때 트래픽을 거절하는  
    load shedding도 함께 설정되어 있다.

- 위의 경우 load shedding 시스템에 의해 요청이 거절되면서 CPU 부하는 낮게 유지될 것이고, 결과적으로 CPU 사용량 기반의  
  auto scaling 정책은 동작할 일이 없을 것이다.

- AZ 단위 장애에 대응하기 위한 auto scaling 정책을 세울 때도 load shedding 로직을 고려해야 한다.  
  서비스는 우리가 설정한 latency를 지키기 위해 AZ가 제공하는 capacity를 모두 채울만큼 확장할 수 있다.  
  AWS에서는 CPU와 같은 시스템 메트릭들을 살펴보면서 서비스가 capacity limit에 얼만큼 다가가는지를 지속적으로 모니터링한다.  
  하지만 load shedding이 있다면 시스템 메트릭은 충분히 낮은데 요청이 거절되는 상황이 발생할 수 있으며, 결국  
  AZ 장애를 위해 마련된 auto scaling 정책이 적용되지 않게될 수 있다. 따라서 load shedding을 사용하는 경우, fleet의  
  capacity와 여유 공간이 항상 어떤 상태인지를 알 수 있도록 테스트를 더욱 정확히 진행해야 한다.

- 사실 치명적이지 않고 순간적으로 발생하는 대용량 트래픽에 대응해 비용을 아끼기 위해 load shedding을 사용할 수도 있다.  
  예를 들어, `amazon.com`의 웹사이트 트래픽을 감당하는 fleet이 있다고 해보자. 이때, 크롤러에 의해 수행되는 검색 트래픽의 경우에는 확장을  
  할 때 고려할 요소가 아닐 가능성이 높다. 하지만 이런 결정을 할 때는 굉장히 조심해야 한다. 모든 요청을 처리하는 데 드는 비용이 절대 동일하지  
  않고, 사람이 만들어내는 트래픽과 크롤러가 발생시키는 트래픽을 함께 고려해 AZ 단위의 가용성을 확보하는 것은 고도화된 설계, 지속적인 테스트  
  등이 필요하다. 그리고 만약 서비스의 클라이언트들이 서비스가 이렇게 구성된 것을 모른다면, AZ 장애가 발생했을 때 load shedding이 발생하는  
  것이 아니라 AZ 장애에 대처하지 못한 것처럼 보여 가용성이 떨어진 것처럼 느끼게 될 수 있다.  
  이러한 이유들 때문에 SoA의 경우, 이런 설계를 최대한 빠르게 시스템에 녹여넣어야 한다.

> 잘못된 설계를 한다면 AZ 단위의 장애 감래는 성공했지만, load shedding에 의해 사용자의 요청이 실패한다는 말이다.  
> 그리고 이를 사용자는 AZ 단위 장애에 대한 대처를 하지 못한 것으로 인지하고, 가용성이 떨어진다고 생각한다는 것이다.

---

## Load shedding mechanisms

- Load shedding과 함께 예측 불가능한 시나리오들을 논의하는 것도 좋지만, 장애를 발생시킬 수 있는 예측 가능한 상황들을 보는 것도  
  중요하다. AWS의 서비스들은 capacity를 더욱 추가하지 않고 AZ 단위의 장애를 감래하기 위해 미리 충분한 여유 capacity를  
  가진다. 그리고 클라이언트들에게 형평성을 제공하기 위해 throttling을 사용한다.

- 하지만 이런 보호 메커니즘들과 운영 노하우들에도 불구하고, 서비스는 어느 순간 자신의 capacity를 모두 사용하게 되고, 결국  
  시스템은 과부하된다. 이런 상황의 이유는 예상하지 못한 트래픽의 급증, fleet capacity의 갑작스런 장애(잘못된 배포 등), 클라이언트들이  
  처리 비용이 싼 요청들에서(cache read 등) 비싼 요청(cache miss, write 등)을 더 많이 호출하는 등의 상황이 있다.  
  서비스에 부하가 발생하면, 먼저 처리하기로 결정한 요청들은 마무리를 할 수 있게 해야한다. 이 부분에서는 AWS가 부하를 관리하기 위해  
  사용한 여러 가지 기법들과 고민점들을 살펴보자.

### Understanding the cost of dropping requests

- 테스트를 수행할 때는 goodput이 안정적으로 유지되는 것보다 훨씬 더 많은 부하를 일으켜봐야 한다.  
  이런 접근법의 중요한 이유 중 하나는 load shedding이 동작해 요청을 거절하는 상황이 발생하는 경우, 요청을 거절하는 비용이  
  가능한 한 최소한이 되도록 하기 위함이다. 사소한 로그문, socket 설정 같은 부분에서 요청을 거절하는 비용이 필요 이상으로 증가할 수 있.

- 드문 경우들에서 요청을 빠르게 거절하는 것이 요청을 잡고있는 것보다 비용이 더 발생할 수 있다.  
  이런 경우에는 거절된 요청들을 성공적으로 처리되는 요청들의 latency와 일치하도록 일부러 붙잡을 수 있다.  
  하지만 이것 또한 요청을 붙잡고 있는 비용이 최소한으로 적을 때 해야한다. 예를 들어, 애플리케이션 스레드를 사용하는 경우는 바람직하지 못하다.

### Prioritizing requests

- 서버가 과부하되었을 때, 들어오는 요청들 중 처리할 것과 거절할 것을 분류할 수 있는 기회가 주어진다.  
  서버가 받을 가장 중요한 요청은 아마 load balancer로부터 받는 ping request일 것이다. 만약 서버가 적절한 시간 내에  
  load balancer의 ping request에 응답하지 않는다면, load balanancer가 해당 서버에 특정 주기 동안 새로운 요청들을  
  전달하지 않게 되고, 서버는 유휴 상태로 놓여지기 때문이다. 이렇게 ping request 말고도 서비스마다 요청에 우선순위를  
  부여해 처리할 수 있어야 한다.

- `amazon.com`의 데이터를 렌더링하는 웹 서비스를 생각해보자. 이때, 검색 엔진과 크롤러에 의해 발생하는 트래픽보다 사람이  
  만들어낸 트래픽을 처리하는게 훨씬 더 중요할 것이다. 물론 크롤러의 요청도 중요하지만, peak 시간대가 아닐 때 처리해도 괜찮다.  
  하지만 `amazon.com`과 같이 많은 수의 서비스가 상호 작용하는 복잡한 환경의 경우, 만약 서비스들에 각각 지정된 우선순위가  
  모두 다르다면, 시스템적인 가용성이 저하될 것이고 일부 작업들은 낭비될 것이다.

- 우선순위를 매기는 작접과 throttling을 함께 사용해 엄격한 throttling ceiling을 피하면서도 서비스를 과부하로부터 보호할 수 있다.  
  Amazon에서는 클라이언트에 설정된 throttle limit보다 더 많은 요청을 burst할 수 있는 경우, 이 클라이언트의 초과 요청들은  
  다른 클라이언트들의 quota를 준수한 요청보다 우선순위가 낮아진다. 이런 burst capacity가 사용 불가능해지는 상황을 최소화하기  
  위해 Amazon은 요청을 적절한 서비스에 전달하는 placement 알고리즘의 고도화와 더불어 예측 불가능한 workload에 대응하기 위해  
  미리 충분한 capacity를 가진 fleet을 provisioning한다.

### Keeping an eye on the clock

- 서비스가 수신한 요청을 처리하던 도중 클라이언트의 timeout을 지났음을 알게되었을 때, 두 가지 선택지가 있다.  
  하나는 하던 작업을 중단하고 빠르게 요청을 실패 처리하는 것이고, 다른 하나는 요청을 계속 처리하고 뒤늦은 (클라이언트는 받지 못하는) 요청을  
  반환하는 것이다. 서버의 관점에서는 성공적인 응답을 반환한 것이지만, 클라이언트의 관점에서는 timeout error가 발생하게 되는 것이다.

- 이런 작업 낭비를 회피하는 방법 중 하나는 클라이언트가 각 요청에 timeout과 관련된 정보를 함께 보내, 서버에게 자신이 얼마나  
  기다릴 것인지를 알려주는 방법이 있다. 서버는 이 정보를 활용해 요청을 처리하던 도중 timeout을 지났음을 알게되면 fail fast할 수 있다.

- 이런 timeout에 관련된 정보는 duration 혹은 절대 시간(absolute time)으로 표현될 수 있다.  
  하지만 불행히도 분산 시스템에 포함된 많은 서버들은 정확히 동일한 시간 정보를 가지지 못한다.  
  이에 대응하기 위해 Amazon Time Sync Service는 EC2 인스턴스들의 시간을 각 AWS region에 있는 atomic clock과 함께  
  인공 위성도 함께 활용해 정확히 동기화할 수 있는 기능을 제공한다. 잘 동기화된 clock은 로깅 목적으로도 매우 중요하다.  
  시간이 잘못 동기화된 두 서버의 로그를 시간대를 토대로 분석하는 것은 트러블슈팅을 훨씬 더 복잡하고 어렵게 만들기 때문이다.

- "시간을 관리" 하는 또다른 방법 중 하나는 시간을 단일 인스턴스에서 측정하도록 하는 것이다. 서버는 다른 서버와의 소통 없이 local에서 시간을  
  관리하는 작업을 훨씬 더 수월하게 할 수 있다. 하지만 불행히도 duration을 통해 timeout을 나타내는 것도 그만의 문제점들이 있다.  
  그중 하나로, 사용하는 타이머는 항상 모노토닉해야 하며, 서버가 NTP(Network Time Protocol)로 동기화되더라도 시간이 절대  
  뒤로 가면 안된다. 그리고 훨씬 더 어려운 문제점 중 하나로, duration을 측정하기 위해 서버는 스톱워치를 언제 시작해야 하는지  
  알아야 한다. 극도로 과부화된 상황을 가정하면, 수많은 거대한 양의 요청들이 TCP(Transmission Control Protocol)에 의해 queuing될  
  수 있으며, 서버가 buffer에서 요청을 받아 실제로 요청이 처리되는 시각에는 이미 클라이언트 timeout이 발생한 상태일 수 있다.

- AWS에서는 클라이언트가 요청에 timeout hint를 함께 전달하면, 이 정보를 최대한 투명하게 전달하려고 노력한다.  
  SoA의 경우, 요청이 처리되기 위해 여러 서비스를 왔다갔다 해야할 수 있는데, AWS는 이에 대응하기 위해 각 hop 사이에 "남은 시간"에 대한 정보를  
  downstream 서비스에 전달해 전체 call chain의 끝에서 응답이 클라이언트 timeout없이 전달되기 위해 남은 시간이 얼마인지를 알 수 있게 한다.

- 서버가 client deadline을 알게 된다면, 이제 deadline을 지키기 위한 수단을 서비스의 어느 부분에 구현해야 하느냐의 의문이 생긴다.  
  만약 서비스에 request queue가 있다면, 이를 활용해 각 요청을 queue에서 꺼내와 처리하기 전에 timeout을 평가할 수 있다.  
  하지만 이것도 꽤나 복잡한 작업인데, 요청이 처리되는지 얼마나 걸릴지를 미리 알기 어렵기 때문이다. 일부 시스템은 API 요청을 처리하는 데에  
  걸리는 시간을 지속적으로 모니터링해 클라이언트로부터 전달된 deadline이 예상되는 latency를 초과하는 경우 미리 실패 처리한다.  
  하지만 실세계의 시스템은 이렇게 단순하지 않다. 예를 들어 cache hit은 cache miss보다 더 빠르게 요청을 처리할 수 있는데,  
  요청을 실제로 처리하기 전에는 cache가 hit일지, miss일지 미리 알 수 없다. 또는 서비스의 백엔드 리소스가 partitioning되어 있고,  
  그 중 일부 partition의 처리 속도가 느려졌을 수도 있다. 이렇게 요청에 걸리는 시간을 미리 예측하기란 정말 어렵다.

- AWS는 복잡성과 tradeoff를 고려해 결국 요청을 쌓아두고 처리 가능한 요청인지 판별하는 대신, 클라이언트가 timeout hint를  
  보내도록 걍요하고, "per-request time-to-live" 정책을 강요하고, 거절된 요청들은 빠르게 버리는 방식을 선택했다고 한다.

### Finishing what was started

- 이미 진행된 의미 있는 작업이 낭비되는 일은 별로 좋지 못하며, 특히 과부하인 상태의 경우 더욱 그렇다.  
  진행된 작업을 중단하는 것은 과부하를 더욱 악화시킬 수 있는데, 클라이언트들이 종종 서비스가 적절한 시간 내에 응답하지  
  않는 경우 재시도하기 때문이다. 이런 일이 발생하면 리소스를 소모하는 하나의 요청이 리소스를 소모하는 다수의 요청으로 바뀌고,  
  서비스에 부하를 몇 배 더 발생시키게 된다. 클라이언트는 timeout을 맞이해 재시도할 때, 기존 connection을 버리고 새로운 별도의  
  connection을 만들고 요청을 보내게 된다. 그렇기에 만약 서버가 기존 요청에 대한 처리를 완료하고 응답을 보낼 때 클라이언트는  
  새로운 재시도한 요청에 대한 응답만을 기다리고 있기 때문에 클라이언트는 결코 기존 요청에 대한 응답을 받을 수 없게될 수 있다.

- 이런 작업의 낭비와 관련된 문제들 때문에 서비스들은 유한한 작업(bounded work)를 수행하도록 해야 한다.  
  일례로 대부분의 경우, 대량의 dataset을 반환해야 하는 API를 만들 때는 pagination을 지원하도록 한다. 이런 API는 결과의 일부를  
  반환하며, 클라이언트가 다음 결과를 가져올 때 사용할 token을 함께 반환한다. 이렇게 서버가 요청을 수행하기 위해 사용할 메모리, CPU,  
  네트워크 bandwidth에 대해 아는 것은 admission control을 추가하는 데에 큰 도움이 된다.

- 요청에 우선순위를 매겨 중요도를 판단하는 기준 중 하나로 클라이언트가 API를 사용하는 방식을 사용할 수 있다.  
  예를 들어, 서비스가 제공하는 API가 `start()`, `end()`가 있고, 작업이 끝나기 위해서 클라이언트는 이 두 API를 모두 호출해야만  
  한다고 해보자. 이런 경우, 서비스는 `start()` 보다 `end()` 요청을 더욱 중요하게 여겨야 한다. 만약 `start()`를 더 중요하게  
  여긴다면 클라이언트가 자신이 시작한 요청을 마무리하지 못하게될 수 있기 때문이다.

- Pagination도 작업의 낭비가 발생할 여지가 있는 부분 중 하나이다. 클라이언트가 서비스가 반환하는 데이터를 순회하기 위해 순차적으로  
  요청을 보낸다고 해보자. 만약 `N-1` 번 페이지에서 에러를 맞이하게 되면, 그 이전에 호출했던 `N-2` 번의 API 호출은 모두 낭비되고,  
  클라이언트는 처음부터 다시 요청하게 될 것이다. 이로써 위에서 본 `end()` 요청과 같이 첫 번째 페이지에 대한 요청이 이후 페이지에 대한  
  요청보다 중요도가 높아야 한다는 사실을 알 수 있다. 그리고 이런 이유로 동기 작업으로 처리되는 작업을 무한히 하는 것이 아니라,  
  paginate해 유한한 작업들로 나눠야하는 이유를 알 수 있다.

### Watching out for queues

- 내부에 queue를 두고 사용하는 경우, 요청의 duration을 살펴보는 것도 도움이 된다. 많은 현대 애플리케이션들은 in-memory queue를  
  사용해 요청을 처리할 thread pool과 연결한다. 실행자(executor)를 가진 웹 서비스 프레임워크는 일반적으로 앞에 queue를 둔다.  
  그리고 TCP 기반 서비스의 경우, OS는 각 socket 앞에 buffer를 두고 관리하며, 이 buffer에 처리되기를 기다리는 요청들이  
  많이 쌓일 수 있다.

- Queue에서 요청을 빼올 때, 해당 요청이 얼마나 오랫동안 queue에 보관되어 있는지 알 수 있다.  
  이 duration은 서비스 메트릭에 기록되면 좋다. 그리고 queue의 크기를 유한하게 두는 것에 더해, 들어오는 요청들이 queue에 보관될  
  최대 시간을 지정하고 만약 이 시간보다 오랫동안 보관되는 요청들이 있다면, 해당 요청들을 제거하는 것이 좋다.  
  이 방식은 서버가 더 새로운 요청들을 처리하고, 처리에 성공할 가능성을 최대한 높여준다. 이 접근법을 고도화해 AWS에서는  
  protocol이 지원하는 경우 LIFO(Last In First Out) queue를 사용한다.  
  (HTTP/1.1 pipelining은 LIFO queue를 지원하지 않지만, HTTP/2는 일반적으로 지원한다.)

- Load balancer 또한 서비스에 과부하가 걸렸을 때 surge queue라는 기능을 통해 들어오는 새로운 요청 혹은 connection들을  
  queueing할 수 있다. 이 경우 문제가 생길 수 있는데, 서버가 최종적으로 queue로부터 요청을 받아가더라도, 해당 요청이 얼마나  
  오랫동안 queue에 보관되어 있던 것인지 알 수 없기 때문이다. 이를 방지gk기 위해 초과되는 요청들을 queueing하지 않고 바로  
  빠르게 실패처리(fail-fast) 하는 것이 더 낫다(spillover). AWS의 경우, CLB(Classic Load Balancer)는 surge queue를  
  사용하며 ALB(Application Load Balancer)는 초과 트래픽을 바로 거절한다. 설정이 어떻든 관계없이 surge queue의 크기,  
  spillover 양 등 load balancer의 메트릭은 지속적으로 모니터링되어야 한다.

- AWS는 queue를 모니터링하는 것이 매우 중요하다고 생각한다. 실제로 예상하지 못한 곳(시스템, 라이브러리 등)들에서  
  queue를 사용하고 있는 경우가 꽤 많다.

### Protecting against overload in lower layers

- 서비스는 꽤나 많은 layer들로 구성되는데, load balancer부터 netfilter와 iptables를 제공하는 OS까지, 서비스 프레임워크부터  
  코드단까지 여러 layer들이 있으며 각 layer들은 서비스를 보호하기 위한 기능을 제공한다.

- NGINX와 같은 HTTP proxy는 max connection(`max_conns`) 기능을 제공하는데, 이 기능을 사용하면 백엔드 서버에게  
  전달될 활성 요청 혹은 connection의 최대 개수를 제한할 수 있다. 이는 유용한 기능이지만, 위에서 봤듯이 최후의 수단으로  
  사용되어야 한다. Proxy를 사용하면 트래픽에 중요도를 부여하기 어려워지며, raw in-flight 요청들이 함께 집계되는 것은  
  서비스가 실제로 과부하 상태인지 아닌지 여부를 판단하는 데에 때때로 부정확한 정보를 제공하기 때문이다.

- 이 문서의 첫 부분에서 우리는 저자가 Service Frameworks 팀에서 근무했을 때 맞이했던 문제점들에 대해 알아보았다.  
  저자의 팀은 각 팀이 사용하는 load balancer에게 기본적인 max connection 값을 추천하고 싶었다.  
  결과적으로는 각 팀에게 load balancer와 proxy의 max connection을 높게 가져가도록 하고, 서버단에서 서버단의 정보를 토대로  
  더욱 정교한 load shedding 알고리즘을 개발해 사용하도록 가이드했다. 하지만 여전히 max connection 값은 중요했는데,  
  load balancer 또는 proxy의 max connection 값이 서버의 스레드, 프로세스, file descriptor 수 등의 리소스  
  제한보다 더 크다면 서버가 load balancer의 health check 요청을 처리할 수 없게 되기 때문이다.

- OS 단에서 서버의 리소스를 제한하도록 하는 기능은 굉장히 강력하고, 긴급 상황에 유용하게 사용될 수 있다.  
  그리고 과부하는 언제든지 발생할 수 있기 때문에 적절한 처리 방식과 명령어들을 준비하도록 신경써야 한다.  
  예를 들어, iptables 유틸리티는 서버가 수신할 수 있는 connection 수에 상한을 지정할 수 있고, 이를 초과하는 connection들을  
  어느 서버 프로세스보다 더욱 효율적으로 거절할 수 있다. 그리고 제한된 단위 안에서 새로운 connection을 허용하거나, source IP 주소마다  
  connection 수립률을 제한할 수 있다. Source IP 필터는 강력하지만 전통적인 load balancer에서는 사용할 수 없다.  
  하지만 ELB Network Load Balancer는 네트워크 가상화(network virtualization)를 통해 OS단에서 호출자의  
  source IP 주소를 보관함으로써 source IP filter와 같은 iptables의 규칙들이 예상대로 동작하도록 한다.

### Protecting in layers

- 일부 경우, 서버가 요청을 거절하면서 속도가 느려질 정도로 리소스가 매우 부족한 상황이 발생할 수 있다. 이런 경우는 굉장히 현실적이며,  
  이에 대응하기 위해서는 서버와 클라이언트 사이의 모든 network hop을 지켜보고 이들이 load shedding을 구현하기 위해  
  어떻게 협력해야 하는지 파악해야 한다. 예를 들어, 여러 AWS 서비스들은 기본적으로 load shedding을 지원한다.  
  만약 Amazon API Gateway를 서비스의 앞에 둔다면, API Gateway 단에서 API마다 최대 요청 처리율을 지정할 수 있다.  
  그리고 API Gateway, Load Balancer, CloudFront 중 하나라도 서비스 앞단에 두는 경우, AWS WAF를 통해  
  다방면으로 초과되는 트래픽을 제한할 수 있다.

- 가시성도 매우 중요하다. 초과되는 트래픽은 최대한 빨리 거절하는 것이 중요한데, 이 방식이 가장 비용이 저렴하기 때문이다.  
  하지만 이를 제대로 관리하기 위해선 가시성이 확보되어야 한다. 이것이 layer들을 사용해 애플리케이션을 보호해야 하는 이유인데,  
  서버가 요청을 거절하기 위한 작업을 최대한 적게 수행하도록 하고, 제한되는 트래픽에 대한 logging을 더욱 상세하게 할 수 있기  
  때문이다. 서버가 거절할 수 있는 트래픽의 양에는 한계가 있기 때문에, 이를 초과하는 엄청난 양의 트래픽에 대해서는 앞단의 layer에서 보호해야 한다.

---
